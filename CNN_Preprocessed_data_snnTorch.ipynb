{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn \n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import mne\n",
    "from mne.time_frequency import psd_welch\n",
    "import torch\n",
    "import csv\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "datetimeFormat = '%Y-%m-%d %H:%M:%S.%f'\n",
    "import re\n",
    "\n",
    "from numpy.fft import fft, fftfreq\n",
    "from scipy import signal\n",
    "from mne.time_frequency import tfr_morlet\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "%matplotlib notebook\n",
    "mne.set_log_level('ERROR') \n",
    "\n",
    "path_to_data = './data_files.txt'\n",
    "path_to_csv = './Invasive_data.csv'\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "def square_data(inputs):\n",
    "    data_df = pd.Series(inputs)\n",
    "    data_df_sq = data_df.pow(2)\n",
    "    data_df_np = data_df_sq.to_numpy()\n",
    "    return data_df_np\n",
    "\n",
    "def normalize_data(inputs):\n",
    "    data_df_HRC1 = pd.Series(inputs)\n",
    "    data_df_std_dev = data_df_HRC1.rolling(time_steps).std() # 30 seconds\n",
    "    data_df_norm = data_df_HRC1.divide(data_df_std_dev)  \n",
    "    data_df_norm[:time_steps-1].update(data_df_HRC1[:time_steps-1]) # if NaN, norm = original value\n",
    "    data_df_HRC1_np = data_df_norm.to_numpy()\n",
    "    return data_df_HRC1_np\n",
    "\n",
    "def diff_data(inputs):  #TODO : Do this before rereferencing\n",
    "    data_df = pd.Series(inputs)\n",
    "    data_df_diff= data_df.diff()\n",
    "    data_df_np = data_df_diff.to_numpy()\n",
    "    return data_df_np\n",
    "\n",
    "def dur(num):\n",
    "    if num/60 > 1:\n",
    "        return 60.0\n",
    "    else:\n",
    "        return float(num)\n",
    "\n",
    "# Set reference channel for all data points for all channels individually\n",
    "HRA = ['HRA1','HRA2','HRA3','HRA4','HRA5']\n",
    "HRB = ['HRB1','HRB2','HRB3','HRB4','HRB5','HRC1','HRC2','HRC3','HRC4','HRC5']\n",
    "HRC = ['HRB1','HRB2','HRB3','HRB4','HRB5','HRC1','HRC2','HRC3','HRC4','HRC5']\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA-enabled GPU found. Training should be faster.')\n",
    "else:\n",
    "    print('No GPU found. Training will be slow')\n",
    "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "\n",
    "my_database ={'Patient 1':{'PatientID': 0, 'FileID': [], 'Frequency': 512, 'Duration': 0}}\n",
    "num_of_files = range(8)\n",
    "\n",
    "#fnames = fetch_data(subjects=subjects, recording=recordings, on_missing='warn') # LIST\n",
    "print('HERE WE GO!!!!!!!!!!!!!!')\n",
    "with open(path_to_data) as file:\n",
    "    lines = file.readlines()\n",
    "    lines_iter = iter(lines)\n",
    "    lines = [line.rstrip() for line in lines]\n",
    "    print('lines', lines, len(lines))\n",
    "            \n",
    "def load_raw_dataset(line, my_database):\n",
    "    \"\"\"Load a recording from the iEEG dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_fname : str\n",
    "        Path to the .data file containing the raw data.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mne.io.Raw :\n",
    "        Raw object containing the EEG. \n",
    "    \"\"\"\n",
    "    #for line in range(0,len(lines)):\n",
    "    print(line, type(line))\n",
    "    #if not line.startswith(\"#\"):\n",
    "    entry = 'EEG_inv_' + str(line)[-18:-15] + '_' + str(line)[-9:-5]\n",
    "    print('Value for entry',entry)\n",
    "    my_database['Patient 1']['PatientID'] = str(line)[-18:-15]\n",
    "    my_database['Patient 1']['FileID'] = str(line)[-9:-5]\n",
    "    #print(my_database)\n",
    "    data_raw =  mne.io.read_raw_nicolet(str(line), 'eeg', preload=True)\n",
    "    \n",
    "    return data_raw\n",
    "\n",
    "# Check \n",
    "#load_raw_dataset(lines,my_database)\n",
    "# Load recordings\n",
    "data_raws = [load_raw_dataset(line, my_database) for line in lines]\n",
    "print('Database entries',len(data_raws) , type(data_raws) )\n",
    "#print(data_raws[0].info)\n",
    "#print(data_raws[0].plot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate spike using \n",
    "    #1 Rate coding: spikegen.rate   (On normal data and on first derivvative of data)\n",
    "    #2 Latency coding: spikegen.latency\n",
    "    #3 Delta modulation: spikegen.delta\n",
    "#Delta Encoding\n",
    "\n",
    "#!pip install plotly\n",
    "import snntorch as snn\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "dtype = torch.float\n",
    "from matplotlib.widgets import Slider,Button\n",
    "import plotly.express as px\n",
    "from snntorch import surrogate\n",
    "from snntorch import backprop\n",
    "from snntorch import functional as SF\n",
    "from snntorch import utils\n",
    "from snntorch import spikeplot as splt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import torch.nn.functional as F\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(data_raw, csv_row ):\n",
    "    df = pd.read_csv(path_to_csv, usecols=['meas_date','onset', 'offset', 'channels'])\n",
    "    cnt_data =0\n",
    "    cnt_data_ch = 0\n",
    "    columns_ = []\n",
    "    data_ch_dict = [None] * len(data_raw)\n",
    "    data_epoch = [None] * len(data_raw)\n",
    "    \n",
    "    for col in range(0,len(df.columns)):\n",
    "        data = df.iloc[csv_row,col]\n",
    "        columns_.append(data)\n",
    "    \n",
    "    meas_data = columns_[0]\n",
    "    onset = columns_[1]\n",
    "    offset = columns_[2]\n",
    "    channel = columns_[3]\n",
    "\n",
    "    tdur_seiz = datetime.datetime.strptime(offset, datetimeFormat)- datetime.datetime.strptime(onset, datetimeFormat)\n",
    "    tdiff_on = datetime.datetime.strptime(onset, datetimeFormat)- datetime.datetime.strptime(meas_data, datetimeFormat)\n",
    "    tdiff_off = datetime.datetime.strptime(offset, datetimeFormat)- datetime.datetime.strptime(meas_data, datetimeFormat)\n",
    "    #print(\"seizure starts after:\",tdiff_on, \"seizure lasts for\",tdur_seiz,\"stops after\",tdiff_off)\n",
    "    #print('Seizure onset',onset,'Seizure offset',offset )\n",
    "    #print(print(\"seizure starts after:\",tdiff_on.seconds, \"seizure lasts for\",tdur_seiz.seconds,\"stops after\",tdiff_off.seconds))\n",
    "\n",
    "    for arr in range(len(channel)):\n",
    "        for lis in range(len(channel[arr])):\n",
    "            channel[arr][lis].replace('\\'','')\n",
    "            channel[arr][lis].replace(']','')\n",
    "            channel[arr][lis].replace('[','')\n",
    "            channel[arr][lis].replace(' ','')\n",
    "    #print(\"Shape and type of tdiff_on:\",type(tdiff_on),tdiff_on, 'channel',channel)\n",
    "    \n",
    "    if channel.find(',') != -1:\n",
    "        channel = list(channel.split(\",\"))\n",
    "        #rint('HEY FOUND A ,', channel,len(channel), type(channel))\n",
    "    else:\n",
    "        channel = list(channel.split(\",\"))\n",
    "        #print('HEY DID NOT FIND ,', channel, len(channel), type(channel))\n",
    "    \n",
    "    cnt_data += 1\n",
    "\n",
    "    #ch_name = entry \n",
    "    ref_ch = HRB\n",
    "\n",
    "    data_ch_dict = data_raw.copy()\n",
    "    #print('Before re-referencing',data_ch_dict.info)\n",
    "    #data_ch_dict.plot()\n",
    "    data_ch_dict = data_ch_dict.pick_channels(ch_names= ref_ch, ordered=False)\n",
    "    data_ch_dict.set_eeg_reference(ref_channels=ref_ch)\n",
    "    #print('After re-referencing',data_ch_dict.info)\n",
    "    #data_ch_dict.plot()\n",
    "\n",
    "    data_ch_dict.load_data().filter(l_freq=0.2, h_freq=48, method='iir')\n",
    "    #print('After filtering',data_ch_dict.info)\n",
    "\n",
    "    do_normalization = False\n",
    "    calculate_dt= False\n",
    "    #time_steps = (10*60)/2\n",
    "    time_steps = 10\n",
    "    \n",
    "    #Calculate time difference     #before re-referencing TODO\n",
    "    if calculate_dt == True:\n",
    "        #print('First derivative', channel)\n",
    "        data_ch_dict.apply_function(diff_data, picks=channel)\n",
    "        #print('After first derivative',data_ch_dict.info)\n",
    "        #data_ch_dict.plot()\n",
    "    else:\n",
    "        data_ch_dict = data_ch_dict\n",
    "        #print('First derivative not taken')\n",
    "    \n",
    "    # TODO ; Temporary use HRB2 and HRC2\n",
    "    channel_temp = ['HRB2','HRC2']\n",
    "    data_ch_dict.pick_channels(ch_names = channel_temp, ordered = False)\n",
    "    #data_ch_dict.plot()\n",
    "\n",
    "    #Crop the data to include less non-ictal activity : Temporary logic\n",
    "    def define_df(data_ch_dict, idx):\n",
    "        df1 = data_ch_dict.to_data_frame()\n",
    "        df1['time_s'] = df1['time'] / 1000.0\n",
    "        df1['target'] = 0\n",
    "        df1['target'] = np.where(( (df1.time_s+ idx >= np.float(tdiff_on.seconds)) & (df1.time_s + idx <= np.float(tdiff_off.seconds))),1,df1.target)\n",
    "        return df1\n",
    "    \n",
    "    if (tdiff_on.seconds < 1200):\n",
    "        data_ch_dict_n = data_ch_dict.copy().crop(tmin=0,tmax=250) #0,1200\n",
    "        #print('Seizure occurs in first half')\n",
    "        df_subset = define_df(data_ch_dict_n, 0)\n",
    "    elif (tdiff_on.seconds > 2400):\n",
    "        data_ch_dict_n = data_ch_dict.copy().crop(tmin=2500, tmax =2800) # 2400,3600\n",
    "        print('Seizure occurs in end')\n",
    "        df_subset = define_df(data_ch_dict_n, 2500)\n",
    "    else:\n",
    "        data_ch_dict_n = data_ch_dict.copy().crop(tmin=1200,tmax=2400) # 1200,2400\n",
    "        print('Seizure occurs in mid ')\n",
    "        df_subset = define_df(data_ch_dict_n, 1300)\n",
    "        \n",
    "    #data_ch_dict_n.plot()\n",
    "    \n",
    "    '''print(df_subset)\n",
    "    # Store data, time and label as dataframe\n",
    "    df = data_ch_dict.to_data_frame()\n",
    "    df['time_s'] = df['time'] / 1000.0\n",
    "    df['target'] = 0\n",
    "    df['target'] = np.where(( (df.time_s >= np.float(tdiff_on.seconds)) & (df.time_s <= np.float(tdiff_off.seconds))),1,df.target)\n",
    "    #print(df)'''\n",
    "    \n",
    "    #Extract 5s epochs : Create epochs and labels\n",
    "    interval = 5  #TODO 2 sec\n",
    "    time_window = interval * data_ch_dict.info['sfreq'] # 5*512\n",
    "    \n",
    "    data_epoch = mne.make_fixed_length_epochs(data_ch_dict_n, duration= interval, preload=True, id= csv_row) \n",
    "    #print('Fixed Length Epochs',csv_row,data_epoch, len(data_epoch), type(data_epoch) )\n",
    "    #print('Epochs return value',data_epoch.get_data(), np.shape(data_epoch.get_data()), type(data_epoch.get_data()) )\n",
    "    \n",
    "    data_labels = [None] * len(data_epoch)  # 720\n",
    "    for seg in range(0,len(data_epoch)):\n",
    "        df_start = int(seg*time_window)\n",
    "        df_end = int((seg+1)*time_window)\n",
    "        #print('df start and end', df_start, df_end, len(data_epoch),df_subset.loc[df_start:df_end])\n",
    "        count_labels = df_subset.loc[df_start:df_end,'target'].sum()\n",
    "        if count_labels > 1000:\n",
    "            #print('SEIZURE DETECTED', csv_row, ':', count_labels,'seg', seg, 'start',df_start,'end',df_end)\n",
    "            label = 1\n",
    "        else:\n",
    "            #print('NO SEIZURE DETECTED FOR', csv_row,':',count_labels,'seg', seg, 'start',df_start,'end',df_end)\n",
    "            label = 0\n",
    "        data_labels[seg] = label\n",
    "        count_labels = 0\n",
    "    #print( np.shape(np.array(data_labels)),type(np.array(data_labels)) )   #RETURN\n",
    "    #print('Label generated for',csv_row,':',data_epoch[0])\n",
    "    \n",
    "    '''#Visualize label generation\n",
    "    print(df_subset)\n",
    "    t1 = df_subset.time_s\n",
    "    t2 = df_subset.time_s\n",
    "\n",
    "    plt.figure()\n",
    "    plt.subplot(211)\n",
    "    plt.plot(t1, df_subset.HRB2)\n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.plot(t2, df_subset.target)\n",
    "    plt.show()#'''\n",
    "    return data_epoch.get_data() , np.array(data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, ConcatDataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "class EpochsDataset(Dataset):\n",
    "    \"\"\"Class to expose an MNE Epochs object as PyTorch dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    epochs_data : np.ndarray\n",
    "        The epochs data, shape (n_epochs, n_channels, n_times).\n",
    "    epochs_labels : np.ndarray\n",
    "        The epochs labels, shape (n_epochs,)\n",
    "    transform : callable | None\n",
    "        The function to eventually apply to each epoch\n",
    "        for preprocessing (e.g. scaling). Defaults to None.\n",
    "    \"\"\"\n",
    "    def __init__(self, epochs_data, epochs_labels, transform=None):\n",
    "        assert len(epochs_data) == len(epochs_labels)\n",
    "        self.epochs_data = epochs_data\n",
    "        self.epochs_labels = epochs_labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.epochs_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X, y = self.epochs_data[idx], self.epochs_labels[idx]\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "        X = torch.as_tensor(X[None, ...])\n",
    "        return X, y\n",
    "\n",
    "#transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "#with dt Mean [[ 2.3293406e-22][-2.8587362e-22]     Std Dev [[2.29891510e-05][1.07505107e-05]]\n",
    "#without dt Mean [[-1.35525272e-21] [ 6.77626358e-22]] Std Dev [[6.45315196e-05] [3.75374791e-05]]\n",
    "def scale(X):\n",
    "    \"\"\"Standard scaling of data along the last dimention.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape (n_channels, n_times)\n",
    "        The input signals.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X_t : array, shape (n_channels, n_times)\n",
    "        The scaled signals.\n",
    "    \"\"\"\n",
    "    X -= np.mean(X, axis=1, keepdims=True)\n",
    "    #print('Mean',np.mean(X, axis=1, keepdims=True))\n",
    "    #print('Std Dev',  np.std(X, axis=1, keepdims=True))\n",
    "    return X / np.std(X, axis=1, keepdims=True)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "# Apply windowing and move to pytorch dataset\n",
    "all_datasets = [EpochsDataset(*process_file(data_raw, csv_row), transform=scale) \n",
    "                for csv_row,data_raw in enumerate(data_raws)]\n",
    "\n",
    "# Concatenate into a single dataset\n",
    "dataset = ConcatDataset(all_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Making training test validation split\n",
    "from sklearn.model_selection import LeavePGroupsOut  #TODO; Do LOO split for validation\n",
    "\n",
    "def train_test_split(dataset):\n",
    "    \"\"\"Split dataset into train and test \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : ConcatDataset\n",
    "        The dataset to split.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    ConcatDataset\n",
    "        The training data and the testing data.\n",
    "    \"\"\"\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    print('Train and test size', train_size, test_size)\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "torch.manual_seed(87)\n",
    "np.random.seed(87)\n",
    "train_ds, test_ds = train_test_split(dataset)\n",
    "print('Number of examples in each set:')\n",
    "print(f'Training: {len(train_ds)}')\n",
    "print(f'Test: {len(test_ds)}')\n",
    "print(train_ds)\n",
    "\n",
    "## Computing class weight\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "classes_mapping = {0: 'non-ictal', 1: 'ictal'}\n",
    "train_y = pd.Series([y for _, y in train_ds]).map(classes_mapping)\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_y), y=train_y)\n",
    "print(class_weights)\n",
    "#Visualizing the class distribution, Very imbalanced right now #TODO\n",
    "#ax_ = train_y.value_counts().plot(kind='barh')\n",
    "#ax_.set_xlabel('Number of training examples');\n",
    "#ax_.set_ylabel('Sleep stage');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN AND MONITOR NETWORK\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create dataloaders\n",
    "train_batch_size = 47  # Important hyperparameter For Now : (720/4)\n",
    "valid_batch_size = 94  # Can be made as large as what fits in memory; won't impact performance\n",
    "num_workers = 0  # Number of processes to use for the data loading process; 0 is the main Python process\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=train_batch_size, shuffle=True, num_workers=num_workers)\n",
    "#loader_valid = DataLoader(\n",
    "#    valid_ds, batch_size=valid_batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=valid_batch_size, shuffle=False, num_workers=num_workers)\n",
    "print('Training files in a batch', len(train_loader),' Testing files in a batch', len(test_loader))\n",
    "print('Training files total ', len(train_ds),' Testing files total', len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a basic supervised learning algorithm will be implemented.\n",
    "#train a multi-layer fully-connected spiking neural network using gradient descent to perform image classification.\n",
    "############### TUTORIAL 5 ########################\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from snntorch import surrogate\n",
    "\n",
    "# Network Architecture\n",
    "num_inputs = 2*2560\n",
    "num_mid1 = 1024\n",
    "num_mid2 = 512\n",
    "num_mid3 = 256\n",
    "num_outputs = 2\n",
    "\n",
    "# Temporal Dynamics\n",
    "num_steps = 25\n",
    "beta = 0.95\n",
    "device = 'cpu'\n",
    "batch_size = 47  # Change to 180 later / 126\n",
    "#spike_grad = surrogate.fast_sigmoid(slope=25)\n",
    "\n",
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_mid1)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.fc2 = nn.Linear(num_mid1, num_outputs)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "        #self.fc3 = nn.Linear(num_mid2, num_mid3)\n",
    "        #self.lif3 = snn.Leaky(beta=beta)\n",
    "        #self.fc4 = nn.Linear(num_mid3, num_outputs)\n",
    "        #self.lif4 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        #mem3 = self.lif3.init_leaky()\n",
    "        #mem4 = self.lif4.init_leaky()\n",
    "        \n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            #print('x',x.size())\n",
    "            cur1 = self.fc1(x)\n",
    "            #print('cur1',cur1.size())\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            #print('spk1',spk1.size(), 'mem1',mem1.size())\n",
    "            cur2 = self.fc2(spk1)\n",
    "            #print('cur2',cur2.size())\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            '''\n",
    "            #print('spk2',spk2.size(), 'mem2',mem2.size())\n",
    "            cur3 = self.fc3(spk2)\n",
    "            #print('cur3',cur3.size())\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            #print('spk3',spk3.size(), 'mem3',mem3.size())\n",
    "            cur4 = self.fc4(spk3)\n",
    "            #print('cur4',cur4.size())\n",
    "            spk4, mem4 = self.lif4(cur4, mem4)\n",
    "            #print('spk4',spk4.size(), 'mem4',mem4.size())'''\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "        #print('output', torch.stack(spk2_rec, dim=0).size, torch.stack(spk2_rec).size()) \n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
    "\n",
    "# Load the network onto CUDA if available\n",
    "net = Net().to(device)\n",
    "\n",
    "# pass data into the network, sum the spikes over time\n",
    "# and compare the neuron with the highest number of spikes\n",
    "# with the target\n",
    "\n",
    "def print_batch_accuracy(data, targets, train=False):\n",
    "    output, _ = net(data.view(batch_size, -1))\n",
    "    check_all_output = output.sum(dim=0)\n",
    "    print('spk_rec values', check_all_output.size(),'zero',check_all_output[0],'one', check_all_output[1],'last',check_all_output[46] )\n",
    "    val, idx = output.sum(dim=0).max(1)\n",
    "    print('idx from',output.sum(dim=0).max(1), 'idx.size',idx.size(), 'targets', targets)\n",
    "    acc = np.mean((targets == idx).detach().cpu().numpy())\n",
    "    print('acc', acc)\n",
    "\n",
    "    if train:\n",
    "        print(f\"Train set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Test set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "\n",
    "def train_printer():\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    print(f\"Train Set Loss: {loss_hist[counter]:.2f}\")\n",
    "    print(f\"Test Set Loss: {test_loss_hist[counter]:.2f}\")\n",
    "    print_batch_accuracy(data, targets, train=True)\n",
    "    print_batch_accuracy(test_data, test_targets, train=False)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "#loss = nn.CrossEntropyLoss()\n",
    "#loss = nn.CrossEntropyLoss(weight=torch.Tensor(class_weights).to(device)) \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "loss = SF.ce_rate_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plotting Settings\n",
    "def plot_cur_mem_spk(cur, mem, spk, thr_line=False, vline=False, title=False, ylim_max1=2.5, ylim_max2=2.5, num_steps =200):\n",
    "    # Generate Plots\n",
    "    fig, ax = plt.subplots(3, figsize=(8,6), sharex=True, \n",
    "                        gridspec_kw = {'height_ratios': [1, 1, 0.4]})\n",
    "\n",
    "    # Plot input current\n",
    "    ax[0].plot(cur, c=\"tab:orange\")\n",
    "    ax[0].set_ylim([-ylim_max1, ylim_max1])\n",
    "    ax[0].set_xlim([0, num_steps])\n",
    "    ax[0].set_ylabel(\"Input Current ($I_{in}$)\")\n",
    "    if title:\n",
    "        ax[0].set_title(title)\n",
    "\n",
    "    # Plot membrane potential\n",
    "    ax[1].plot(mem)\n",
    "    ax[1].set_ylim([-ylim_max2, ylim_max2]) \n",
    "    ax[1].set_ylabel(\"Membrane Potential ($U_{mem}$)\")\n",
    "    if thr_line:\n",
    "        ax[1].axhline(y=thr_line, alpha=0.25, linestyle=\"dashed\", c=\"black\", linewidth=2)\n",
    "    plt.xlabel(\"Time step\")\n",
    "\n",
    "    # Plot output spike using spikeplot\n",
    "    splt.raster(spk, ax[2], s=400, c=\"black\", marker=\"|\") # s is size of scatter points\n",
    "    if vline:\n",
    "        ax[2].axvline(x=vline, ymin=0, ymax=6.75, alpha = 0.15, linestyle=\"dashed\", c=\"black\", linewidth=2, zorder=0, clip_on=False)\n",
    "    plt.ylabel(\"Output spikes\")\n",
    "    plt.yticks([]) \n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_snn_spikes(spk_in, spk1_rec, spk2_rec, title):\n",
    "    # Generate Plots\n",
    "    fig, ax = plt.subplots(3, figsize=(8,7), sharex=True, \n",
    "                        gridspec_kw = {'height_ratios': [1, 1, 0.4]})\n",
    "\n",
    "    # Plot input spikes\n",
    "    splt.raster(spk_in[:,0], ax[0], s=0.03, c=\"black\")\n",
    "    ax[0].set_ylabel(\"Input Spikes\")\n",
    "    ax[0].set_title(title)\n",
    "\n",
    "    # Plot hidden layer spikes\n",
    "    splt.raster(spk1_rec.reshape(num_steps, -1), ax[1], s = 0.05, c=\"black\")\n",
    "    ax[1].set_ylabel(\"Hidden Layer\")\n",
    "\n",
    "    # Plot output spikes\n",
    "    splt.raster(spk2_rec.reshape(num_steps, -1), ax[2], c=\"black\", marker=\"|\")\n",
    "    ax[2].set_ylabel(\"Output Spikes\")\n",
    "    ax[2].set_ylim([0, 1])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######   Tutorial 3: A Feedforward Spiking Neural Network\n",
    "num_steps = 2560\n",
    "\n",
    "data, targets = next(iter(train_loader))   # Normalized data\n",
    "data = data.to(device=device, dtype=torch.float32)\n",
    "#print('data',data.size(), data)\n",
    "#data = spikegen.delta(data, threshold=0.5)\n",
    "targets = targets.to(device=device, dtype=torch.int64)\n",
    "print('targets',targets.size(), targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Continued Plot spike for 1 datapoint of channel when target =1\n",
    "lif1 = snn.Leaky(beta=0.819, threshold=1.2)\n",
    "\n",
    "def leaky_integrate_and_fire(mem, x, w, beta, threshold=1.2):\n",
    "    spk = (mem > threshold) # if membrane exceeds threshold, spk=1, else, 0\n",
    "    mem = beta * mem + w*x - spk*threshold\n",
    "    return spk, mem\n",
    "\n",
    "data_point = 2\n",
    "x = data[data_point][0][0]\n",
    "cur_in = data[data_point][0][0]\n",
    "print('x is for channel 1',x.size(), x, 'with the target',targets[data_point] )\n",
    "mem = torch.zeros(1)\n",
    "spk_out = torch.zeros(1)\n",
    "mem_rec = []\n",
    "spk_rec = []\n",
    "\n",
    "# neuron parameters\n",
    "w = 0.5\n",
    "beta = 0.819\n",
    "\n",
    "# neuron simulation\n",
    "for step in range(num_steps):\n",
    "    #spk, mem = leaky_integrate_and_fire(mem, x[step], w=w, beta=beta) # Replace with w*X[t], mem\n",
    "    spk, mem = lif1(cur_in[step], mem)\n",
    "    mem_rec.append(mem)\n",
    "    spk_rec.append(spk)\n",
    "\n",
    "# convert lists to tensors\n",
    "mem_rec = torch.stack(mem_rec)\n",
    "spk_rec = torch.stack(spk_rec)\n",
    "\n",
    "plot_cur_mem_spk(x*w, mem_rec, spk_rec, thr_line=1.2,ylim_max1=2.5,\n",
    "                 title=\"LIF Neuron Model With Weighted Step Voltage\", num_steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot spike for 1 datapoint of channel when target =0\n",
    "data_point = 1\n",
    "x = data[data_point][0][0]\n",
    "cur_in = data[data_point][0][0]\n",
    "print('x is for channel 1',x.size(), x, 'with the target',targets[data_point] )\n",
    "mem = torch.zeros(1)\n",
    "spk_out = torch.zeros(1)\n",
    "mem_rec = []\n",
    "spk_rec = []\n",
    "\n",
    "# neuron parameters\n",
    "w = 0.5\n",
    "beta = 0.819\n",
    "\n",
    "# neuron simulation\n",
    "for step in range(num_steps):\n",
    "    #spk, mem = leaky_integrate_and_fire(mem, x[step], w=w, beta=beta)\n",
    "    spk, mem = lif1(cur_in[step], mem)\n",
    "    mem_rec.append(mem)\n",
    "    spk_rec.append(spk)\n",
    "\n",
    "# convert lists to tensors\n",
    "mem_rec = torch.stack(mem_rec)\n",
    "spk_rec = torch.stack(spk_rec)\n",
    "\n",
    "plot_cur_mem_spk(x*w, mem_rec, spk_rec, thr_line=1.2,ylim_max1=2.5,\n",
    "                 title=\"LIF Neuron Model With Weighted Step Voltage\", num_steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feedforward Spiking Neural Network\n",
    "# layer parameters\n",
    "num_inputs = 2560   # What should be the correct input\n",
    "num_hidden = 1000\n",
    "num_outputs = 2\n",
    "beta = 0.819\n",
    "data_point = 2   # Chosen a batch with truth =1\n",
    "\n",
    "# initialize layers\n",
    "fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "lif1 = snn.Leaky(beta=0.819, threshold=0.5)  #1.2\n",
    "fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "lif2 = snn.Leaky(beta=0.819, threshold=0.5)\n",
    "\n",
    "# Initialize hidden states\n",
    "mem1 = lif1.init_leaky()\n",
    "mem2 = lif2.init_leaky()\n",
    "\n",
    "# record outputs\n",
    "mem2_rec = []\n",
    "spk1_rec = []\n",
    "spk2_rec = []\n",
    "\n",
    "'''The number of time steps will be set to that of the longest recording in the mini-batch, \n",
    "   and all other samples will be padded with zeros to match it.'''\n",
    "#Dimension of input should be for 1 batch time_step X 1X  feature_dimension (spk_in)\n",
    "#spk_in = spikegen.rate_conv(torch.rand((200, 784))).unsqueeze(1) #Should be 2560 X 1 X 1\n",
    "spk_in = ((data[data_point][0][0]).repeat(2560,1)).unsqueeze(1)   #([2560, 1, 2560])\n",
    "#spk_in = spikegen.delta(spk_in_o, threshold=0.01)\n",
    "print(f\"Dimensions of spk_in: {spk_in.size()}\")\n",
    "\n",
    "# network simulation\n",
    "for step in range(num_steps):\n",
    "    cur1 = fc1(spk_in[step]) # post-synaptic current <-- spk_in x weight\n",
    "    spk1, mem1 = lif1(cur1, mem1) # mem[t+1] <--post-syn current + decayed membrane\n",
    "    cur2 = fc2(spk1)\n",
    "    spk2, mem2 = lif2(cur2, mem2)\n",
    "\n",
    "    mem2_rec.append(mem2)\n",
    "    spk1_rec.append(spk1)\n",
    "    spk2_rec.append(spk2)\n",
    "\n",
    "# convert lists to tensors\n",
    "mem2_rec = torch.stack(mem2_rec)\n",
    "spk1_rec = torch.stack(spk1_rec)\n",
    "spk2_rec = torch.stack(spk2_rec)\n",
    "print('mem_rec', mem2_rec.shape, 'spk1', spk1_rec.shape, 'spk2', spk2_rec.shape)\n",
    "print('spk_in_o',spk_in, 'mem_rec',mem2_rec[0][0][:], 'spk1', spk1_rec[0][0][:], 'spk2', spk2_rec)\n",
    "\n",
    "#plot_snn_spikes(spk_in, spk1_rec, spk2_rec, \"Fully Connected Spiking Neural Network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "fig, ax = plt.subplots(facecolor='w', figsize=(12, 7))\n",
    "labels=['non-ictal', 'ictal']\n",
    "spk2_rec = spk2_rec.squeeze(1).detach().cpu()\n",
    "\n",
    "#  Plot spike count histogram\n",
    "anim = splt.spike_count(spk2_rec, fig, ax, labels=labels, animate=True)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TUtorial 5 : Training Spiking CNNs ################### for 1 channel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from snntorch import surrogate\n",
    "\n",
    "# Network Architecture\n",
    "num_inputs = 2560\n",
    "num_mid1 = 1000\n",
    "#num_mid2 = 512\n",
    "#num_mid3 = 256\n",
    "num_outputs = 2\n",
    "\n",
    "# Temporal Dynamics\n",
    "num_steps = 2560\n",
    "beta = 0.819\n",
    "device = 'cpu'\n",
    "batch_size_train = 47  # Change to 180 later / 126\n",
    "batch_size_test = 94 \n",
    "#spike_grad = surrogate.fast_sigmoid(slope=25)\n",
    "\n",
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_mid1)\n",
    "        self.lif1 = snn.Leaky(beta=beta, threshold=0.1)\n",
    "        self.fc2 = nn.Linear(num_mid1, num_outputs)\n",
    "        self.lif2 = snn.Leaky(beta=beta, threshold=0.1)\n",
    "        #self.fc3 = nn.Linear(num_mid2, num_mid3)\n",
    "        #self.lif3 = snn.Leaky(beta=beta)\n",
    "        #self.fc4 = nn.Linear(num_mid3, num_outputs)\n",
    "        #self.lif4 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        #mem3 = self.lif3.init_leaky()\n",
    "        #mem4 = self.lif4.init_leaky()\n",
    "        \n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            #print('x',x.size())\n",
    "            cur1 = self.fc1(x)\n",
    "            #print('cur1',cur1.size())\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            #print('spk1',spk1.size(), 'mem1',mem1.size())\n",
    "            cur2 = self.fc2(spk1)\n",
    "            #print('cur2',cur2.size())\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            '''\n",
    "            #print('spk2',spk2.size(), 'mem2',mem2.size())\n",
    "            cur3 = self.fc3(spk2)\n",
    "            #print('cur3',cur3.size())\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            #print('spk3',spk3.size(), 'mem3',mem3.size())\n",
    "            cur4 = self.fc4(spk3)\n",
    "            #print('cur4',cur4.size())\n",
    "            spk4, mem4 = self.lif4(cur4, mem4)\n",
    "            #print('spk4',spk4.size(), 'mem4',mem4.size())'''\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "        #print('output', torch.stack(spk2_rec, dim=0).size, torch.stack(spk2_rec).size()) \n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
    "\n",
    "# Load the network onto CUDA if available\n",
    "net = Net().to(device)\n",
    "\n",
    "# pass data into the network, sum the spikes over time\n",
    "# and compare the neuron with the highest number of spikes\n",
    "# with the target\n",
    "\n",
    "def print_batch_accuracy(data, targets,batch_size=batch_size_test ,train=False):\n",
    "    output, _ = net(data.view(batch_size, -1))\n",
    "    check_all_output = output.sum(dim=0)\n",
    "    print('spk_rec values', check_all_output.size(),'Spike accumulated for both labels', check_all_output )\n",
    "    val, idx = output.sum(dim=0).max(1)\n",
    "    print('idx from',output.sum(dim=0).max(1), 'idx.size',idx.size(), 'targets', targets)\n",
    "    acc = np.mean((targets == idx).detach().cpu().numpy())\n",
    "    #print('acc', acc)\n",
    "\n",
    "    if train:\n",
    "        print(f\"Train set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "        acc_hist.append(acc.item())\n",
    "    else:\n",
    "        print(f\"Test set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "        test_acc_hist.append(acc.item())\n",
    "\n",
    "def train_printer():\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    print('Counter', counter)\n",
    "    print(f\"Train Set Loss: {loss_hist[counter]:.2f}\")\n",
    "    print(f\"Test Set Loss: {test_loss_hist[counter]:.2f}\")\n",
    "    print_batch_accuracy(data, targets, batch_size=batch_size_train, train=True)\n",
    "    print_batch_accuracy(test_data, test_targets, batch_size=batch_size_test,train=False)\n",
    "    print(f\"Train Accuracy: {acc_hist[counter]:.2f}\")\n",
    "    print(f\"Test Accuracy: {test_acc_hist[counter]:.2f}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "#loss = nn.CrossEntropyLoss()\n",
    "#loss = nn.CrossEntropyLoss(weight=torch.Tensor(class_weights).to(device)) \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "#loss = SF.ce_rate_loss()\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "acc_hist = []\n",
    "test_acc_hist = []\n",
    "counter = 0\n",
    "indices = torch.tensor([0])\n",
    "\n",
    "# Outer training loop\n",
    "for epoch in range(num_epochs):\n",
    "    iter_counter = 0\n",
    "    \n",
    "    # Minibatch training loop\n",
    "    for data, targets in iter(train_loader):\n",
    "        data = data.to(device=device, dtype=torch.float32)\n",
    "        data = torch.index_select(data, 2, indices)\n",
    "        #rint('data', data.shape, data)\n",
    "        check_nan = torch.isnan(data)\n",
    "        #print('idx for nan in training',tf.where(check_nan))\n",
    "        print('does input have nan training', torch.count_nonzero(check_nan))\n",
    "        if (torch.count_nonzero(check_nan) != 0):\n",
    "            corrupted_batch = (tf.where(check_nan))[0][0]\n",
    "            #print('corrupted data', data.shape,'corrupt batch',corrupted_batch)\n",
    "            for i in range(0,47):\n",
    "                print('check if data is corrupt',i,data[i])\n",
    "        #print('data',data.size(), data)\n",
    "        #data = spikegen.delta(data, threshold=0.5)\n",
    "        #print('spikegen data',data.size(), data)\n",
    "        targets = targets.to(device=device, dtype=torch.int64)\n",
    "        #print('targets',targets.size(), targets)\n",
    "\n",
    "        # forward pass\n",
    "        net.train()\n",
    "        #print('data',data.shape, 'input data shape', data.view(batch_size_train, -1).shape)\n",
    "        spk_rec, mem_rec = net(data.view(batch_size_train, -1))\n",
    "        print('######## DURING TRAINING ############')\n",
    "        #print('spk_rec',spk_rec.size(), 'mem_rec',mem_rec.size())\n",
    "        #print('spk_rec',spk_rec, 'mem_rec',mem_rec)\n",
    "        for name, param in net.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print('Name of parameter',name, 'Parameter data', (param.data).shape,param.data)\n",
    "        print('mem_rec',mem_rec)\n",
    "        #print('Non-zero elements in spk_rec',torch.count_nonzero(spk_rec))\n",
    "        \n",
    "        # initialize the loss & sum over time\n",
    "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "        for step in range(num_steps):\n",
    "            #print('Check loss dims', num_steps, mem_rec[step].shape, targets.shape)\n",
    "            loss_val += loss(mem_rec[step], targets)\n",
    "        #print('Loss val for train ', loss_val)\n",
    "        #loss_val = loss(spk_rec, targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "        \n",
    "        #train_printer()\n",
    "        \n",
    "        # Test set\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            test_data, test_targets = next(iter(test_loader))\n",
    "            test_data = test_data.to(device=device, dtype=torch.float32)\n",
    "            test_data = torch.index_select(test_data, 2, indices)\n",
    "            print('test_data', test_data.shape, test_data)\n",
    "            check_nan = torch.isnan(test_data)\n",
    "            print('idx for nan in testing',tf.where(check_nan))\n",
    "            print('does input have nan testing', torch.count_nonzero(check_nan))\n",
    "            #test_data = spikegen.delta(test_data, threshold=0.5)\n",
    "            test_targets = test_targets.to(device=device, dtype=torch.int64)\n",
    "\n",
    "            # Test set forward pass\n",
    "            #print('test_data',test_data.shape, 'input test_data', test_data.view(batch_size_test, -1).shape)\n",
    "            test_spk, test_mem = net(test_data.view(batch_size_test, -1))\n",
    "            print('######## DURING TESTING ############')\n",
    "            #print('spk_rec',test_spk.size(), 'mem_rec',test_mem.size())\n",
    "            #print('spk_rec',test_spk, 'mem_rec',test_mem)\n",
    "            for name, param in net.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    print('Name of parameter',name, 'Parameter data', (param.data).shape,param.data)\n",
    "            print('mem_rec',test_mem)\n",
    "            \n",
    "            # Test set loss\n",
    "            test_loss = torch.zeros((1), dtype=dtype, device=device)\n",
    "            for step in range(num_steps):\n",
    "                test_loss += loss(test_mem[step], test_targets)\n",
    "            #print('Loss val for test ', test_loss)\n",
    "            test_loss_hist.append(test_loss.item())\n",
    "\n",
    "            # Print train/test loss/accuracy\n",
    "            #if counter % 10 == 0:\n",
    "            train_printer()\n",
    "            counter += 1\n",
    "            #iter_counter +=1\n",
    "            #print('counter', counter, 'iter_counter', iter_counter)\n",
    "        print('####### END OF AN ITERATION')\n",
    "        \n",
    "''' # For one sample\n",
    "\n",
    "data, targets = next(iter(train_loader))   # Normalized data\n",
    "data = data.to(device=device, dtype=torch.float32)\n",
    "indices = torch.tensor([0])\n",
    "data = torch.index_select(data, 2, indices)                      # Select only 1 channel\n",
    "#data = spikegen.delta(data, threshold=0.5)\n",
    "targets = targets.to(device=device, dtype=torch.int64)\n",
    "print('batch_size', data.shape)\n",
    "spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
    "print(mem_rec.size())\n",
    "\n",
    "# initialize the total loss value\n",
    "loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "loss_val = loss(spk_rec, targets)\n",
    "\n",
    "print('loss_val', loss_val, loss_val.size())\n",
    "print(f\"Training loss: {loss_val.item():.3f}\")\n",
    "\n",
    "print_batch_accuracy(data, targets, train=True)\n",
    "\n",
    "# clear previously stored gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# calculate the gradients\n",
    "loss_val.backward()\n",
    "\n",
    "# weight update\n",
    "optimizer.step()\n",
    "\n",
    "# calculate new network outputs using the same data\n",
    "spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
    "\n",
    "# initialize the total loss value\n",
    "loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "    \n",
    "loss_val = loss(spk_rec, targets)\n",
    "\n",
    "print(f\"Training loss: {loss_val.item():.3f}\")\n",
    "print_batch_accuracy(data, targets, train=True)''' # end of one sample\n",
    "\n",
    "'''\n",
    "        _, idx = spk_rec.sum(dim=0).max(1)\n",
    "        print('spk_rec values and indices',spk_rec.sum(dim=0).max(1), 'idx:', idx , 'idx.size',idx.size())\n",
    "        acc = np.mean((targets == idx).detach().cpu().numpy())\n",
    "        print('acc', acc)\n",
    "        \n",
    "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "\n",
    "        # sum loss at every step\n",
    "        for step in range(num_steps):\n",
    "            loss_val += loss(mem_rec[step], targets)\n",
    "\n",
    "        print(f\"Training loss: {loss_val.item():.3f}\")\n",
    "        \n",
    "        # clear previously stored gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # calculate the gradients\n",
    "        loss_val.backward()\n",
    "\n",
    "        # weight update\n",
    "        optimizer.step()\n",
    "        \n",
    "        # calculate new network outputs using the same data\n",
    "        spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
    "        print('spk_rec 2',spk_rec.size(), 'mem_rec',mem_rec.size())\n",
    "        print('spk_rec 2',spk_rec, 'mem_rec',mem_rec)\n",
    "        print('Non-zero elements in spk_rec II',torch.count_nonzero(spk_rec))\n",
    "        \n",
    "        _, idx = spk_rec.sum(dim=0).max(1)  # Returns the value and column index at which maximum value is stored\n",
    "        print('spk_rec values and indices II',spk_rec.sum(dim=0).max(1), 'idx:', idx , 'idx.size',idx.size())\n",
    "        \n",
    "        \n",
    "        # initialize the total loss value\n",
    "        #loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "\n",
    "        # sum loss at every step\n",
    "        #for step in range(num_steps):\n",
    "        #    loss_val += loss(mem_rec[step], targets)\n",
    "        #print(f\"Training loss: {loss_val.item():.3f}\")\n",
    "        \n",
    "        #print_batch_accuracy(data, targets, train=True)\n",
    "        #spk_rec_output, _ = net(data.view(batch_size, -1))\n",
    "        #_, idx = spk_rec_output.sum(dim=0).max(1)\n",
    "        #print('spk_rec values and indices',spk_rec_output.sum(dim=0).max(1), 'idx:', idx , 'idx.size',idx.size())\n",
    "        #acc = np.mean((targets == idx).detach().cpu().numpy())\n",
    "        #print('acc', acc)\n",
    "     '''   \n",
    "# TODO ; diff_data is important, test it and add it\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "plt.plot(loss_hist)\n",
    "plt.plot(test_loss_hist)\n",
    "plt.title(\"Loss Curves\")\n",
    "plt.legend([\"Train Loss\", \"Test Loss\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "# Plot Accuracy\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "plt.plot(acc_hist)\n",
    "plt.plot(test_acc_hist)\n",
    "plt.title(\"Accuracy Curves\")\n",
    "plt.legend([\"Train accuracy\", \"Test Accuracy\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Tutorial 6 : Surrogate Gradient Descent in a Convolutional Spiking Neural Network for 2 channels\n",
    "#Solution to dead neuron problem : Smooth the Spike- Heaviside Step function during backward pass to get a smooth gradient.\n",
    "# This smoothing function is Shifted(Centered at Uth) SIGMOID or FAST-SIGMOID \n",
    "# K is Hyperparameter, as K increases, Sigmoid function -> Heaviside Step function\n",
    "\n",
    "# FORWARD PASS - Determine S using Shifted Heaviside Function\n",
    "# BACKWARD PASS - Sigmoid. (or Spike Operator)\n",
    "\n",
    "spike_grad = surrogate.fast_sigmoid(slope=25)\n",
    "beta = 0.8\n",
    "num_steps = 50\n",
    "batch_size_train = 47  \n",
    "batch_size_test = 94 \n",
    "# reference self.lif1 = snn.Leaky(beta=beta, threshold=0.1) | snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "\n",
    "# Define Network   : This goes out of memory, seqeunctial does not!\n",
    "'''\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        self.conv1 = nn.Conv2d(1, 15, (2,25))\n",
    "        self.lif1 = snn.Leaky(beta=beta, threshold=0.1, spike_grad=spike_grad)\n",
    "        self.conv2 = nn.Conv2d(15, 15, (1,11))\n",
    "        self.lif2 = snn.Leaky(beta=beta, threshold=0.1, spike_grad=spike_grad)\n",
    "        self.conv3 = nn.Conv2d(15, 10, (1,5))\n",
    "        self.lif3 = snn.Leaky(beta=beta, threshold=0.1, spike_grad=spike_grad)\n",
    "        self.fc1 = nn.Linear(10*1*38, 2)\n",
    "        self.lif4 = snn.Leaky(beta=beta, threshold=0.1, spike_grad=spike_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden states and outputs at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky() \n",
    "        mem3 = self.lif3.init_leaky()\n",
    "        mem4 = self.lif4.init_leaky()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk4_rec = []\n",
    "        mem4_rec = []\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            cur1 = F.max_pool2d(self.conv1(x), (1,4))\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = F.max_pool2d(self.conv2(spk1), (1,4))\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            cur3 = F.max_pool2d(self.conv3(spk2), (1,4))\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            cur4 = self.fc1(spk3.view(batch_size_train, -1))\n",
    "            spk4, mem4 = self.lif4(cur4, mem4)\n",
    "\n",
    "            spk4_rec.append(spk4)\n",
    "            mem4_rec.append(mem4)\n",
    "\n",
    "        return torch.stack(spk4_rec), torch.stack(mem4_rec)\n",
    "        #'''\n",
    "net = nn.Sequential(nn.Conv2d(1, 15, (2,25)),\n",
    "                    snn.Leaky(beta=beta,threshold=1.0, spike_grad=spike_grad, init_hidden=True),\n",
    "                    nn.MaxPool2d((1,4)),\n",
    "                    nn.Conv2d(15, 15, (1,11)),\n",
    "                    #nn.BatchNorm2d(15),\n",
    "                    snn.Leaky(beta=beta,threshold=1.0, spike_grad=spike_grad, init_hidden=True),\n",
    "                    nn.MaxPool2d((1,4)),\n",
    "                    nn.Conv2d(15, 10, (1,5)),\n",
    "                    #nn.BatchNorm2d(10),\n",
    "                    snn.Leaky(beta=beta,threshold=1.0, spike_grad=spike_grad, init_hidden=True),\n",
    "                    nn.MaxPool2d((1,4)),\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(10*1*38, 2),\n",
    "                    snn.Leaky(beta=beta, threshold=1.0, spike_grad=spike_grad, init_hidden=True, output=True)\n",
    "                    ).to(device)\n",
    "\n",
    "\n",
    "net1 = nn.Sequential(nn.Conv2d(1, 15, (2,25)),\n",
    "                    nn.MaxPool2d((1,4)),\n",
    "                    snn.Leaky(beta=beta,threshold=0.7, spike_grad=None, init_hidden=True),\n",
    "                    nn.Conv2d(15, 15, (1,11)),\n",
    "                    nn.BatchNorm2d(15),\n",
    "                    nn.MaxPool2d((1,4)),\n",
    "                    snn.Leaky(beta=beta,threshold=0.7, spike_grad=None, init_hidden=True),\n",
    "                    nn.Conv2d(15, 10, (1,5)),\n",
    "                    nn.BatchNorm2d(10),\n",
    "                    nn.MaxPool2d((1,4)),\n",
    "                    snn.Leaky(beta=beta,threshold=0.7, spike_grad=None, init_hidden=True),\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(10*1*38, 2),\n",
    "                    snn.Leaky(beta=beta, threshold=0.7, spike_grad=None, init_hidden=True, output=True)\n",
    "                    ).to(device)#'''\n",
    "#net = Net().to(device)\n",
    "print(net)\n",
    "\n",
    "def print_batch_accuracy(data, targets,batch_size=batch_size_test ,train=False):\n",
    "    #output, _ = net(data.view(batch_size, -1))\n",
    "    output, mem_rec = forward_pass(net, num_steps, data)\n",
    "    check_all_output = output.sum(dim=0)\n",
    "    print('spk_rec values', check_all_output.size(),'Spike accumulated for both labels', check_all_output )\n",
    "    val, idx = output.sum(dim=0).max(1)\n",
    "    print('idx from',output.sum(dim=0).max(1), 'idx.size',idx.size(), 'targets', targets)\n",
    "    acc = np.mean((targets == idx).detach().cpu().numpy())\n",
    "    #print('acc', acc)\n",
    "\n",
    "    if train:\n",
    "        print(f\"Train set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "        acc_hist.append(acc.item())\n",
    "    else:\n",
    "        print(f\"Test set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "        test_acc_hist.append(acc.item())\n",
    "\n",
    "def train_printer():\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    print('Counter', counter)\n",
    "    print(f\"Train Set Loss: {loss_hist[counter]:.2f}\")\n",
    "    print(f\"Test Set Loss: {test_loss_hist[counter]:.2f}\")\n",
    "    print_batch_accuracy(data, targets, batch_size=batch_size_train, train=True)\n",
    "    print_batch_accuracy(test_data, test_targets, batch_size=batch_size_test,train=False)\n",
    "    print(f\"Train Accuracy: {acc_hist[counter]:.2f}\")\n",
    "    print(f\"Test Accuracy: {test_acc_hist[counter]:.2f}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "loss = SF.ce_rate_loss()\n",
    "#loss = nn.CrossEntropyLoss()\n",
    "\n",
    "#Cross Entropy spike rate loss at EVERY TIME STEP -Combines logStoftmax and NLLLoss.\n",
    "#The losses are accumulated over time steps to give the final loss and encourage firing of correct class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For one sample\n",
    "\n",
    "def forward_pass(net, num_steps, data):\n",
    "    mem_rec = []\n",
    "    spk_rec = []\n",
    "    utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        spk_out, mem_out = net(data)\n",
    "        spk_rec.append(spk_out)\n",
    "        mem_rec.append(mem_out)\n",
    "  \n",
    "    return torch.stack(spk_rec), torch.stack(mem_rec)\n",
    "\n",
    "data, targets = next(iter(train_loader))   # Normalized data\n",
    "data = data.to(device=device, dtype=torch.float32)\n",
    "#indices = torch.tensor([0])\n",
    "#data = torch.index_select(data, 2, indices)                      # Select only 1 channel\n",
    "#data = spikegen.delta(data, threshold=0.5)\n",
    "targets = targets.to(device=device, dtype=torch.int64)\n",
    "print('data', data.shape)\n",
    "#spk_rec, mem_rec = net(data.view(batch_size_train, -1))\n",
    "spk_rec, mem_rec = forward_pass(net, num_steps, data)\n",
    "print( 'spk_rec',spk_rec, 'mem_rec',mem_rec)\n",
    "print('targets', targets, 'spk_rec', spk_rec)\n",
    "\n",
    "# initialize the total loss value\n",
    "loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "loss_val = loss(spk_rec, targets)\n",
    "\n",
    "print('loss_val', loss_val, loss_val.size())\n",
    "print(f\"Training loss: {loss_val.item():.3f}\")\n",
    "\n",
    "#print_batch_accuracy(data, targets, train=True)\n",
    "_, idx = spk_rec.sum(dim=0).max(1)\n",
    "accuracy = np.mean((targets == idx).detach().cpu().numpy())\n",
    "acc = SF.accuracy_rate(spk_rec, targets)\n",
    "print('1st run idx from',spk_rec.sum(dim=0).max(1), 'idx.size',idx.size(), 'targets', targets, 'accuracy', acc)\n",
    "\n",
    "# clear previously stored gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# calculate the gradients\n",
    "loss_val.backward()\n",
    "\n",
    "# weight update\n",
    "optimizer.step()\n",
    "\n",
    "# calculate new network outputs using the same data\n",
    "#spk_rec, mem_rec = net(data.view(batch_size_train, -1))\n",
    "spk_rec, mem_rec = forward_pass(net, num_steps, data)\n",
    "print( 'spk_rec',spk_rec, 'mem_rec',mem_rec)\n",
    "print('targets', targets, 'spk_rec', spk_rec)\n",
    "\n",
    "# initialize the total loss value\n",
    "loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "    \n",
    "loss_val = loss(spk_rec, targets)\n",
    "\n",
    "print(f\"Training loss: {loss_val.item():.3f}\")\n",
    "#print_batch_accuracy(data, targets, train=True)#''' # end of one sample\n",
    "_, idx = spk_rec.sum(dim=0).max(1)\n",
    "accuracy = np.mean((targets == idx).detach().cpu().numpy())\n",
    "acc = SF.accuracy_rate(spk_rec, targets)\n",
    "print('2nd run idx from',spk_rec.sum(dim=0).max(1), 'idx.size',idx.size(), 'targets', targets, 'accuracy', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training on entire dataset\n",
    "\n",
    "num_epochs = 1\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "acc_hist = []\n",
    "test_acc_hist = []\n",
    "counter = 0\n",
    "indices = torch.tensor([0])\n",
    "\n",
    "def forward_pass(net, num_steps, data):\n",
    "    mem_rec = []\n",
    "    spk_rec = []\n",
    "    utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        spk_out, mem_out = net(data)\n",
    "        spk_rec.append(spk_out)\n",
    "        mem_rec.append(mem_out)\n",
    "  \n",
    "    return torch.stack(spk_rec), torch.stack(mem_rec)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "loss = SF.ce_rate_loss()\n",
    "\n",
    "# Outer training loop\n",
    "for epoch in range(num_epochs):\n",
    "    iter_counter = 0\n",
    "    \n",
    "    # Minibatch training loop\n",
    "    for data, targets in iter(train_loader):\n",
    "        data = data.to(device=device, dtype=torch.float32)\n",
    "        #data = torch.index_select(data, 2, indices)\n",
    "        print('data', data.shape, data)\n",
    "        check_nan = torch.isnan(data)\n",
    "        #print('idx for nan in training',tf.where(check_nan))\n",
    "        print('does input have nan training', torch.count_nonzero(check_nan))\n",
    "        if (torch.count_nonzero(check_nan) != 0):\n",
    "            corrupted_batch = (tf.where(check_nan))[0][0]\n",
    "            #print('corrupted data', data.shape,'corrupt batch',corrupted_batch)\n",
    "            for i in range(0,47):\n",
    "                print('check if data is corrupt',i,data[i])\n",
    "        #print('data',data.size(), data)\n",
    "        #data = spikegen.delta(data, threshold=0.5)\n",
    "        #print('spikegen data',data.size(), data)\n",
    "        targets = targets.to(device=device, dtype=torch.int64)\n",
    "        #print('targets',targets.size(), targets)\n",
    "        \n",
    "        print('######## DURING TRAINING ############')\n",
    "        # forward pass\n",
    "        net.train()\n",
    "        #print('data',data.shape, 'input data shape', data.view(batch_size_train, -1).shape)\n",
    "        #spk_rec, mem_rec = net(data.view(batch_size_train, -1))\n",
    "        spk_rec, mem_rec = forward_pass(net, num_steps, data)\n",
    "        print( 'spk_rec',spk_rec, 'mem_rec',mem_rec)\n",
    "        print('targets', targets, 'spk_rec', spk_rec)\n",
    "        \n",
    "        #print('spk_rec',spk_rec.size(), 'mem_rec',mem_rec.size())\n",
    "        #print('spk_rec',spk_rec, 'mem_rec',mem_rec)\n",
    "        #for name, param in net.named_parameters():\n",
    "        #    if param.requires_grad:\n",
    "        #        print('Name of parameter',name, 'Parameter data', (param.data).shape,param.data)\n",
    "        #print('Non-zero elements in spk_rec',torch.count_nonzero(spk_rec))\n",
    "        \n",
    "        # initialize the loss & sum over time\n",
    "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "        for step in range(num_steps):\n",
    "            #print('Check loss dims', num_steps, mem_rec[step].shape, targets.shape)\n",
    "            #loss_val += loss(mem_rec[step], targets)   # Used loss for FCN\n",
    "            loss_val += loss(spk_rec, targets)\n",
    "        #print('Loss val for train ', loss_val)\n",
    "        #loss_val = loss(spk_rec, targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "        \n",
    "        #train_printer()\n",
    "        \n",
    "        # Test set\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            test_data, test_targets = next(iter(test_loader))\n",
    "            test_data = test_data.to(device=device, dtype=torch.float32)\n",
    "            #test_data = torch.index_select(test_data, 2, indices)\n",
    "            print('test_data', test_data.shape, test_data)\n",
    "            check_nan = torch.isnan(test_data)\n",
    "            print('idx for nan in testing',tf.where(check_nan))\n",
    "            print('does input have nan testing', torch.count_nonzero(check_nan))\n",
    "            #test_data = spikegen.delta(test_data, threshold=0.5)\n",
    "            test_targets = test_targets.to(device=device, dtype=torch.int64)\n",
    "            \n",
    "            print('######## DURING TESTING ############')\n",
    "            # Test set forward pass\n",
    "            #print('test_data',test_data.shape, 'input test_data', test_data.view(batch_size_test, -1).shape)\n",
    "            #test_spk, test_mem = net(test_data.view(batch_size_test, -1))\n",
    "            test_spk, test_mem = forward_pass(net, num_steps, test_data)\n",
    "            #print('spk_rec',test_spk.size(), 'mem_rec',test_mem.size())\n",
    "            #print('spk_rec',test_spk, 'mem_rec',test_mem)\n",
    "            #for name, param in net.named_parameters():\n",
    "            #    if param.requires_grad:\n",
    "            #        print('Name of parameter',name, 'Parameter data', (param.data).shape,param.data)\n",
    "            \n",
    "            # Test set loss\n",
    "            test_loss = torch.zeros((1), dtype=dtype, device=device)\n",
    "            for step in range(num_steps):\n",
    "                test_loss += loss(test_spk, test_targets)\n",
    "            #print('Loss val for test ', test_loss)\n",
    "            test_loss_hist.append(test_loss.item())\n",
    "\n",
    "            # Print train/test loss/accuracy\n",
    "            #if counter % 10 == 0:\n",
    "            train_printer()\n",
    "            counter += 1\n",
    "            #iter_counter +=1\n",
    "            #print('counter', counter, 'iter_counter', iter_counter)\n",
    "        print('####### END OF AN ITERATION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "plt.plot(loss_hist)\n",
    "plt.plot(test_loss_hist)\n",
    "plt.title(\"Loss Curves\")\n",
    "plt.legend([\"Train Loss\", \"Test Loss\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "# Plot Accuracy\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "plt.plot(acc_hist)\n",
    "plt.plot(test_acc_hist)\n",
    "plt.title(\"Accuracy Curves\")\n",
    "plt.legend([\"Train accuracy\", \"Test Accuracy\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this time, we won't return membrane as we don't need it \n",
    "def forward_pass(net, data):  \n",
    "    spk_rec = []\n",
    "    mem_rec =[]\n",
    "    utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
    "\n",
    "    #for step in range(data.size(0)):  # data.size(0) = number of time steps\n",
    "    #    spk_out, mem_out = net(data[step])\n",
    "    for step in range(num_steps):  # data.size(0) = number of time steps\n",
    "        spk_out, mem_out = net(data)\n",
    "        mem_rec.append(mem_out)\n",
    "        spk_rec.append(spk_out)\n",
    "  \n",
    "    return torch.stack(spk_rec), torch.stack(mem_rec)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=2e-2, betas=(0.9, 0.999))\n",
    "loss_fn = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)\n",
    "#loss_fn = SF.ce_rate_loss()\n",
    "\n",
    "\n",
    "num_epochs = 3\n",
    "num_iters = 50\n",
    "\n",
    "loss_hist = []\n",
    "acc_hist = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, targets) in enumerate(iter(train_loader)):\n",
    "        data = data.to(device=device, dtype=torch.float32)\n",
    "        targets = targets.to(device=device, dtype=torch.int64)\n",
    "\n",
    "        net.train()\n",
    "        spk_rec, mem_rec = forward_pass(net, data)\n",
    "        #for name, param in net.named_parameters():\n",
    "        #    if param.requires_grad:\n",
    "        #        print('Name of parameter',name, 'Parameter data', (param.data).shape,param.data)\n",
    "        print('mem_rec', spk_rec, mem_rec, 'Truth', targets)\n",
    "        loss_val = loss_fn(spk_rec, targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    " \n",
    "        print(f\"Epoch {epoch}, Iteration {i} \\nTrain Loss: {loss_val.item():.2f}\")\n",
    "\n",
    "        acc = SF.accuracy_rate(spk_rec, targets) \n",
    "        acc_hist.append(acc)\n",
    "        print(f\"Accuracy: {acc * 100:.2f}%\\n\")\n",
    "\n",
    "        # This will end training after 50 iterations by default\n",
    "        if i == num_iters:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchsummary\n",
    "# neuron and simulation parameters\n",
    "spike_grad = surrogate.fast_sigmoid(slope=25)\n",
    "beta = 0.5\n",
    "num_steps = 50\n",
    "#device = 'cuda'\n",
    "\n",
    "from torchsummary import summary\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(device)\n",
    "\n",
    "net = nn.Sequential(nn.Conv2d(1, 15, (2,25)),\n",
    "                    nn.MaxPool2d((1,4)),\n",
    "                    snn.Leaky(beta=beta,threshold=1.0, spike_grad=spike_grad, init_hidden=True),\n",
    "                    nn.Conv2d(15, 15, (1,11)),\n",
    "                    nn.BatchNorm2d(15),\n",
    "                    nn.MaxPool2d((1,4)),\n",
    "                    snn.Leaky(beta=beta,threshold=1.0, spike_grad=spike_grad, init_hidden=True),\n",
    "                    nn.Conv2d(15, 10, (1,5)),\n",
    "                    nn.BatchNorm2d(10),\n",
    "                    nn.MaxPool2d((1,4)),\n",
    "                    snn.Leaky(beta=beta,threshold=1.0, spike_grad=spike_grad, init_hidden=True),\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(10*1*38, 2),\n",
    "                    snn.Leaky(beta=beta, threshold=1.0, spike_grad=spike_grad, init_hidden=True, output=True)\n",
    "                    ).to(device)\n",
    "\n",
    "#summary(net.to(device), ( 1, 2, 2560), 180)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "plt.plot(loss_hist)\n",
    "plt.plot(test_loss_hist)\n",
    "plt.title(\"Loss Curves\")\n",
    "plt.legend([\"Train Loss\", \"Test Loss\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "# drop_last switched to False to keep all samples\n",
    "#test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    for name, param in net.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(name, param.data) \n",
    "    for data, targets in test_loader:\n",
    "        data = data.to(device=device, dtype=torch.float32)\n",
    "        targets = targets.to(device=device, dtype=torch.int64)\n",
    "    \n",
    "        # forward pass\n",
    "        test_spk, _ = net(data.view(data.size(0), -1))\n",
    "\n",
    "        # calculate total accuracy\n",
    "        _, predicted = test_spk.sum(dim=0).max(1)\n",
    "        total += targets.size(0)\n",
    "        print('Targets', targets, 'Prediction', predicted)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "print(f\"Total correctly classified test set images: {correct}/{total}\")\n",
    "print(f\"Test Set Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiments with Data Encoding\n",
    "from snntorch import spikegen\n",
    "from snntorch import utils \n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "num_steps = 25\n",
    "\n",
    "for i, (data, targets) in enumerate(iter(test_loader)):\n",
    "        data = data.to(device=device, dtype=torch.float32)\n",
    "        print('data',  data.size())\n",
    "        targets = targets.to(device=device, dtype=torch.int64)\n",
    "        print('targets', targets.size())\n",
    "        '''\n",
    "        #Delta Modulation\n",
    "        spike_data = spikegen.delta(data, threshold=0.1)\n",
    "\n",
    "        # Create fig, ax\n",
    "        fig = plt.figure(facecolor=\"w\", figsize=(8, 1))\n",
    "        ax = fig.add_subplot(111)\n",
    "\n",
    "        # Raster plot of delta converted data\n",
    "        splt.raster(spike_data[0,0,0,:], ax)\n",
    "\n",
    "        plt.title(\"Input Neuron\")\n",
    "        plt.xlabel(\"Time step\")\n",
    "        plt.yticks([])\n",
    "        #plt.xlim(0, len(data))\n",
    "        plt.show()\n",
    "        '''\n",
    "        \n",
    "        #Rate Encode\n",
    "        spike_data_delta = spikegen.delta(data, threshold=1.0)\n",
    "        spike_data = spikegen.rate(spike_data_delta, num_steps=20)\n",
    "        print('spike_data', spike_data.size())\n",
    "        \n",
    "        spike_data_sample = spike_data[:, 84, 0]\n",
    "        #print(spike_data_sample.size())\n",
    "        #fig, ax = plt.subplots()\n",
    "        #anim = splt.animator(spike_data_sample, fig, ax)\n",
    "        #HTML(anim.to_html5_video())\n",
    "        \n",
    "        spike_data_sample2 = spike_data_sample.reshape((num_steps, -1))\n",
    "        print('targets',targets, targets.size())\n",
    "        print('spike_data_sample2', spike_data_sample2, spike_data_sample2.size())\n",
    "\n",
    "        # raster plot\n",
    "        fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "        ax = fig.add_subplot(111)\n",
    "        splt.raster(spike_data_sample2, ax, s=1.5, c=\"black\")\n",
    "\n",
    "        plt.title(\"Input Layer\")\n",
    "        plt.xlabel(\"Time step\")\n",
    "        plt.ylabel(\"Neuron Number\")\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training SNN 1\n",
    "from snntorch import utils \n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from IPython.display import HTML\n",
    "\n",
    "def analog_to_spike(batch_x, batch_y):\n",
    "    #print('shape for analog to spike',np.shape(batch_x), np.shape(batch_x)[0])\n",
    "    b_s = np.shape(batch_x)[0]\n",
    "    c_s = np.shape(batch_x)[2]\n",
    "    t_s = np.shape(batch_x)[3]\n",
    "    \n",
    "    #batch_x = spikegen.delta(batch_x, threshold=0.4, padding=True, off_spike=False) # Better visualization at 0.4 than 0.5\n",
    "    batch_x = spikegen.rate(batch_x, num_steps=2 ,gain=1, offset=0, first_spike_time=0, time_var_input=False )\n",
    "    '''\n",
    "    for b in range(0,1):\n",
    "        for c in range(0,c_s):    \n",
    "            # Create fig, ax\n",
    "            fig = plt.figure(facecolor=\"w\", figsize=(8, 4))\n",
    "            ax = fig.add_subplot(111)\n",
    "\n",
    "            # Raster plot of delta converted data\n",
    "            splt.raster(batch_x[b,0,c,:], ax)\n",
    "\n",
    "            plt.title(\"Input Neuron \"+ \"for batchsample_\"+ str(b) +\"_for channel_\"+ str(c) + \"_with_label:\" +str(batch_y[b]))\n",
    "            plt.xlabel(\"Time step\")\n",
    "            plt.yticks([])\n",
    "            plt.xlim(0,t_s )\n",
    "            fig.canvas.draw()\n",
    "        break #'''\n",
    "   \n",
    "    return batch_x\n",
    "\n",
    "def forward_pass(net, data, num_steps):  \n",
    "    spk_rec = []\n",
    "    mem_rec = []\n",
    "    utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
    "\n",
    "    for step in range(num_steps): \n",
    "        spk_out, mem_out = net(data)\n",
    "        spk_rec.append(spk_out)\n",
    "        mem_rec.append(mem_out)\n",
    "    #print('spk_rec',torch.stack(spk_rec).size(), 'mem_rec',torch.stack(mem_rec).size()) [10, 180, 2]\n",
    "    return torch.stack(spk_rec), torch.stack(mem_rec)\n",
    "\n",
    "import snntorch.functional as SF\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=2e-3, betas=(0.9, 0.999))\n",
    "#loss_fn = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)\n",
    "loss_fn = SF.ce_max_membrane_loss()\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "num_steps = 10  # run for 25 time steps \n",
    "\n",
    "loss_hist = []\n",
    "acc_hist = []\n",
    "#train_loss = np.zeros(len(loader_train))\n",
    "#y_pred_all, y_true_all = list(), list()\n",
    "#metric = balanced_accuracy_score\n",
    "\n",
    "# Testing loop\n",
    "def test_accuracy(data_loader, net, num_steps):\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        acc = 0\n",
    "        net.eval()\n",
    "        #for name, param in net.named_parameters():\n",
    "        #    if param.requires_grad:\n",
    "        #        print( name, param.data)\n",
    "                \n",
    "        data_loader = iter(data_loader)\n",
    "        for data, targets in data_loader:\n",
    "            data = data.to(device=device, dtype=torch.float32)\n",
    "            targets = targets.to(device=device, dtype=torch.int64)\n",
    "            data = analog_to_spike(data,targets)\n",
    "            spk_rec, _ = forward_pass(net, data, num_steps)  #spk_rec is [10, 256, 2]\n",
    "            #print('Test accuracy vals', spk_rec.size(), 'targets',targets.size(), 'size',spk_rec.size(1))\n",
    "            _, idx = spk_rec.sum(dim=0).max(1)\n",
    "            #print('target test', targets, 'idx test', idx)\n",
    "            accuracy = np.mean((targets == idx).detach().cpu().numpy())\n",
    "            #print('Decoded accuracy', accuracy)\n",
    "            acc += SF.accuracy_rate(spk_rec, targets) * spk_rec.size(1)\n",
    "            total += spk_rec.size(1)\n",
    "    print('Accuracy', acc, 'Total', total)\n",
    "    #print(f\"Accuracy testing: {acc/total:.2f}%\")\n",
    "\n",
    "    return acc/total\n",
    "# TODO : data (torch.Tensor) – Data tensor for a single batch of shape [num_steps x batch x input_size]\n",
    "# or use Rate data (torch.Tensor) – Data tensor for a single batch of shape [batch x input_size]x\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, targets) in enumerate(iter(loader_train)):\n",
    "        data = data.to(device=device, dtype=torch.float32)\n",
    "        targets = targets.to(device=device, dtype=torch.int64)\n",
    "        data = analog_to_spike(data,targets)\n",
    "        \n",
    "        net.train()\n",
    "        spk_rec, _ = forward_pass(net, data, num_steps)\n",
    "        \n",
    "        loss_val = loss_fn(spk_rec, targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #y_pred_all.append(torch.argmax(spk_rec, axis=1).cpu().numpy())\n",
    "        #y_true_all.append(targets.cpu().numpy())\n",
    "        \n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "        #train_loss[i] = loss_val.item()\n",
    "\n",
    "        # print every 25 iterations\n",
    "        if i % 25 == 0:\n",
    "            print(f\"Epoch {epoch}, Iteration {i} \\nTrain Loss: {loss_val.item():.2f}\")\n",
    "            _, idx = spk_rec.sum(dim=0).max(1)\n",
    "            print('target train', targets, 'idx train', idx)\n",
    "            # check accuracy on a single batch\n",
    "            acc = SF.accuracy_rate(spk_rec, targets)  \n",
    "            acc_hist.append(acc)\n",
    "            #print(f\"Accuracy training: {acc * 100:.2f}\")\n",
    "            \n",
    "    #print(net)        \n",
    "    print(f\"Test set accuracy epoch: {test_accuracy(loader_test, net, num_steps)*100:.3f}%\")     \n",
    "    \n",
    "    #y_pred = np.concatenate(y_pred_all)\n",
    "    #y_true = np.concatenate(y_true_all)\n",
    "    #perf = metric(y_true, y_pred)\n",
    "    #print('Perf',np.mean(train_loss) , 'y_pred',y_pred,'y_true', y_true,'len', np.shape(y_pred))\n",
    "    #print('loss_hist', loss_hist, len(loss_hist))\n",
    "    print('End of Epoch\\n')\n",
    "'''\n",
    "#With time_var = True\n",
    "\n",
    "from snntorch import backprop\n",
    "\n",
    "num_epochs = 3\n",
    "#print('Num_steps is defined?', num_steps)\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for data, target in loader_train:\n",
    "        data = data.to(device=device, dtype=torch.float32)\n",
    "        target = target.to(device=device, dtype=torch.int64)\n",
    "        data = analog_to_spike(data,target)\n",
    "        print(data.size(), target.size())\n",
    "        loss = backprop.RTRL(net, data, target, optimizer, loss_fn, num_steps=False, time_var=True, device=device)\n",
    "\n",
    "    print(f\"Epoch {epoch}, Train Loss: {loss.item():.2f}\")\n",
    "'''\n",
    "\n",
    "#print(f\"Test set accuracy: {test_accuracy(loader_test, net, num_steps)*100:.3f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot statistics for SNN1\n",
    "\n",
    "# Visualizing the learning curves\n",
    "history_df = pd.DataFrame(history)\n",
    "ax1 = history_df.plot(x='epoch', y=['train_loss', 'valid_loss'], marker='o')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax2 = history_df.plot(x='epoch', y=['train_perf', 'valid_perf'], marker='o')\n",
    "ax2.set_ylabel('balanced_accuracy_score')\n",
    "\n",
    "# VISUALIZE THE RESULTS\n",
    "classes_mapping = {0: 'non-ictal', 1: 'ictal'}\n",
    "y_train = pd.Series([y for _, y in train_ds]).map(classes_mapping)\n",
    "#Visualizing the class distribution, Very imbalanced right now #TODO\n",
    "#ax_ = y_train.value_counts().plot(kind='barh')\n",
    "#ax_.set_xlabel('Number of training examples');\n",
    "#ax_.set_ylabel('Sleep stage');\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(conf_mat, classes_mapping):\n",
    "    ticks = list(classes_mapping.keys())\n",
    "    tick_labels = classes_mapping.values()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    im = ax.imshow(conf_mat, cmap='Reds')\n",
    "\n",
    "    ax.set_yticks(ticks)\n",
    "    ax.set_yticklabels(tick_labels)\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(tick_labels)\n",
    "    ax.set_ylabel('True label')\n",
    "    ax.set_xlabel('Predicted label')\n",
    "    ax.set_title('Confusion matrix')\n",
    "\n",
    "    for i in range(len(ticks)):\n",
    "        for j in range(len(ticks)):\n",
    "            text = ax.text(\n",
    "                j, i, conf_mat[i, j], ha='center', va='center', color='k')\n",
    "\n",
    "    fig.colorbar(im, ax=ax, fraction=0.05, label='# examples')\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "conf_mat = confusion_matrix(y_true, y_pred)\n",
    "plot_confusion_matrix(conf_mat, classes_mapping);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP SNN coding suggestions here \n",
    "'''\n",
    "from snntorch import surrogate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "beta = 0.9\n",
    "\n",
    "# Initialize surrogate gradient\n",
    "spike_grad = surrogate.fast_sigmoid()  # passes default parameters from a closure\n",
    "\n",
    "# Define Network\n",
    "class Net(nn.Module):\n",
    " def __init__(self):\n",
    "     super().__init__()\n",
    "\n",
    " # Initialize layers, specify the ``spike_grad`` argument\n",
    "     self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "     self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "     self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "     self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "         lif1 = snn.Leaky(beta=0.9, init_hidden=True) # only returns spk\n",
    "         lif2 = snn.Leaky(beta=0.9, init_hidden=True, output=True) # returns mem and spk if output=True\n",
    "\n",
    "When initializing neurons, set init_hidden=True. This enables the methods in snntorch.\n",
    "backprop to automatically clear the hidden state variables, as well as detach them from the computational graph when necessary.\n",
    " \n",
    " def forward(self, x, syn1, mem1, spk1, syn2, mem2):\n",
    "     cur1 = self.fc1(x)\n",
    "     spk1, mem1 = self.lif1(cur1, mem1)\n",
    "     cur2 = self.fc2(spk1)\n",
    "     spk2, mem2 = self.lif2(cur2, mem2)\n",
    "     return mem1, spk1, mem2, spk2\n",
    "\n",
    " net = Net().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=betas)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=2e-3, betas=(0.9, 0.999))\n",
    "loss_fn = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)\n",
    "\n",
    "# Time-variant input data\n",
    "for input, target in dataset:\n",
    "   loss = BPTT(net, input, target, num_steps, batch_size, optimizer, criterion)  # THIS IS WHAT I SHOULD USE!\n",
    "\n",
    "# Time-invariant input data\n",
    "for input, targets in dataset:\n",
    "   loss = BPTT(net, input, target, num_steps, batch_size, optimizer, criterion, time_varying_data=False)\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "For making alpha and beta a learnable parameter for each layer than hyperparameter\n",
    "\n",
    "The same approach as above can be used for implementing learnable thresholds, using learn_threshold=True.\n",
    "\n",
    "Each neuron has the option to inhibit other neurons within the same dense layer from firing. \n",
    "This can be invoked by setting inhibition=True. Only with FC layer\n",
    "\n",
    "         self.lif1 = snn.Synaptic(alpha=alpha, beta=beta,\n",
    "                                 spike_grad=spike_grad,\n",
    "                                 threshold=0.5, learn_alpha=True,\n",
    "                                 learn_beta=True)\n",
    "        self.lif2 = snn.Synaptic(alpha=alpha, beta=beta,\n",
    "                                 spike_grad=spike_grad,\n",
    "                                 threshold=0.5, learn_alpha=True,\n",
    "                                 learn_beta=True)\n",
    "        self.lif3 = snn.Synaptic(alpha=alpha, beta=beta,\n",
    "                                 spike_grad=spike_grad,\n",
    "                                 threshold=0.5, learn_alpha=True,\n",
    "                                 learn_beta=True)\n",
    "        self.lif4 = snn.Synaptic(alpha=alpha, beta=beta,\n",
    "                                 spike_grad=spike_grad,\n",
    "                                 threshold=0.5, learn_alpha=True,\n",
    "                                 learn_beta=True)\n",
    "        self.lif5 = snn.Synaptic(alpha=alpha, beta=beta,\n",
    "                                 spike_grad=spike_grad,\n",
    "                                 threshold=0.5, learn_alpha=True,\n",
    "                                 learn_beta=True)\n",
    "############################################################################\n",
    "\n",
    "beta = 0.9\n",
    "a = 1\n",
    "\n",
    "threshold = nn.Parameter( a )\n",
    "lif = snn.Leaky(beta, threshold=threshold)\n",
    "\n",
    ". Alternatively, a = torch.rand((num_outputs), dtype=torch.float32) will let you set individual thresholds per neuron.\n",
    "\n",
    "self.register_parameter(name = \"threshold\", param = threshold)\n",
    "############################################################################\n",
    "NOTE TOD: Try out different type of backprops \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "loss_fn = SF.mse_count_loss()\n",
    "reg_fn = SF.l1_rate_sparsity()\n",
    "\n",
    "\n",
    "# train_loader is of type torch.utils.data.DataLoader\n",
    "# if input data is time-static, set time_var=False, and specify num_steps.\n",
    "# if input data is time-varying, set time_var=True and do not specify num_steps. (only for backprop)\n",
    "\n",
    "for epoch in range(5):\n",
    "    loss = backprop.RTRL(net, train_loader, optimizer=optimizer,\n",
    "    criterion=loss_fn, num_steps=num_steps, time_var=False,\n",
    "    regularization=reg_fn, device=device)\n",
    "    \n",
    "############################################################################\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
