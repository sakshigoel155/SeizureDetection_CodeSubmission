{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices(\"GPU\")\n",
    "print(torch.__version__)\n",
    "print(tf.__version__)\n",
    "print(torch.cuda.get_arch_list())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pd.__version__) 1.4.1\n",
    "#print(np.__version__) 1.21.2\n",
    "#print(matplotlib.__version__) 3.5.1\n",
    "#print(sklearn.__version__) 1.0.2\n",
    "#print(mne.__version__) 0.24.1\n",
    "#'Corrections, to_data_frame, get_epoch , no of output neuron'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = './patient_info_txt/data_files_'\n",
    "csv_file_path = './patient_info_txt/Invasive_data_'\n",
    "patient_id = [115,139,253,264,442,583,620,635,862,958,1146]\n",
    "p_id = 2 \n",
    "interval = 2\n",
    "test_list = [80] #7[12,14] #8[106,109] #9[80,81]\n",
    "soz_channel_list = {                      #seizure\n",
    "    115 : [\"HR3\",\"HR4\",\"HR5\",\"HR6\"],      #26\n",
    "    139 : [\"HL2\",\"HL4\",\"HL3\",\"TBA1\"],     #6\n",
    "    253 : [\"HRB2\",\"HRC2\",\"HRB3\",\"HRC3\"],  #7  \n",
    "    264 : [\"BLA1\",\"BLC1\",\"BRA1\",\"TRA3\"],  #8   \n",
    "    442 : [\"HRA4\",\"TBA1\",\"HRA5\",\"TBA2\"],  #22  \n",
    "    583 : [\"TLB1\",\"TLA1\",\"HL1\",\"TBA1\"],   #23  \n",
    "    620 : [\"TLA1\",\"TLB2\",\"TLB3\",\"TLA2\"],  #7   yes\n",
    "    635 : [\"HL1\",\"HL9\",\"HRA1\",\"HRA2\"],     #21  yes\n",
    "    862 : [\"IHB2\",\"IHB3\",\"IHB1\",\"GC7\"],   #9    yes\n",
    "    958 : [\"GE3\",\"GH6\",\"OPL5\",\"TBB2\"],    #16  yes\n",
    "    1146 : [\"ICL1\",\"SCL7\",\"SCL8\",\"SCR5\"], #26\n",
    "}\n",
    "print(soz_channel_list[patient_id[p_id]], [patient_id[p_id]], test_list)\n",
    "patient_channels = soz_channel_list[patient_id[p_id]]\n",
    "#index for function - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import mne\n",
    "from mne.time_frequency import psd_welch\n",
    "import csv\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "datetimeFormat = '%Y-%m-%d %H:%M:%S.%f'\n",
    "import re\n",
    "import imblearn\n",
    "\n",
    "from numpy.fft import fft, fftfreq\n",
    "from scipy import signal\n",
    "from mne.time_frequency import tfr_morlet\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "%matplotlib notebook\n",
    "mne.set_log_level('ERROR') \n",
    "\n",
    "\n",
    "#path_to_data = './patient_info_txt/data_files_442.txt'\n",
    "#path_to_csv = './patient_info_txt/Invasive_data_442.csv'\n",
    "\n",
    "path_to_data = data_file_path + str(patient_id[p_id]) + '.txt'\n",
    "path_to_csv = csv_file_path + str(patient_id[p_id]) + '.csv'\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "def square_data(inputs):\n",
    "    data_df = pd.Series(inputs)\n",
    "    data_df_sq = data_df.pow(2)\n",
    "    data_df_np = data_df_sq.to_numpy()\n",
    "    return data_df_np\n",
    "\n",
    "def normalize_data(inputs):\n",
    "    data_df_HRC1 = pd.Series(inputs)\n",
    "    data_df_std_dev = data_df_HRC1.rolling(time_steps).std() # 30 seconds\n",
    "    data_df_norm = data_df_HRC1.divide(data_df_std_dev)  \n",
    "    data_df_norm[:time_steps-1].update(data_df_HRC1[:time_steps-1]) # if NaN, norm = original value\n",
    "    data_df_HRC1_np = data_df_norm.to_numpy()\n",
    "    return data_df_HRC1_np\n",
    "\n",
    "def diff_data(inputs):  #TODO : Do this before rereferencing\n",
    "    data_df = pd.Series(inputs)\n",
    "    data_df_diff= data_df.diff()\n",
    "    data_df_np = data_df_diff.to_numpy()\n",
    "    return data_df_np\n",
    "\n",
    "def dur(num):\n",
    "    if num/60 > 1:\n",
    "        return 60.0\n",
    "    else:\n",
    "        return float(num)\n",
    "\n",
    "#Reref    \n",
    "#HRA = ['HRA1','HRA2','HRA3','HRA4','HRA5']\n",
    "#HRB = ['HRB1','HRB2','HRB3','HRB4','HRB5','HRC1','HRC2','HRC3','HRC4','HRC5']\n",
    "#HRC = ['HRB1','HRB2','HRB3','HRB4','HRB5','HRC1','HRC2','HRC3','HRC4','HRC5']\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA-enabled GPU found. Training should be faster.')\n",
    "else:\n",
    "    print('No GPU found. Training will be slow')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device = 'cpu'\n",
    "my_database ={'Patient 1':{'PatientID': 0, 'FileID': [], 'Frequency': 512, 'Duration': 0}}\n",
    "\n",
    "print('HERE WE GO!!!!!!!!!!!!!!')\n",
    "with open(path_to_data) as file:\n",
    "    lines = file.readlines()\n",
    "    lines_iter = iter(lines)\n",
    "    lines = [line.rstrip() for line in lines]\n",
    "    #print('lines', lines, len(lines))\n",
    "            \n",
    "def load_raw_dataset(line, my_database):\n",
    "    \"\"\"Load a recording from the iEEG dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_fname : str\n",
    "        Path to the .data file containing the raw data.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mne.io.Raw :\n",
    "        Raw object containing the EEG. \n",
    "    \"\"\"\n",
    "    #for line in range(0,len(lines)):\n",
    "    print(line, type(line))\n",
    "    #if not line.startswith(\"#\"):\n",
    "    entry = 'EEG_inv_' + str(line)[-18:-15] + '_' + str(line)[-9:-5]\n",
    "    print('Value for entry',entry)\n",
    "    my_database['Patient 1']['PatientID'] = str(line)[-18:-15]\n",
    "    my_database['Patient 1']['FileID'] = str(line)[-9:-5]\n",
    "    print(my_database)\n",
    "    data_raw =  mne.io.read_raw_nicolet(str(line), 'eeg', preload=True)\n",
    "    \n",
    "    subj_num, rec_num = int(str(line)[-18:-15]), int(str(line)[-9:-5])\n",
    "    print('subj_num',subj_num ,'rec_num',rec_num)\n",
    "    data_raw.info['subject_info'] = {'id': subj_num, 'rec_id': rec_num}\n",
    "    return data_raw\n",
    "\n",
    "# Check \n",
    "#load_raw_dataset(lines,my_database)\n",
    "# Load recordings\n",
    "data_raws = [load_raw_dataset(line, my_database) for line in lines]\n",
    "print('Database entries',len(data_raws) , type(data_raws) )\n",
    "\n",
    "#for idx in range(0,len(data_raws)):\n",
    "#    print(data_raws[idx].info)\n",
    "#    print(data_raws[idx].plot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#data_raws[1].plot()\n",
    "#data_raws[6].plot()\n",
    "#data_raws[8].plot()\n",
    "#data_raws[10].plot()\n",
    "# Add annotation to data - onset, duration, description (in s), orig_time 0 meas_date\n",
    "'''\n",
    "my_annot = mne.Annotations(onset= 22*60,  # in seconds\n",
    "                           duration= 21,  # in seconds, too\n",
    "                           description='True Prediction')\n",
    "print(my_annot)\n",
    "data_raws[1].set_annotations(my_annot)\n",
    "print(data_raws[1].annotations)\n",
    "data_raws[1].plot()'''\n",
    "\n",
    "my_annot = mne.Annotations(onset= 1*60,  # in seconds\n",
    "                           duration= 80,  # in seconds, too\n",
    "                           description='True Prediction')\n",
    "print(my_annot)\n",
    "data_raws[15].set_annotations(my_annot)\n",
    "print(data_raws[15].annotations)\n",
    "data_raws[15].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set reference channel for all data points for all channels individually\n",
    "def set_reference(data_raw, p_id):\n",
    "    data_array = []\n",
    "    sfreq = data_raw.info['sfreq']\n",
    "    print('sfreq', sfreq)\n",
    "    print('4 electrodes for the patient',soz_channel_list[patient_id[p_id]])\n",
    "    patient_channels = soz_channel_list[patient_id[p_id]]\n",
    "    print('patient channels',patient_channels)\n",
    "    all_channels = data_raw.ch_names\n",
    "    #print('all channels',all_channels)\n",
    "    for item in patient_channels:\n",
    "        #print('For channel', item)\n",
    "        ch_group = \" \".join(re.findall(\"[a-zA-Z]+\", item))\n",
    "        ch_group = str(ch_group)\n",
    "        ch_group = [x for x in all_channels if re.search(ch_group, x)]\n",
    "        #print('Electrode group', str(ch_group))\n",
    "        data_ch_dict = data_raw.copy()\n",
    "        data_ch_dict = data_ch_dict.pick_channels(ch_names= ch_group, ordered=False)\n",
    "        data_ch_dict.set_eeg_reference(ref_channels= ch_group)\n",
    "        data_ch_dict = data_ch_dict.pick_channels(ch_names= [item], ordered=False)\n",
    "        df_channel = data_ch_dict.to_data_frame()\n",
    "        #print('Each channel', df_channel)\n",
    "        #print('Info', data_ch_dict.info)\n",
    "        df_channel_array = np.array(df_channel[item])\n",
    "        #print(df_channel_array, df_channel_array.shape)\n",
    "        data_array.append(df_channel_array)\n",
    "        \n",
    "    reref_data = np.array(data_array)\n",
    "    print('reref data', reref_data, reref_data.shape)\n",
    "    info = mne.create_info(ch_names= patient_channels, sfreq=sfreq, ch_types = ['eeg']* 4)\n",
    "    print('Assert',len(reref_data), len(info['ch_names']), patient_channels)\n",
    "    simulated_raw = mne.io.RawArray(reref_data, info)\n",
    "    #print(simulated_raw.info)\n",
    "    return simulated_raw \n",
    "\n",
    "reref_raws = []\n",
    "for idx in range(0,len(data_raws)):\n",
    "    #print('BEFORE RE-REFERENCING',data_raws[idx].info)\n",
    "    #data_raws[idx].plot()\n",
    "    reref_raws.append(set_reference(data_raws[idx],p_id))\n",
    "    #reref_raws[idx] = set_reference(data_raws[idx],2)\n",
    "    #print('AFTER RE-REFERENCING',reref_raws[idx].info)\n",
    "    #reref_raws[idx].plot()\n",
    "    #break\n",
    "\n",
    "print(reref_raws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use notch filter \n",
    "#https://mne.tools/stable/generated/mne.io.Raw.html#mne.io.Raw.notch_filter\n",
    "for idx in range(0,len(reref_raws)):\n",
    "    print(reref_raws[idx].info)\n",
    "    break\n",
    "print(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_(t_sample, meas_data):\n",
    "    #time = float(datetime.datetime.strptime(t_sample, datetimeFormat))\n",
    "    time = float((datetime.datetime.strptime(t_sample, datetimeFormat)- \n",
    "                 datetime.datetime.strptime(meas_data, datetimeFormat)).seconds)\n",
    "    return time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(data_raw, csv_row, interval ):\n",
    "    df = pd.read_csv(path_to_csv, usecols=['meas_date','onset', 'offset','onset_sample','offset_sample', 'channels'])\n",
    "    cnt_data =0\n",
    "    cnt_data_ch = 0\n",
    "    columns_ = []\n",
    "    meas_date = []\n",
    "    seizure_count = []\n",
    "    data_ch_dict = [None] * len(data_raw)\n",
    "    data_epoch = [None] * len(data_raw)\n",
    "    #print('len of data_raw', len(data_raw))\n",
    "    \n",
    "    #Generate information on number of seizures in a file\n",
    "    all_meas_data = df.iloc[:,0]\n",
    "    all_onset = df.iloc[:,1]\n",
    "    all_offset = df.iloc[:,2]\n",
    "    all_onset_sample = df.iloc[:,3]\n",
    "    all_offset_sample = df.iloc[:,4]\n",
    "    meas_date = all_meas_data.tolist()\n",
    "    onset_date = all_onset.tolist()\n",
    "    offset_date = all_offset.tolist()\n",
    "    onset_sample_date = all_onset_sample.tolist()\n",
    "    offset_sample_date = all_offset_sample.tolist()\n",
    "    for i in range(0,len(meas_date)):\n",
    "        if i==0:\n",
    "            seiz = 1 ; cnt =1\n",
    "        else:\n",
    "            if (meas_date[i-1] == meas_date[i]):\n",
    "                cnt +=1 ; seiz = cnt\n",
    "            else:\n",
    "                seiz = 1 ; cnt =1\n",
    "        seizure_count.append(seiz)\n",
    "    #print('Seizure count list',seizure_count, len(seizure_count), csv_row)\n",
    "    \n",
    "    num_of_seizure_in_file = [1] * len(seizure_count)\n",
    "    for i in range(0,len(seizure_count)): \n",
    "        if seizure_count[i] == 2:\n",
    "            num_of_seizure_in_file[i] = 2\n",
    "            num_of_seizure_in_file[i-1] = 2\n",
    "        if seizure_count[i] == 3:\n",
    "            num_of_seizure_in_file[i] = 3\n",
    "            num_of_seizure_in_file[i-1] = 3\n",
    "            num_of_seizure_in_file[i-2] = 3\n",
    "    #print('num of seizures from a file',num_of_seizure_in_file)\n",
    "    \n",
    "    #print(\"seizure starts after:\",tdiff_on, \"seizure lasts for\",tdur_seiz,\"stops after\",tdiff_off)\n",
    "    #print('Seizure onset',onset,'Seizure offset',offset )\n",
    "    #print(print(\"seizure starts after:\",tdiff_on.seconds, \"seizure lasts for\",tdur_seiz.seconds,\"stops after\",tdiff_off.seconds))\n",
    "\n",
    "    '''\n",
    "    for arr in range(len(channel)):\n",
    "        for lis in range(len(channel[arr])):\n",
    "            channel[arr][lis].replace('\\'','')\n",
    "            channel[arr][lis].replace(']','')\n",
    "            channel[arr][lis].replace('[','')\n",
    "            channel[arr][lis].replace(' ','')\n",
    "    #print(\"Shape and type of tdiff_on:\",type(tdiff_on),tdiff_on, 'channel',channel)\n",
    "    \n",
    "    if channel.find(',') != -1:\n",
    "        channel = list(channel.split(\",\"))\n",
    "        #rint('HEY FOUND A ,', channel,len(channel), type(channel))\n",
    "    else:\n",
    "        channel = list(channel.split(\",\"))\n",
    "        #print('HEY DID NOT FIND ,', channel, len(channel), type(channel))\n",
    "    '''\n",
    "    cnt_data += 1\n",
    "    \n",
    "    #Signal prepreocessing\n",
    "    #print('measurement data',meas_data, meas_data_next)\n",
    "    \n",
    "    #ch_name = entry \n",
    "    #ref_ch = HRB\n",
    "    #ref_ch = data_raw.ch_names\n",
    "\n",
    "    data_ch_dict = data_raw.copy()\n",
    "    #print('Before re-referencing',data_ch_dict.info)\n",
    "    #data_ch_dict.plot()\n",
    "    #data_ch_dict = data_ch_dict.pick_channels(ch_names= ref_ch, ordered=False)\n",
    "    #data_ch_dict.set_eeg_reference(ref_channels=ref_ch) #TODO add rereferencing\n",
    "    #print('After re-referencing',data_ch_dict.info)\n",
    "    #data_ch_dict.plot()\n",
    "    \n",
    "    #Resampling and anti-aliasing low pass filter\n",
    "    data_ch_dict.load_data().filter(l_freq=None, h_freq=90, method='fir',picks=patient_channels)\n",
    "    #print('AFTER ANTI ALISAING FILTER')\n",
    "    #data_ch_dict.plot_psd(picks=patient_channels)\n",
    "    if data_ch_dict.info['sfreq'] != 256.0:\n",
    "        data_ch_dict = data_ch_dict.resample(256)\n",
    "    #data_ch_dict.plot()\n",
    "    \n",
    "    #print('After resampling',data_ch_dict.info)\n",
    "    data_ch_dict.load_data().filter(l_freq=0.2, h_freq=48, method='fir',picks=patient_channels)\n",
    "    #print('After filtering',data_ch_dict.info)\n",
    "    #print('AFTER BPF')\n",
    "    #data_ch_dict.plot_psd(picks=patient_channels)\n",
    "    \n",
    "    do_normalization = False\n",
    "    calculate_dt= False\n",
    "    #time_steps = (10*60)/2\n",
    "    time_steps = 10\n",
    "    \n",
    "    #Calculate time difference     #before re-referencing TODO\n",
    "    if calculate_dt == True:\n",
    "        #print('First derivative', channel)\n",
    "        data_ch_dict.apply_function(diff_data, picks=channel)\n",
    "        #print('After first derivative',data_ch_dict.info)\n",
    "        #data_ch_dict.plot()\n",
    "    else:\n",
    "        data_ch_dict = data_ch_dict\n",
    "        #print('First derivative not taken')\n",
    "    \n",
    "    # TODO ; Temporary use HRB2 and HRC2\n",
    "    #channel_temp = ['HRB2','HRC2', 'HRB3','HRC3']\n",
    "    #channel_temp = ['HRB2','HRC2']\n",
    "    #channel_temp = ['TLB1', 'TLA1', 'HL1', 'TBA1']\n",
    "    #channel_temp = ['HRA4', 'TBA1', 'HRA5', 'TBA2']\n",
    "    #data_ch_dict.pick_channels(ch_names = channel_temp, ordered = False)\n",
    "    #print(data_ch_dict.info)\n",
    "    \n",
    "    #seizure starts after: 17 seizure lasts for 55 stops after 73\n",
    "    #print(print(\"seizure starts after:\",tdiff_on.seconds, \"seizure lasts for\",tdur_seiz.seconds,\"stops after\",tdiff_off.seconds))\n",
    "    #print(print(\"seizure starts after:\",tdiff_on.microseconds, \"seizure lasts for\",tdur_seiz.microseconds,\"stops after\",tdiff_off.microseconds))\n",
    "    #seizure starts after: 0:00:17.558594 seizure lasts for 0:00:55.945312 stops after 0:01:13.503906\n",
    "\n",
    "    #Generate labels\n",
    "    for col in range(0,len(df.columns)):\n",
    "        data = df.iloc[csv_row,col]\n",
    "        columns_.append(data)\n",
    "    \n",
    "    meas_data = columns_[0]\n",
    "    onset = columns_[1]\n",
    "    offset = columns_[2]\n",
    "    onset_sample = columns_[3]\n",
    "    offset_sample = columns_[4]\n",
    "    channel = columns_[5]\n",
    "\n",
    "    tdur_seiz = datetime.datetime.strptime(offset, datetimeFormat)- datetime.datetime.strptime(onset, datetimeFormat)\n",
    "    tdiff_on = datetime.datetime.strptime(onset, datetimeFormat)- datetime.datetime.strptime(meas_data, datetimeFormat)\n",
    "    tdiff_off = datetime.datetime.strptime(offset, datetimeFormat)- datetime.datetime.strptime(meas_data, datetimeFormat)\n",
    "    \n",
    "    '''\n",
    "    if (csv_row == len(seizure_count)-1):\n",
    "        print('This file has ',seizure_count[csv_row],'seizures')\n",
    "        \n",
    "    elif(csv_row == len(seizure_count)-2):\n",
    "        if(seizure_count[csv_row] == 1 and seizure_count[csv_row+1] == 2 ):\n",
    "            print('This file has 2 seizures')\n",
    "        elif(seizure_count[csv_row] == 2 and seizure_count[csv_row+1] == 3 ):\n",
    "            print('This file has 3 seizures')\n",
    "        else:\n",
    "            print('This file has 1 seizure')\n",
    "        \n",
    "    elif ((seizure_count[csv_row] == 1) and (seizure_count[csv_row+1] ==1)):\n",
    "        print('This file has only one seizure')\n",
    "    \n",
    "    elif(((seizure_count[csv_row] == 1) and (seizure_count[csv_row+1] ==2) and (seizure_count[csv_row+2] ==1)) or ((seizure_count[csv_row] ==2) and (seizure_count[csv_row+1] ==1))):\n",
    "        print('This file has 2 seizures')\n",
    "    \n",
    "    elif(((seizure_count[csv_row] == 1) and (seizure_count[csv_row+1] ==2) and (seizure_count[csv_row+2] ==3)) or ((seizure_count[csv_row] == 2) and (seizure_count[csv_row+1] ==3)) or (seizure_count[csv_row] == 3)):\n",
    "        print('This file has 3 seizures')'''\n",
    "    \n",
    "    #Extract 5s epochs : Create epochs and labels\n",
    "    interval = interval  #TODO 2 sec\n",
    "    time_window = interval * data_ch_dict.info['sfreq'] # 5*512\n",
    "    #print('time window', time_window)\n",
    "    \n",
    "    #Split\n",
    "    if (num_of_seizure_in_file[csv_row] == 1):\n",
    "        data_ch_dict = data_ch_dict.copy()\n",
    "        print('data_ch_dict_1',data_ch_dict)\n",
    "        df = data_ch_dict.to_data_frame()\n",
    "        df['time_s'] = df['time'] / 1000.0\n",
    "        df['target'] = 0\n",
    "        df['target'] = np.where(((df.time_s >= float(tdiff_on.seconds)) & (df.time_s <= float(tdiff_off.seconds))),1 ,df.target)        \n",
    "        seizure_length = float(tdiff_off.seconds) - float(tdiff_on.seconds)\n",
    "        print('Length of seizure is', seizure_length, 'seconds' )\n",
    "        \n",
    "    elif (num_of_seizure_in_file[csv_row] == 2):\n",
    "        if (seizure_count[csv_row] ==1):\n",
    "            split_1 = dt_(offset_date[csv_row],meas_date[csv_row])  +((dt_(onset_date[csv_row+1],meas_date[csv_row]) - dt_(offset_date[csv_row],meas_date[csv_row]))/2 )\n",
    "            data_ch_dict_2_1 = data_ch_dict.copy().crop(tmin = 0, tmax = split_1)\n",
    "            data_ch_dict = data_ch_dict_2_1.copy()\n",
    "            print('data_ch_dict_2_1',data_ch_dict)\n",
    "            df = data_ch_dict.to_data_frame()\n",
    "            df['time_s'] = df['time'] / 1000.0\n",
    "            df['target'] = 0\n",
    "            df['target'] = np.where(((df.time_s >=float(tdiff_on.seconds)) & (df.time_s <= float(tdiff_off.seconds))),1,df.target)\n",
    "            print('2_1', split_1,float(tdiff_on.seconds), float(tdiff_off.seconds))\n",
    "            seizure_length = float(tdiff_off.seconds) - float(tdiff_on.seconds)\n",
    "            print('Length of seizure is', seizure_length, 'seconds' )\n",
    "            \n",
    "        if (seizure_count[csv_row] ==2):\n",
    "            split_1 = dt_(offset_date[csv_row-1],meas_date[csv_row])  +((dt_(onset_date[csv_row],meas_date[csv_row]) - dt_(offset_date[csv_row-1],meas_date[csv_row]))/2 )\n",
    "            start = (dt_(onset_date[csv_row],meas_date[csv_row]) - dt_(offset_date[csv_row-1],meas_date[csv_row]))/2\n",
    "            end = start + (dt_(offset_date[csv_row],meas_date[csv_row]) - dt_(onset_date[csv_row],meas_date[csv_row]))\n",
    "            data_ch_dict_2_2 = data_ch_dict.copy().crop(tmin= split_1)\n",
    "            data_ch_dict = data_ch_dict_2_2.copy()\n",
    "            print('data_ch_dict_2_2',data_ch_dict)\n",
    "            df = data_ch_dict.to_data_frame()\n",
    "            df['time_s'] = df['time'] / 1000.0\n",
    "            df['target'] = 0\n",
    "            df['target'] = np.where(((df.time_s >= start) & (df.time_s <= end)),1,df.target)\n",
    "            print('2_2', split_1, start, end)\n",
    "            seizure_length = end - start\n",
    "            print('Length of seizure is', seizure_length, 'seconds' )\n",
    "            \n",
    "    elif (num_of_seizure_in_file[csv_row] == 3):\n",
    "        if ( seizure_count[csv_row] ==1):\n",
    "            split_2 = dt_(offset_date[csv_row],meas_date[csv_row])  +((dt_(onset_date[csv_row+1],meas_date[csv_row]) - dt_(offset_date[csv_row],meas_date[csv_row]))/2 )\n",
    "            data_ch_dict_3_1 = data_ch_dict.copy().crop(tmin=0 ,tmax= split_2)\n",
    "            data_ch_dict = data_ch_dict_3_1.copy()\n",
    "            print('data_ch_dict_3_1',data_ch_dict)\n",
    "            df = data_ch_dict.to_data_frame()\n",
    "            df['time_s'] = df['time'] / 1000.0\n",
    "            df['target'] = 0\n",
    "            df['target'] = np.where(((df.time_s >= float(tdiff_on.seconds)) & (df.time_s <= float(tdiff_off.seconds))),1,df.target)\n",
    "            print('3_1', split_2, float(tdiff_on.seconds), float(tdiff_off.seconds))\n",
    "            seizure_length = float(tdiff_off.seconds) - float(tdiff_on.seconds)\n",
    "            print('Length of seizure is', seizure_length, 'seconds' )\n",
    "            \n",
    "        if ( seizure_count[csv_row] ==2):\n",
    "            split_2 = dt_(offset_date[csv_row-1],meas_date[csv_row])  +((dt_(onset_date[csv_row],meas_date[csv_row]) - dt_(offset_date[csv_row-1],meas_date[csv_row]))/2 )\n",
    "            split_3 = dt_(offset_date[csv_row],meas_date[csv_row])  +((dt_(onset_date[csv_row+1],meas_date[csv_row]) - dt_(offset_date[csv_row],meas_date[csv_row]))/2 )\n",
    "            start = (dt_(onset_date[csv_row],meas_date[csv_row]) - dt_(offset_date[csv_row-1],meas_date[csv_row]))/2\n",
    "            end = start + (dt_(offset_date[csv_row],meas_date[csv_row]) - dt_(onset_date[csv_row],meas_date[csv_row])) \n",
    "            data_ch_dict_3_2 = data_ch_dict.copy().crop(tmin=split_2 ,tmax=split_3)\n",
    "            data_ch_dict = data_ch_dict_3_2.copy()\n",
    "            print('data_ch_dict_3_2',data_ch_dict)\n",
    "            df = data_ch_dict.to_data_frame()\n",
    "            df['time_s'] = df['time'] / 1000.0\n",
    "            df['target'] = 0\n",
    "            df['target'] = np.where(((df.time_s >= start) & (df.time_s <= end)),1,df.target)\n",
    "            print('3_2', split_2, split_3, start, end)\n",
    "            seizure_length = end - start\n",
    "            print('Length of seizure is', seizure_length, 'seconds' )\n",
    "            \n",
    "        if ( seizure_count[csv_row]==3):\n",
    "            split_3 = dt_(offset_date[csv_row-1],meas_date[csv_row])  +((dt_(onset_date[csv_row],meas_date[csv_row]) - dt_(offset_date[csv_row-1],meas_date[csv_row]))/2 )\n",
    "            start = (dt_(onset_date[csv_row],meas_date[csv_row]) - dt_(offset_date[csv_row-1],meas_date[csv_row]))/2\n",
    "            end = start + (dt_(offset_date[csv_row],meas_date[csv_row]) - dt_(onset_date[csv_row],meas_date[csv_row]))\n",
    "            data_ch_dict_3_3 = data_ch_dict.copy().crop(tmin=split_3 )\n",
    "            data_ch_dict = data_ch_dict_3_3.copy()\n",
    "            print('data_ch_dict_3_3',data_ch_dict)\n",
    "            df = data_ch_dict.to_data_frame()\n",
    "            df['time_s'] = df['time'] / 1000.0\n",
    "            df['target'] = 0\n",
    "            df['target'] = np.where(((df.time_s >= start) & (df.time_s <= end)),1,df.target)\n",
    "            print('3_3', split_3, start, end)\n",
    "            seizure_length = end - start\n",
    "            print('Length of seizure is', seizure_length, 'seconds' )\n",
    "    \n",
    "     # Store data, time and label as dataframe\n",
    "    #df = data_ch_dict.to_data_frame()\n",
    "    #print(df)\n",
    "    #df['time_s'] = df['time'] / 1000.0\n",
    "    #df['target'] = 0\n",
    "    #df['target'] = np.where(((df.time_s >= float(tdiff_on.seconds)) & (df.time_s <= float(tdiff_off.seconds))),1,df.target)\n",
    "    #print(df) \n",
    "    \n",
    "    '''\n",
    "    #Crop the data to include less non-ictal activity : Temporary logic\n",
    "    def define_df(data_ch_dict, idx):\n",
    "        df1 = data_ch_dict.to_data_frame()\n",
    "        df1['time_s'] = df1['time'] / 1000.0\n",
    "        df1['target'] = 0\n",
    "        df1['target'] = np.where(( (df1.time_s+ idx >= float(tdiff_on.seconds)) & (df1.time_s + idx <= float(tdiff_off.seconds))),1,df1.target)\n",
    "        return df1\n",
    "    if (tdiff_on.seconds < 1200):\n",
    "        data_ch_dict_n = data_ch_dict.copy().crop(tmin=0,tmax=1200) #0,1200\n",
    "        print('Seizure occurs in first half')\n",
    "        df_subset = define_df(data_ch_dict_n, 0)\n",
    "    elif (tdiff_on.seconds > 2400):\n",
    "        data_ch_dict_n = data_ch_dict.copy().crop(tmin=2400, tmax =3599) # 2400,3600\n",
    "        print('Seizure occurs in end')\n",
    "        df_subset = define_df(data_ch_dict_n, 2400)\n",
    "    else:\n",
    "        data_ch_dict_n = data_ch_dict.copy().crop(tmin=1200,tmax=2400) # 1200,2400\n",
    "        print('Seizure occurs in mid ')\n",
    "        df_subset = define_df(data_ch_dict_n, 1200)\n",
    "       #'''    \n",
    "    \n",
    "    data_epoch = mne.make_fixed_length_epochs(data_ch_dict, duration= interval, preload=True, id= csv_row) \n",
    "    #data_epoch = mne.make_fixed_length_epochs(data_ch_dict_n, duration= interval, preload=True, id= csv_row)\n",
    "    print('Fixed Length Epochs',csv_row,data_epoch, len(data_epoch), type(data_epoch) )\n",
    "    #stronger_reject_criteria = dict(eeg=1000e-6)        #10microV\n",
    "    #data_epoch.drop_bad(reject=stronger_reject_criteria)\n",
    "    #print(data_epoch.drop_log)\n",
    "    #print('Epochs return value',data_epoch.get_data(), np.shape(data_epoch.get_data()), type(data_epoch.get_data()) )\n",
    "    \n",
    "    data_labels = [None] * len(data_epoch)\n",
    "    time_labels = [None] * len(data_epoch)\n",
    "    #print('Number of total samples in 1 file',len(data_epoch) )\n",
    "    for seg in range(0,len(data_epoch)):\n",
    "        df_start = int(seg*time_window)\n",
    "        df_end = int((seg+1)*time_window)\n",
    "        #count_labels = df_subset.loc[df_start:df_end,'target'].sum()\n",
    "        count_labels = df.loc[df_start:df_end,'target'].sum()\n",
    "        if count_labels > time_window/2: #Update this threshold as needed TODO\n",
    "            #print('SEIZURE DETECTED', csv_row, ':', count_labels,'seg', seg, 'start',df_start,'end',df_end)\n",
    "            label = 1\n",
    "        else:\n",
    "            #print('NO SEIZURE DETECTED FOR', csv_row,':',count_labels,'seg', seg, 'start',df_start,'end',df_end)\n",
    "            label = 0\n",
    "        data_labels[seg] = label\n",
    "        count_labels = 0\n",
    "        time_labels[seg] = df.iloc[df_start:df_end,0]\n",
    "    #print( np.shape(np.array(time_labels)),type(np.array(time_labels)) )   #RETURN\n",
    "    #print('time_labels',time_labels)\n",
    "    print('Label generated for',csv_row,':',data_epoch[0])\n",
    "    \n",
    "    #Generate time_labels\n",
    "    \n",
    "    # Non trimmed\n",
    "    #print(df)\n",
    "    '''\n",
    "    t1 = df.time_s\n",
    "    t2 = df.time_s\n",
    "\n",
    "    plt.figure()\n",
    "    plt.subplot(511)\n",
    "    plt.plot(t1, df.iloc[:,1])\n",
    "    plt.subplot(512)\n",
    "    plt.plot(t1, df.iloc[:,2])\n",
    "    plt.subplot(513)\n",
    "    plt.plot(t1, df.iloc[:,3])\n",
    "    plt.subplot(514)\n",
    "    plt.plot(t1, df.iloc[:,4])\n",
    "    #plt.plot(t1, df.TLB1)\n",
    "\n",
    "    plt.subplot(515)\n",
    "    plt.plot(t2, df.target)\n",
    "    plt.show()#'''\n",
    "    '''\n",
    "    #print(df_subset)  # Trimmed\n",
    "    t1 = df_subset.time_s\n",
    "    t2 = df_subset.time_s\n",
    "\n",
    "    plt.figure()\n",
    "    plt.subplot(211)\n",
    "    plt.plot(t1, df_subset.HRB2)\n",
    "    #plt.plot(t1, df_subset.TLB1)\n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.plot(t2, df_subset.target)\n",
    "    plt.show()#'''\n",
    "    return data_epoch.get_data() , np.array(data_labels), np.array(time_labels), seizure_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pkl and unpkl all seizures for all patients\n",
    "import pickle\n",
    "with open('patient_id.pkl','wb') as pickle_file:\n",
    "    pickle.dump(df,pickle_file)\n",
    "    \n",
    "with open('patient_id.pkl','rb') as pickle_file:\n",
    "    pickle.load(pickle_file, same_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt =0\n",
    "for csv_row,data_raw in enumerate(reref_raws):\n",
    "    #print(csv_row,'raw data', data_raw)\n",
    "    print(np.shape(process_file(data_raw, csv_row,interval)[0]))\n",
    "    #print('###################################################')\n",
    "    #print(np.shape(process_file(data_raw, csv_row,interval)[1]))\n",
    "    #print('###################################################')\n",
    "    #print(np.shape(process_file(data_raw, csv_row,interval)[2]))\n",
    "    #cnt +=1\n",
    "    #if cnt >3 :\n",
    "    #print(np.shape(process_file(data_raw, csv_row,interval)[0]), np.shape(process_file(data_raw, csv_row,interval)[1]))\n",
    "    #    break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, ConcatDataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "class EpochsDataset(Dataset):\n",
    "    \"\"\"Class to expose an MNE Epochs object as PyTorch dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    epochs_data : np.ndarray\n",
    "        The epochs data, shape (n_epochs, n_channels, n_times).\n",
    "    epochs_labels : np.ndarray\n",
    "        The epochs labels, shape (n_epochs,)\n",
    "    epochs_time_labels : np.ndarray\n",
    "        The time labels, shape (n_epochs,n_times)\n",
    "    subj_num: None | int\n",
    "        Subject number.\n",
    "    rec_num: None | int\n",
    "        Recording number.\n",
    "    transform : callable | None\n",
    "        The function to eventually apply to each epoch\n",
    "        for preprocessing (e.g. scaling). Defaults to None.\n",
    "    \"\"\"\n",
    "    def __init__(self, epochs_data, epochs_labels, epochs_time_labels,subj_num=None, rec_num=None,transform=None):\n",
    "        assert len(epochs_data) == len(epochs_labels)\n",
    "        self.epochs_data = epochs_data\n",
    "        self.epochs_labels = epochs_labels\n",
    "        self.epochs_time_labels = epochs_time_labels\n",
    "        self.subj_num = subj_num\n",
    "        self.rec_num = rec_num\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.epochs_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X, y = self.epochs_data[idx], self.epochs_labels[idx]\n",
    "        time_label = self.epochs_time_labels[idx]\n",
    "        #print('check getitem',X.shape,y.shape)\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "        X = torch.as_tensor(X[None, ...])\n",
    "        return X, y, time_label\n",
    "\n",
    "#transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "def scale(X):\n",
    "    \"\"\"Standard scaling of data along the last dimention.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape (n_channels, n_times)\n",
    "        The input signals.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X_t : array, shape (n_channels, n_times)\n",
    "        The scaled signals.\n",
    "    \"\"\"\n",
    "    #print('Input',X)\n",
    "    X -= np.mean(X, axis=1, keepdims=True)\n",
    "    #print('Mean',np.mean(X, axis=1, keepdims=True))\n",
    "    #print('Std Dev',np.std(X, axis=1, keepdims=True))\n",
    "    #print('Return', X / np.std(X, axis=1, keepdims=True))\n",
    "    return X / np.std(X, axis=1, keepdims=True)\n",
    "\n",
    "#TODO: ''######## IMPORTANT #########''torch.utils.data. needed? and if it is actually normalized \n",
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))  \n",
    "        ])\n",
    "\n",
    "# To remove seizures less than 10s\n",
    "all_datasets = []\n",
    "cnt_discarded_seizures = 0\n",
    "tmp_cnt = 0\n",
    "\n",
    "for csv_row,data_raw in enumerate(reref_raws):\n",
    "    r_data, r_label, r_time_label, len_seizure = process_file(data_raw, csv_row, interval)\n",
    "    if len_seizure >= 10.1:\n",
    "        valid_dataset = EpochsDataset(r_data, r_label, r_time_label, subj_num=data_raws[csv_row].info['subject_info']['id'], \n",
    "                              rec_num=data_raws[csv_row].info['subject_info']['rec_id'], transform=scale)\n",
    "        tmp_cnt += 1\n",
    "        all_datasets.append(valid_dataset) \n",
    "    else:\n",
    "        cnt_discarded_seizures +=1\n",
    "        print('The length of the seizure is',len_seizure,'s. Hence, discarded!')\n",
    "\n",
    "print('Number of selected datasets are:',len(all_datasets),'. Number of seizures less than 10s:',cnt_discarded_seizures)\n",
    "# Does not remove seizures less than 10s \n",
    "'''all_datasets = [EpochsDataset(*process_file(data_raw, csv_row, interval), subj_num=data_raws[csv_row].info['subject_info']['id'], \n",
    "                              rec_num=data_raws[csv_row].info['subject_info']['rec_id'], transform=scale) \n",
    "                for csv_row,data_raw in enumerate(reref_raws)]  ''' #replaced from data_raws\n",
    "\n",
    "# Concatenate into a single dataset\n",
    "dataset = ConcatDataset(all_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data',all_datasets[0].epochs_data.shape)\n",
    "print('label',all_datasets[0].epochs_labels.shape)\n",
    "print('time_info',all_datasets[0].epochs_time_labels.shape)\n",
    "\n",
    "#print('data',all_datasets[0].epochs_data)\n",
    "print('label',all_datasets[0].epochs_labels)\n",
    "print('time_info',all_datasets[0].epochs_time_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#1. Test file split  - flag, divide\n",
    "#2. Dataloader with timelabel info\n",
    "#3. Randomsampler for selecting train-test in dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making training test validation split: 80-20\n",
    "from sklearn.model_selection import LeavePGroupsOut  \n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def train_test_split(dataset):\n",
    "    \"\"\"Split dataset into train and test \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : ConcatDataset\n",
    "        The dataset to split.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    ConcatDataset\n",
    "        The training data and the testing data.\n",
    "    \"\"\"\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    print('Train and test size', train_size, test_size)\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "torch.manual_seed(87)\n",
    "np.random.seed(87)\n",
    "train_ds, test_ds = train_test_split(dataset)\n",
    "print('Number of examples in each set:')\n",
    "print(f'Training: {len(train_ds)}')\n",
    "print(f'Test: {len(test_ds)}')\n",
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "  \"\"\"\n",
    "    Resetting model weights to avoid weight leakage.\n",
    "  \"\"\"\n",
    "  for layer in m.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            #print(f'Reset trainable parameters of layer = {layer}')\n",
    "            layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING NEURAL NETWORK  # 512Hz, 5sec\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class IEEGSeizureDetection(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional neural network \n",
    "    X_shape torch.Size([180, 1, 2, 2560]) Y_shape torch.Size([180])\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_channels : int\n",
    "        Number of EEG channels.\n",
    "    sfreq : float\n",
    "        EEG sampling frequency. 512 Hz\n",
    "    n_conv_chs : int\n",
    "        Number of convolutional channels. Set to 2.\n",
    "    time_conv_size_s : float\n",
    "        Size of filters in temporal convolution layers, in seconds. \n",
    "        (512 samples at sfreq=512). e.g Use 0.5 for 256 samples\n",
    "    max_pool_size_s : float\n",
    "        Max pooling size, in seconds. \n",
    "    n_classes : int\n",
    "        Number of classes.\n",
    "    input_size_s : float\n",
    "        Size of the input, in seconds.\n",
    "    dropout : float\n",
    "        Dropout rate before the output dense layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_channels, sfreq, n_conv_chs=15, time_conv_size_s=0.5,\n",
    "                 max_pool_size_s=0.03125, n_classes=2, input_size_s=5,\n",
    "                 dropout=0.25):\n",
    "        super(IEEGSeizureDetection, self).__init__()\n",
    "\n",
    "        time_conv_size = int(time_conv_size_s * sfreq) #256\n",
    "        max_pool_size = int(max_pool_size_s * sfreq)   #16\n",
    "        input_size = int(input_size_s * sfreq)         #2560\n",
    "        pad_size = time_conv_size // 2                 #128\n",
    "        self.n_channels = 4                   #2\n",
    "        len_last_layer = self._len_last_layer(\n",
    "            n_channels, input_size, max_pool_size, n_conv_chs) #300\n",
    "        beta = 0.89\n",
    "        print('Check parameters',time_conv_size,max_pool_size,input_size,pad_size,self.n_channels, len_last_layer)\n",
    "        # 256 16 2560 128 2 300\n",
    "        '''\n",
    "        if n_channels > 1:\n",
    "            self.spatial_conv = nn.Conv2d(1, n_channels, (n_channels, 1))\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(1, n_conv_chs, (1, time_conv_size), padding=(0, pad_size)),\n",
    "            nn.BatchNorm2dd(n_conv_chs),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((1, max_pool_size)),\n",
    "            nn.Conv2d(n_conv_chs, n_conv_chs, (1, time_conv_size),padding=(0, pad_size)),\n",
    "            nn.BatchNorm2d(n_conv_chs),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((1, max_pool_size))\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(len_last_layer, n_classes)\n",
    "        )'''\n",
    "        self.conv1 = nn.Conv2d(1, 15, (n_channels,25) )\n",
    "        self.batchnorm1 = nn.BatchNorm2d(15)\n",
    "        self.maxpool = nn.MaxPool2d((1,n_channels))\n",
    "        self.conv2 = nn.Conv2d(15, 15 , (1,11))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(15)\n",
    "        self.conv3 = nn.Conv2d(15, 10 , (1,5))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(10)\n",
    "        self.fc1 = nn.Linear(10*1*38, 2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    @staticmethod\n",
    "    def _len_last_layer(n_channels, input_size, max_pool_size, n_conv_chs):\n",
    "        return n_channels * (input_size // (max_pool_size ** 2)) * n_conv_chs\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Parameters\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "            Batch of EEG windows of shape (batch_size, n_channels, n_times).\n",
    "        \"\"\"\n",
    "        '''\n",
    "        if self.n_channels > 1:\n",
    "            x = self.spatial_conv(x)\n",
    "            x = x.transpose(1, 2)\n",
    "        #[180, 1, 2, 2560]        \n",
    "        x = self.feature_extractor(x)\n",
    "        #([180, 15, 2, -- ])\n",
    "        return self.fc(x.flatten(start_dim=1))\n",
    "        #([180, 2])\n",
    "        '''\n",
    "        \n",
    "        #print('input', x.size(),x)\n",
    "        x = self.conv1(x)\n",
    "        #print('conv1', x.size())\n",
    "        # = self.batchnorm1(x)\n",
    "        x = F.relu(x)\n",
    "        #print('relu1', x.size())\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout(x)\n",
    "        #print('maxpool1', x.size())\n",
    "        x = self.conv2(x)\n",
    "        #print('conv2', x.size())\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.relu(x)\n",
    "        #print('relu2', x.size())\n",
    "        x = self.maxpool(x)\n",
    "        #print('maxpool2', x.size())\n",
    "        x = self.conv3(x)\n",
    "        #print('conv3', x.size())\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.relu(x)\n",
    "        #print('relu3', x.size())\n",
    "        x = self.maxpool(x)\n",
    "        #print('maxpool3', x.size())\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x.flatten(start_dim=1))\n",
    "        #print('fc', x.size())\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "#input torch.Size([180, 1, 2, 2560])\n",
    "#conv1 torch.Size([180, 15, 1, 2536])\n",
    "#relu1 torch.Size([180, 15, 1, 2536])\n",
    "#maxpool1 torch.Size([180, 15, 1, 634])\n",
    "#conv2 torch.Size([180, 15, 1, 624])\n",
    "#relu2 torch.Size([180, 15, 1, 624])\n",
    "#maxpool2 torch.Size([180, 15, 1, 156])\n",
    "#conv3 torch.Size([180, 10, 1, 152])\n",
    "#relu3 torch.Size([180, 10, 1, 152])\n",
    "#maxpool3 torch.Size([180, 10, 1, 38])\n",
    "#fc torch.Size([180, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING NEURAL NETWORK  # 256Hz, 5sec\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class IEEGSeizureDetection_5sec(nn.Module):\n",
    "\n",
    "    def __init__(self, n_channels, sfreq, n_conv_chs=15, time_conv_size_s=0.5,\n",
    "                 max_pool_size_s=0.03125, n_classes=2, input_size_s=5,\n",
    "                 dropout=0.25):\n",
    "        super(IEEGSeizureDetection_5sec, self).__init__()\n",
    "\n",
    "        time_conv_size = int(time_conv_size_s * sfreq) #256\n",
    "        max_pool_size = int(max_pool_size_s * sfreq)   #16\n",
    "        input_size = int(input_size_s * sfreq)         #2560\n",
    "        pad_size = time_conv_size // 2                 #128\n",
    "        self.n_channels = 4                   #2\n",
    "        len_last_layer = self._len_last_layer(\n",
    "            n_channels, input_size, max_pool_size, n_conv_chs) #300\n",
    "        beta = 0.89\n",
    "        print('Check parameters',time_conv_size,max_pool_size,input_size,pad_size,self.n_channels, len_last_layer)\n",
    "        # 256 16 2560 128 2 300\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 15, (n_channels,25) )\n",
    "        self.batchnorm1 = nn.BatchNorm2d(15)\n",
    "        self.maxpool1 = nn.MaxPool2d((1,n_channels))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.conv2 = nn.Conv2d(15, 15 , (1,11))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(15)\n",
    "        self.maxpool2 = nn.MaxPool2d((1,2))\n",
    "        self.conv3 = nn.Conv2d(15, 10 , (1,5))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(10)\n",
    "        self.maxpool3 = nn.MaxPool2d((1,2))\n",
    "        self.conv4 = nn.Conv2d(10, 10 , (1,5))\n",
    "        self.batchnorm4 = nn.BatchNorm2d(10)\n",
    "        self.maxpool4 = nn.MaxPool2d((1,2))\n",
    "        self.fc1 = nn.Linear(10*1*35, 2)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _len_last_layer(n_channels, input_size, max_pool_size, n_conv_chs):\n",
    "        return n_channels * (input_size // (max_pool_size ** 2)) * n_conv_chs\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Parameters\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "            Batch of EEG windows of shape (batch_size, n_channels, n_times).\n",
    "        \"\"\"\n",
    "        #print('input', x.size())\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.batchnorm4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool4(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x.flatten(start_dim=1))\n",
    "        #x = torch.sigmoid(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING NEURAL NETWORK with parameterization for freq = 256Hz and t= 1sec\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class IEEGSeizureDetection_1sec(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional neural network \n",
    "    X_shape torch.Size([180, 1, 4, 1280]) Y_shape torch.Size([180])\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_channels : int\n",
    "        Number of EEG channels.\n",
    "    sfreq : float\n",
    "        EEG sampling frequency. 256 Hz\n",
    "    n_conv_chs : int\n",
    "        Number of convolutional channels. Set to 2.\n",
    "    time_conv_size_s : float\n",
    "        Size of filters in temporal convolution layers, in seconds. \n",
    "        (512 samples at sfreq=512). e.g Use 0.5 for 256 samples\n",
    "    max_pool_size_s : float\n",
    "        Max pooling size, in seconds. \n",
    "    n_classes : int\n",
    "        Number of classes.\n",
    "    input_size_s : float\n",
    "        Size of the input, in seconds.\n",
    "    dropout : float\n",
    "        Dropout rate before the output dense layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_channels, sfreq, n_conv_chs=15, time_conv_size_s=0.5,\n",
    "                 max_pool_size_s=0.03125, n_classes=2, input_size_s=5,\n",
    "                 dropout=0.25):\n",
    "        super(IEEGSeizureDetection_1sec, self).__init__()\n",
    "\n",
    "        time_conv_size = int(time_conv_size_s * sfreq) #256\n",
    "        max_pool_size = int(max_pool_size_s * sfreq)   #16\n",
    "        input_size = int(input_size_s * sfreq)         #2560\n",
    "        pad_size = time_conv_size // 2                 #128\n",
    "        self.n_channels = 4                   #2\n",
    "        len_last_layer = self._len_last_layer(\n",
    "            n_channels, input_size, max_pool_size, n_conv_chs) #300\n",
    "        beta = 0.89\n",
    "        print('Check parameters',time_conv_size,max_pool_size,input_size,pad_size,self.n_channels, len_last_layer)\n",
    "        # 256 16 2560 128 2 300\n",
    "        '''\n",
    "        if n_channels > 1:\n",
    "            self.spatial_conv = nn.Conv2d(1, n_channels, (n_channels, 1))\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(1, n_conv_chs, (1, time_conv_size), padding=(0, pad_size)),\n",
    "            nn.BatchNorm2dd(n_conv_chs),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((1, max_pool_size)),\n",
    "            nn.Conv2d(n_conv_chs, n_conv_chs, (1, time_conv_size),padding=(0, pad_size)),\n",
    "            nn.BatchNorm2d(n_conv_chs),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((1, max_pool_size))\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(len_last_layer, n_classes)\n",
    "        )'''\n",
    "        self.conv1 = nn.Conv2d(1, 15, (n_channels,25) )\n",
    "        self.batchnorm1 = nn.BatchNorm2d(15)\n",
    "        self.maxpool = nn.MaxPool2d((1,4))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.conv2 = nn.Conv2d(15, 15 , (1,11))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(15)\n",
    "        self.conv3 = nn.Conv2d(15, 10 , (1,5))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(10)\n",
    "        self.fc1 = nn.Linear(10*1*8, 8)\n",
    "        self.fc2 = nn.Linear(8, 2)  # TODO : output 1 or 2\n",
    "        #self.fc1 = nn.Linear(10*1*154, 1000)\n",
    "        #self.fc2 = nn.Linear(1000, 300)\n",
    "        #self.fc3 = nn.Linear(300, 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def _len_last_layer(n_channels, input_size, max_pool_size, n_conv_chs):\n",
    "        return n_channels * (input_size // (max_pool_size ** 2)) * n_conv_chs\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Parameters\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "            Batch of EEG windows of shape (batch_size, n_channels, n_times).\n",
    "        \"\"\"\n",
    "        '''\n",
    "        if self.n_channels > 1:\n",
    "            x = self.spatial_conv(x)\n",
    "            x = x.transpose(1, 2)\n",
    "        #[180, 1, 2, 2560]        \n",
    "        x = self.feature_extractor(x)\n",
    "        #([180, 15, 2, -- ])\n",
    "        return self.fc(x.flatten(start_dim=1))\n",
    "        #([180, 2])\n",
    "        '''\n",
    "        \n",
    "        #print('input', x.size())\n",
    "        x = self.conv1(x)\n",
    "        #print('conv1', x.size())\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.relu(x)\n",
    "        #print('relu1', x.size())\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout(x)\n",
    "        #print('maxpool1', x.size())\n",
    "        x = self.conv2(x)\n",
    "        #print('conv2', x.size())\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.relu(x)\n",
    "        #print('relu2', x.size())\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout(x)\n",
    "        #print('maxpool2', x.size())\n",
    "        x = self.conv3(x)\n",
    "        #print('conv3', x.size())\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.relu(x)\n",
    "        #print('relu3', x.size())\n",
    "        #x = self.maxpool(x)\n",
    "        #print('maxpool3', x.size())\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x.flatten(start_dim=1))\n",
    "        x = self.fc2(x)\n",
    "        #x = self.fc3(x)\n",
    "        #print('fc', x.size())\n",
    "        #x = torch.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "#input torch.Size([180, 1, 2, 2560])\n",
    "#conv1 torch.Size([180, 15, 1, 2536])\n",
    "#relu1 torch.Size([180, 15, 1, 2536])\n",
    "#maxpool1 torch.Size([180, 15, 1, 634])\n",
    "#conv2 torch.Size([180, 15, 1, 624])\n",
    "#relu2 torch.Size([180, 15, 1, 624])\n",
    "#maxpool2 torch.Size([180, 15, 1, 156])\n",
    "#conv3 torch.Size([180, 10, 1, 152])\n",
    "#relu3 torch.Size([180, 10, 1, 152])\n",
    "#maxpool3 torch.Size([180, 10, 1, 38])\n",
    "#fc torch.Size([180, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conv1D\n",
    "#conv1D\n",
    "# CREATING NEURAL NETWORK with parameterization for freq = 256Hz and t= 2sec : based on Farrokh's\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class IEEGSeizureDetection_2sec(nn.Module):\n",
    "    \n",
    "    def __init__(self, trial, params, n_channels, sfreq, n_conv_chs=15, time_conv_size_s=0.5,\n",
    "                 max_pool_size_s=0.03125, n_classes=2, input_size_s=5, dropout=0.25):\n",
    "        super(IEEGSeizureDetection_2sec, self).__init__()\n",
    "\n",
    "        time_conv_size = int(time_conv_size_s * sfreq) #256\n",
    "        max_pool_size = int(max_pool_size_s * sfreq)   #16\n",
    "        input_size = int(input_size_s * sfreq)         #2560\n",
    "        pad_size = time_conv_size // 2                 #128\n",
    "        self.n_channels = 4                            #2\n",
    "        len_last_layer = self._len_last_layer(n_channels, input_size, max_pool_size, n_conv_chs) #300\n",
    "        beta = 0.89\n",
    "        print('Check parameters',time_conv_size,max_pool_size,input_size,pad_size,self.n_channels, len_last_layer)\n",
    "\n",
    "        #Quantized Aware Training\n",
    "        #8-bit signed integer with a per-tensor floating-point scale factor (ONNX standard)\n",
    "        '''\n",
    "        weight_bit_width = 4\n",
    "        act_bit_width = 4\n",
    "        #Layer1\n",
    "        self.quant_identity = QuantIdentity(return_quant_tensor=True)\n",
    "        self.conv1 = qnn.QuantConv1d(in_channels=4, out_channels= 20,kernel_size= 17,\n",
    "                                     bias=True,\n",
    "                                     #weight_bit_width=weight_bit_width,\n",
    "                                     #input_quant=Int8ActPerTensorFloat, \n",
    "                                     #weight_quant=Int8ActPerTensorFloat,\n",
    "                                     bias_quant=Int8Bias,\n",
    "                                     #output_quant=Int8ActPerTensorFloat,\n",
    "                                     return_quant_tensor=True)\n",
    "        #print(self.conv1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(20)\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=4)\n",
    "        self.dropout = QuantDropout(0.2)\n",
    "        self.relu1 = QuantReLU(return_quant_tensor=True)\n",
    "        \n",
    "        #Layer2\n",
    "        self.conv2 = qnn.QuantConv1d(in_channels=20, out_channels=10 ,kernel_size= 5,\n",
    "                                     bias=True,\n",
    "                                     #weight_bit_width=weight_bit_width,\n",
    "                                     #input_quant=Int8ActPerTensorFloat, \n",
    "                                     bias_quant=Int8Bias,\n",
    "                                     #output_quant=Int8ActPerTensorFloat,\n",
    "                                     return_quant_tensor=True)\n",
    "        #print(self.conv2)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(10)\n",
    "        self.maxpool2 = nn.MaxPool1d(kernel_size=4)\n",
    "        self.relu2 = QuantReLU(return_quant_tensor=True)\n",
    "        \n",
    "        #Layer3\n",
    "        self.conv3 = qnn.QuantConv1d(in_channels=10, out_channels=10 , kernel_size=5,\n",
    "                                     bias=True,\n",
    "                                     #weight_bit_width=weight_bit_width,\n",
    "                                     #input_quant=Int8ActPerTensorFloat, \n",
    "                                     bias_quant=Int8Bias,\n",
    "                                     #output_quant=Int8ActPerTensorFloat,\n",
    "                                     return_quant_tensor=True)\n",
    "        #print(self.conv3)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(10)\n",
    "        self.maxpool3 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.relu3 = QuantReLU(return_quant_tensor=True)\n",
    "        \n",
    "        #Layer4\n",
    "        self.conv4 = qnn.QuantConv1d(in_channels=10, out_channels=10 , kernel_size=5,\n",
    "                                     bias=True,\n",
    "                                     #weight_bit_width=weight_bit_width,\n",
    "                                     #input_quant=Int8ActPerTensorFloat, \n",
    "                                     bias_quant=Int8Bias,\n",
    "                                     #output_quant=Int8ActPerTensorFloat,\n",
    "                                     return_quant_tensor=True)\n",
    "        #print(self.conv4)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(10)\n",
    "        self.relu4 = QuantReLU(return_quant_tensor=True)\n",
    "        \n",
    "        #Layer5\n",
    "        self.fc1 = qnn.QuantLinear(in_features=10*1*9, out_features= 5, \n",
    "                                   bias=True,   \n",
    "                                   #weight_bit_width=weight_bit_width,\n",
    "                                   #output_quant=Int8ActPerTensorFloat,\n",
    "                                   bias_quant=Int8Bias,\n",
    "                                   return_quant_tensor=True)\n",
    "        #print(self.fc1)\n",
    "        self.relu5 = QuantReLU(return_quant_tensor=True)\n",
    "        \n",
    "        #Layer6\n",
    "        self.fc2 = qnn.QuantLinear(in_features=5, out_features=5, \n",
    "                                   bias=True,  \n",
    "                                   #weight_bit_width=weight_bit_width,\n",
    "                                   #output_quant=Int8ActPerTensorFloat,\n",
    "                                   bias_quant=Int8Bias,\n",
    "                                   return_quant_tensor=True)\n",
    "        #print(self.fc2)\n",
    "        self.relu6 = QuantReLU(return_quant_tensor=True)\n",
    "        \n",
    "        #Layer7\n",
    "        self.fc3 = qnn.QuantLinear(in_features=5, \n",
    "                                   #out_features=1, \n",
    "                                   #input_quant=Int8ActPerTensorFloat, \n",
    "                                   out_features=2,\n",
    "                                   bias=True, \n",
    "                                   #weight_bit_width=weight_bit_width,\n",
    "                                   #output_quant=Int8ActPerTensorFloat,\n",
    "                                   bias_quant=Int8Bias,\n",
    "                                   return_quant_tensor=True)\n",
    "        #print(self.fc3)\n",
    "        #self.sigmoid = QuantSigmoid(return_quant_tensor=True)\n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "        #'''\n",
    "        # Non Quantized\n",
    "        '''\n",
    "        self.conv1 = nn.Conv1d(in_channels=4, out_channels= 20,kernel_size= 17 )\n",
    "        self.batchnorm1 = nn.BatchNorm1d(20)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=n_channels)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=20, out_channels=10 ,kernel_size= 5)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(10)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool1d(kernel_size=n_channels)\n",
    "        self.conv3 = nn.Conv1d(in_channels=10, out_channels=10 , kernel_size=5)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(10)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.maxpool3 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv4 = nn.Conv1d(in_channels=10, out_channels=10 , kernel_size=5)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(10)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(in_features=10*1*9, out_features= 5, bias=True)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=5, out_features=5, bias=True)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(in_features=5, out_features=2, bias=True)\n",
    "        #self.sigmoid = nn.Sigmoid()#'''\n",
    "\n",
    "        #Hyperparameterized\n",
    "        print('Chosen Hyperparameters',params, type(params), len(params))\n",
    "        #print('n_filters_1' in params)\n",
    "        #print('check val',params['n_filters_1'] )\n",
    "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=params['n_filters_1'] ,kernel_size= 17 )\n",
    "        self.batchnorm1 = nn.BatchNorm1d(params['n_filters_1'])\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=n_channels)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=params['n_filters_1'], out_channels=params['n_filters_2'] ,kernel_size= 5)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(params['n_filters_2'])\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool1d(kernel_size=n_channels)\n",
    "        self.conv3 = nn.Conv1d(in_channels=params['n_filters_2'], out_channels=params['n_filters_3'], kernel_size= 5)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(params['n_filters_3'])\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.maxpool3 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv4 = nn.Conv1d(in_channels=params['n_filters_3'], out_channels=params['n_filters_4'] , kernel_size= 5)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(params['n_filters_4'])\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(in_features=params['n_filters_4']*1*9, out_features=params['n_output_fc1'], bias=True)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=params['n_output_fc1'], out_features= params['n_output_fc2'], bias=True)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(in_features=params['n_output_fc2'], out_features=2, bias=True)  # TODO: Output 1 or 2\n",
    "\n",
    "    @staticmethod\n",
    "    def _len_last_layer(n_channels, input_size, max_pool_size, n_conv_chs):\n",
    "        return n_channels * (input_size // (max_pool_size ** 2)) * n_conv_chs\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Parameters\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "            Batch of EEG windows of shape (batch_size, n_channels, n_times).\n",
    "        \"\"\"\n",
    "        #print('in', x.shape)\n",
    "        #x = self.quant_identity(x)  # UNCOMMENT FOR QUANTIZATION \n",
    "        #print(f\"Input has QuantTensor:\\n {x.is_valid} \\n\")\n",
    "        #print('conv1', x.shape)\n",
    "        x = self.conv1(x)\n",
    "        #print(f\"CONV1:\\n {x} \\n\")\n",
    "        #print(f\"CONV1 has QuantTensor:\\n {x.is_valid} \\n\")\n",
    "        #print('bn', x.shape)\n",
    "        x = self.batchnorm1(x)\n",
    "        #print(f\"BN1 has QuantTensor:\\n {x.is_valid} \\n\")\n",
    "        #print('relu', x.shape)\n",
    "        x = self.relu1(x)\n",
    "        #print(f\"R1 has QuantTensor:\\n {x.is_valid} \\n\")\n",
    "        #print('maxpool', x.shape)\n",
    "        x = self.maxpool1(x)\n",
    "        #print(f\"MP1 has QuantTensor:\\n {x.is_valid} \\n\")\n",
    "        #print('dropout', x.shape)\n",
    "        x = self.dropout(x)\n",
    "        #print(f\"DR1 has QuantTensor:\\n {x.is_valid} \\n\")\n",
    "        #print('conv2 shape', x.shape)\n",
    "        x = self.conv2(x)\n",
    "        #print(f\"CONV2:\\n {x} \\n\")\n",
    "        #print(f\"CONV2 has QuantTensor:\\n {x.is_valid} \\n\")\n",
    "        x = self.batchnorm2(x)\n",
    "        #print(f\"BN2 has QuantTensor:\\n {x.is_valid} \\n\")\n",
    "        x = self.relu2(x)\n",
    "        #print(f\"R2 has QuantTensor:\\n {x.is_valid} \\n\")\n",
    "        x = self.maxpool2(x)\n",
    "        #print(f\"MP2 has QuantTensor:\\n {x.is_valid} \\n\")\n",
    "        x = self.dropout(x)\n",
    "        #print(f\"DR2 has QuantTensor:\\n {x.is_valid} \\n\")\n",
    "        x = self.conv3(x)\n",
    "        #print(f\"CONV3:\\n {x} \\n\")\n",
    "        #print(f\"CONV3 has QuantTensor:\\n {x.is_valid} \\n\")\n",
    "        x = self.batchnorm3(x)\n",
    "        #print(f\"BN3 has QuantTensor:\\n {x.is_valid} \\n\")\n",
    "        x = self.relu3(x)\n",
    "        #print(f\"R3 has QuantTensor:\\n {x.is_valid} \\n\")\n",
    "        x = self.maxpool3(x)\n",
    "        #print(f\"MP3 has QuantTensor:\\n {x.is_valid} \\n\")\n",
    "        x = self.dropout(x)\n",
    "        #print(f\"DR3 has QuantTensor:\\n {x.is_valid} \\n\")\n",
    "        x = self.conv4(x)\n",
    "        #print(f\"CONV4:\\n {x} \\n\")\n",
    "        #print(f\"CONV4 has QuantTensor:\\n {x.is_valid} \\n\")\n",
    "        x = self.batchnorm4(x)\n",
    "        #print(f\"BN4 has QuantTensor:\\n {x.is_valid} \\n\")\n",
    "        x = self.relu4(x)\n",
    "        #print(f\"R4 has QuantTensor:\\n {x.is_valid} \\n\")\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu5(self.fc1(x.flatten(start_dim=1)))\n",
    "        #print(f\"FC1:\\n {x} \\n\")\n",
    "        #print(f\"FC1 has QuantTensor:\\n {x.is_valid} \\n\")\n",
    "        x = self.relu6(self.fc2(x))\n",
    "        #print(f\"FC2:\\n {x} \\n\")\n",
    "        #print(f\"FC2 has QuantTensor:\\n {x.is_valid} \\n\")\n",
    "        #x = self.sigmoid(self.fc3(x))\n",
    "        x = self.fc3(x)\n",
    "        #print(f\"FC3:\\n {x} \\n\")\n",
    "        #print(f\"FC3 has QuantTensor:\\n {x.is_valid} \\n\")\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING NEURAL NETWORK with parameterization for freq = 256Hz and t= 2sec : based on Farrokh's\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class IEEGSeizureDetection_2sec(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional neural network \n",
    "    X_shape torch.Size([180, 1, 4, 1280]) Y_shape torch.Size([180])\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_channels : int\n",
    "        Number of EEG channels.\n",
    "    sfreq : float\n",
    "        EEG sampling frequency. 256 Hz\n",
    "    n_conv_chs : int\n",
    "        Number of convolutional channels. Set to 2.\n",
    "    time_conv_size_s : float\n",
    "        Size of filters in temporal convolution layers, in seconds. \n",
    "        (512 samples at sfreq=512). e.g Use 0.5 for 256 samples\n",
    "    max_pool_size_s : float\n",
    "        Max pooling size, in seconds. \n",
    "    n_classes : int\n",
    "        Number of classes.\n",
    "    input_size_s : float\n",
    "        Size of the input, in seconds.\n",
    "    dropout : float\n",
    "        Dropout rate before the output dense layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_channels, sfreq, n_conv_chs=15, time_conv_size_s=0.5,\n",
    "                 max_pool_size_s=0.03125, n_classes=2, input_size_s=5, dropout=0.25):\n",
    "        super(IEEGSeizureDetection_2sec, self).__init__()\n",
    "\n",
    "        time_conv_size = int(time_conv_size_s * sfreq) #256\n",
    "        max_pool_size = int(max_pool_size_s * sfreq)   #16\n",
    "        input_size = int(input_size_s * sfreq)         #2560\n",
    "        pad_size = time_conv_size // 2                 #128\n",
    "        self.n_channels = 4                            #2\n",
    "        len_last_layer = self._len_last_layer(n_channels, input_size, max_pool_size, n_conv_chs) #300\n",
    "        beta = 0.89\n",
    "        print('Check parameters',time_conv_size,max_pool_size,input_size,pad_size,self.n_channels, len_last_layer)\n",
    "        # 256 16 2560 128 2 300\n",
    "        '''\n",
    "        if n_channels > 1:\n",
    "            self.spatial_conv = nn.Conv2d(1, n_channels, (n_channels, 1))\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(1, n_conv_chs, (1, time_conv_size), padding=(0, pad_size)),\n",
    "            nn.BatchNorm2dd(n_conv_chs),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((1, max_pool_size)),\n",
    "            nn.Conv2d(n_conv_chs, n_conv_chs, (1, time_conv_size),padding=(0, pad_size)),\n",
    "            nn.BatchNorm2d(n_conv_chs),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((1, max_pool_size))\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(len_last_layer, n_classes)\n",
    "        )'''\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels= 20,kernel_size= (n_channels,17) )\n",
    "        self.batchnorm1 = nn.BatchNorm2d(20)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(1,n_channels))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=20, out_channels=10 ,kernel_size= (1,5))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(10)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(1,n_channels))\n",
    "        self.conv3 = nn.Conv2d(in_channels=10, out_channels=10 , kernel_size=(1,5))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(10)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=(1,2))\n",
    "        self.conv4 = nn.Conv2d(in_channels=10, out_channels=10 , kernel_size=(1,5))\n",
    "        self.batchnorm4 = nn.BatchNorm2d(10)\n",
    "        self.fc1 = nn.Linear(in_features=10*1*9, out_features= 5, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=5, out_features=5, bias=True)\n",
    "        self.fc3 = nn.Linear(in_features=5, out_features=2, bias=True) \n",
    "\n",
    "    @staticmethod\n",
    "    def _len_last_layer(n_channels, input_size, max_pool_size, n_conv_chs):\n",
    "        return n_channels * (input_size // (max_pool_size ** 2)) * n_conv_chs\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Parameters\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "            Batch of EEG windows of shape (batch_size, n_channels, n_times).\n",
    "        \"\"\"\n",
    "      \n",
    "        #print('input', x.size())\n",
    "        x = self.conv1(x)\n",
    "        #print('conv1', x.size())\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.relu(x)\n",
    "        #print('relu1', x.size())\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.dropout(x)\n",
    "        #print('maxpool1', x.size())\n",
    "        x = self.conv2(x)\n",
    "        #print('conv2', x.size())\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.relu(x)\n",
    "        #print('relu2', x.size())\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.dropout(x)\n",
    "        #print('maxpool2', x.size())\n",
    "        x = self.conv3(x)\n",
    "        #print('conv3', x.size())\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.relu(x)\n",
    "        #print('relu3', x.size())\n",
    "        x = self.maxpool3(x)\n",
    "        #print('maxpool3', x.size())\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv4(x)\n",
    "        #print('conv4', x.size())\n",
    "        x = self.batchnorm4(x)\n",
    "        x = F.relu(x)\n",
    "        #print('relu4', x.size())\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        #x = torch.sigmoid(x)\n",
    "        #print('fc', x.size())\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING NEURAL NETWORK with parameterization for freq = 256Hz and t= 2sec : based on Farrokh's + HPO\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class IEEGSeizureDetection_2sec(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional neural network \n",
    "    X_shape torch.Size([180, 1, 4, 1280]) Y_shape torch.Size([180])\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_channels : int\n",
    "        Number of EEG channels.\n",
    "    sfreq : float\n",
    "        EEG sampling frequency. 256 Hz\n",
    "    n_conv_chs : int\n",
    "        Number of convolutional channels. Set to 2.\n",
    "    time_conv_size_s : float\n",
    "        Size of filters in temporal convolution layers, in seconds. \n",
    "        (512 samples at sfreq=512). e.g Use 0.5 for 256 samples\n",
    "    max_pool_size_s : float\n",
    "        Max pooling size, in seconds. \n",
    "    n_classes : int\n",
    "        Number of classes.\n",
    "    input_size_s : float\n",
    "        Size of the input, in seconds.\n",
    "    dropout : float\n",
    "        Dropout rate before the output dense layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, trial, params, n_channels, sfreq, n_conv_chs=15, time_conv_size_s=0.5,\n",
    "                 max_pool_size_s=0.03125, n_classes=2, input_size_s=5, dropout=0.25):\n",
    "        super(IEEGSeizureDetection_2sec, self).__init__()\n",
    "\n",
    "        time_conv_size = int(time_conv_size_s * sfreq) #256\n",
    "        max_pool_size = int(max_pool_size_s * sfreq)   #16\n",
    "        input_size = int(input_size_s * sfreq)         #2560\n",
    "        pad_size = time_conv_size // 2                 #128\n",
    "        self.n_channels = 4                            #2\n",
    "        len_last_layer = self._len_last_layer(n_channels, input_size, max_pool_size, n_conv_chs) #300\n",
    "        beta = 0.89\n",
    "        #print('Check parameters',time_conv_size,max_pool_size,input_size,pad_size,self.n_channels, len_last_layer)\n",
    "        # 256 16 2560 128 2 300\n",
    "        '''\n",
    "        if n_channels > 1:\n",
    "            self.spatial_conv = nn.Conv2d(1, n_channels, (n_channels, 1))\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(1, n_conv_chs, (1, time_conv_size), padding=(0, pad_size)),\n",
    "            nn.BatchNorm2dd(n_conv_chs),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((1, max_pool_size)),\n",
    "            nn.Conv2d(n_conv_chs, n_conv_chs, (1, time_conv_size),padding=(0, pad_size)),\n",
    "            nn.BatchNorm2d(n_conv_chs),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((1, max_pool_size))\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(len_last_layer, n_classes)\n",
    "        )'''\n",
    "        \n",
    "        #self.params = params\n",
    "        print('Chosen Hyperparameters',params, type(params), len(params))\n",
    "        #print('n_filters_1' in params)\n",
    "        #print('check val',params['n_filters_1'] )\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=params['n_filters_1'] ,kernel_size= (n_channels,17) )\n",
    "        self.batchnorm1 = nn.BatchNorm2d(params['n_filters_1'])\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(1,n_channels))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=params['n_filters_1'], out_channels=params['n_filters_2'] ,kernel_size= (1,5))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(params['n_filters_2'])\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(1,n_channels))\n",
    "        self.conv3 = nn.Conv2d(in_channels=params['n_filters_2'], out_channels=params['n_filters_3'], kernel_size=(1,5))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(params['n_filters_3'])\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=(1,2))\n",
    "        self.conv4 = nn.Conv2d(in_channels=params['n_filters_3'], out_channels=params['n_filters_4'] , kernel_size=(1,5))\n",
    "        self.batchnorm4 = nn.BatchNorm2d(params['n_filters_4'])\n",
    "        self.fc1 = nn.Linear(in_features=params['n_filters_4']*1*9, out_features=params['n_output_fc1'], bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=params['n_output_fc1'], out_features= params['n_output_fc2'], bias=True)\n",
    "        self.fc3 = nn.Linear(in_features=params['n_output_fc2'], out_features=2, bias=True)  # TODO: Output 1 or 2\n",
    "\n",
    "    @staticmethod\n",
    "    def _len_last_layer(n_channels, input_size, max_pool_size, n_conv_chs):\n",
    "        return n_channels * (input_size // (max_pool_size ** 2)) * n_conv_chs\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Parameters\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "            Batch of EEG windows of shape (batch_size, n_channels, n_times).\n",
    "        \"\"\"\n",
    "      \n",
    "        #print('input', x.size())\n",
    "        x = self.conv1(x)\n",
    "        #print('conv1', x.size())\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.relu(x)\n",
    "        #print('relu1', x.size())\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.dropout(x)\n",
    "        #print('maxpool1', x.size())\n",
    "        x = self.conv2(x)\n",
    "        #print('conv2', x.size())\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.relu(x)\n",
    "        #print('relu2', x.size())\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.dropout(x)\n",
    "        #print('maxpool2', x.size())\n",
    "        x = self.conv3(x)\n",
    "        #print('conv3', x.size())\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.relu(x)\n",
    "        #print('relu3', x.size())\n",
    "        x = self.maxpool3(x)\n",
    "        #print('maxpool3', x.size())\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv4(x)\n",
    "        #print('conv4', x.size())\n",
    "        x = self.batchnorm4(x)\n",
    "        x = F.relu(x)\n",
    "        #print('relu4', x.size())\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        #x = torch.sigmoid(x)\n",
    "        #print('fc', x.size())\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "device = 'cuda'\n",
    "#sfreq = data_raws[0].info['sfreq']  # Sampling frequency\n",
    "sfreq = 256.0 \n",
    "#n_channels = data_raws[0].info['nchan']  # Number of channels\n",
    "n_channels = 4   #TODO : Get from function correct value\n",
    "print(sfreq, n_channels)\n",
    "model= IEEGSeizureDetection_2sec(n_channels, sfreq, n_classes=2)\n",
    "model.apply(reset_weights)\n",
    "print(f'Using device \\'{device}\\'.')\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_detection_delay_for_eval(df):\n",
    "    #print(df)\n",
    "    if ((df['True_Label'] == 0).all() and (df['Predicted_Label'] == 0).all()):\n",
    "        print('STATUS : No seizure labeled or detected')\n",
    "        diff = 0\n",
    "        s_d = 0\n",
    "    elif ((df['True_Label'] == 0).all() and not((df['Predicted_Label'] == 0).all())):\n",
    "        print('STATUS : False detection happened')\n",
    "        diff = 0\n",
    "        s_d = 0\n",
    "    elif (not((df['True_Label'] == 0).all()) and (df['Predicted_Label'] == 0).all()):\n",
    "        print('STATUS : Detection Missed')\n",
    "        diff = 0\n",
    "        s_d = 0\n",
    "    elif (not((df['True_Label'] == 0).all()) and not((df['Predicted_Label'] == 0).all())):\n",
    "        #t_pred = df.index[df['Predicted_Label'] == 1].tolist()\n",
    "        #t_true = df.index[df['True_Label'] == 1].tolist()\n",
    "        t_true = np.where(df[ 'True_Label'] == 1)\n",
    "        t_pred = np.where(df[ 'Predicted_Label'] == 1)\n",
    "        #print('Indices', t_pred, t_true)\n",
    "        #print('Indices', t_pred[0][0], t_true[0][0])\n",
    "        if t_pred[0][0] <= t_true[0][0]: #False pos\n",
    "            print('STATUS : False detection happened, but there is a seizure also')\n",
    "        #print('Prediction time', df.iloc[t_pred[0][0],6], 'True time',df.iloc[t_true[0][0],6] )\n",
    "        diff = (df.iloc[t_pred[0][0],6] - df.iloc[t_true[0][0],6])/1000.0 \n",
    "        print('STATUS : Detection delay of',diff,'secs')\n",
    "        s_d = 1\n",
    "    return diff, s_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_confusion_matrix(conf_mat):\n",
    "    classes_mapping = {0: 'non-ictal', 1: 'ictal'} \n",
    "    ticks = list(classes_mapping.keys())\n",
    "    tick_labels = classes_mapping.values()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    im = ax.imshow(conf_mat, cmap='Reds')\n",
    "    ax.set_yticks(ticks)\n",
    "    ax.set_yticklabels(tick_labels)\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(tick_labels)\n",
    "    ax.set_ylabel('True label')\n",
    "    ax.set_xlabel('Predicted label')\n",
    "    ax.set_title('Confusion matrix')\n",
    "\n",
    "    for i in range(len(ticks)):\n",
    "        for j in range(len(ticks)):\n",
    "            text = ax.text(\n",
    "                j, i, conf_mat[i, j], ha='center', va='center', color='k')\n",
    "\n",
    "    fig.colorbar(im, ax=ax, fraction=0.05, label='# examples')\n",
    "    fig.tight_layout()\n",
    "    #fig.savefig('./save_plot/confusion_matrix_'+i+'.png')\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, cohen_kappa_score\n",
    "from sklearn.metrics import average_precision_score, f1_score,  auc\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix\n",
    "\n",
    "def analog_to_spike(batch_x, batch_y):\n",
    "    #print('shape for analog to spike',np.shape(batch_x), np.shape(batch_x)[0])\n",
    "    b_s = np.shape(batch_x)[0]\n",
    "    c_s = np.shape(batch_x)[2]\n",
    "    t_s = np.shape(batch_x)[3]\n",
    "\n",
    "    batch_x = spikegen.delta(batch_x, threshold=0.5, off_spike=False)\n",
    "    #print('shape for loop',np.shape(batch_x), len(batch_x),b_s,c_s,t_s, 'shape of y',np.shape(batch_y))\n",
    "    #print('Check the value to be plotted', batch_x[0,0,0,:], len(batch_x[0,0,0,:])\n",
    "    '''\n",
    "    for b in range(0,5):\n",
    "        for c in range(0,c_s):    \n",
    "            # Create fig, ax\n",
    "            fig = plt.figure(facecolor=\"w\", figsize=(8, 4))\n",
    "            ax = fig.add_subplot(111)\n",
    "\n",
    "            # Raster plot of delta converted data\n",
    "            splt.raster(batch_x[b,0,c,:], ax)\n",
    "\n",
    "            plt.title(\"Input Neuron\"+ \"for batchsample_\"+ str(b) +\"_for channel_\"+ str(c) + \"_with_label:\" +str(batch_y[b]))\n",
    "            plt.xlabel(\"Time step\")\n",
    "            plt.yticks([])\n",
    "            plt.xlim(0,t_s )\n",
    "            fig.canvas.draw()\n",
    "    '''\n",
    "    return batch_x\n",
    "            \n",
    "\n",
    "def _do_train(model, loader, optimizer, criterion, device, metric):\n",
    "    # training loop\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = np.zeros(len(loader))\n",
    "    y_pred_all, y_true_all = list(), list()\n",
    "    for idx_batch, (batch_x, batch_y, batch_t) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        batch_x = batch_x.to(device=device, dtype=torch.float32)   #([180, 1, 2, 2560])\n",
    "        batch_x = torch.squeeze(batch_x,1)\n",
    "        batch_y = batch_y.to(device=device, dtype=torch.int64)\n",
    "        output = model(batch_x)\n",
    "        #output = torch.squeeze(output,1)(\n",
    "        #batch_y = batch_y.unsqueeze(1).float()\n",
    "        #print('actual class',batch_y, batch_y.shape)\n",
    "        #print('predicted class', torch.argmax(output, axis=1).cpu().numpy())\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        y_pred_all.append(torch.argmax(output, axis=1).cpu().numpy())\n",
    "        y_true_all.append(batch_y.cpu().numpy())\n",
    "\n",
    "        train_loss[idx_batch] = loss.item()\n",
    "        \n",
    "    y_pred = np.concatenate(y_pred_all)\n",
    "    y_true = np.concatenate(y_true_all)\n",
    "    perf = metric(y_true, y_pred)\n",
    "    acc = np.mean(y_true == y_pred)\n",
    "    #print('TRAIN: y_pred',y_pred,'y_true',y_true )\n",
    "    return np.mean(train_loss), perf   \n",
    "        \n",
    "'''\n",
    "def _validate(model, loader, criterion, device, metric):\n",
    "    # validation loop\n",
    "    model.eval()\n",
    "    \n",
    "    val_loss = np.zeros(len(loader))\n",
    "    y_pred_all, y_true_all = list(), list()\n",
    "    with torch.no_grad():\n",
    "        for idx_batch, (batch_x, batch_y, batch_t) in enumerate(loader):\n",
    "            batch_x = batch_x.to(device=device, dtype=torch.float32)\n",
    "            batch_y = batch_y.to(device=device, dtype=torch.int64)\n",
    "            output = model.forward(batch_x)\n",
    "            #output = torch.squeeze(output,1)\n",
    "            #print('predicted class', torch.argmax(output, axis=1).cpu().numpy())\n",
    "            #print('actual class', batch_y)\n",
    "            loss = criterion(output, batch_y)\n",
    "            val_loss[idx_batch] = loss.item()\n",
    "            \n",
    "            y_pred_all.append(torch.argmax(output, axis=1).cpu().numpy())\n",
    "            y_true_all.append(batch_y.cpu().numpy())\n",
    "            \n",
    "    y_pred = np.concatenate(y_pred_all)\n",
    "    y_true = np.concatenate(y_true_all)\n",
    "    perf = metric(y_true, y_pred)\n",
    "    #print('VALID: y_pred',y_pred,'y_true',y_true)    \n",
    "    return np.mean(val_loss), perf'''\n",
    "\n",
    "def _validate(model, loader, criterion, device, metric):\n",
    "    # validation loop\n",
    "    model.eval()\n",
    "    \n",
    "    average_precision = 0\n",
    "    f1_score_eval = 0\n",
    "    val_loss = np.zeros(len(loader))\n",
    "    y_pred_all, y_true_all, y_score_all = list(), list(), list()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx_batch, (batch_x, batch_y, batch_t) in enumerate(loader):\n",
    "            batch_x = batch_x.to(device=device, dtype=torch.float32)\n",
    "            batch_x = torch.squeeze(batch_x,1)\n",
    "            batch_y = batch_y.to(device=device, dtype=torch.int64)\n",
    "            batch_t = batch_t\n",
    "            output = model.forward(batch_x)\n",
    "            #output = torch.squeeze(output,1)\n",
    "            #print('predicted class', torch.argmax(output, axis=1).cpu().numpy())\n",
    "            #print('actual class', batch_y)\n",
    "            loss = criterion(output, batch_y)\n",
    "            val_loss[idx_batch] = loss.item()\n",
    "            \n",
    "            pred_probab = nn.Softmax(dim=1)(output)  # Move this TODO\n",
    "            probs = pred_probab[:, 1]\n",
    "            \n",
    "            y_pred_all.append(torch.argmax(output, axis=1).cpu().numpy())\n",
    "            y_true_all.append(batch_y.cpu().numpy())\n",
    "            y_score_all.append(probs.detach().cpu().numpy())\n",
    "            #print('Debug Average Precsion','pred_probab',pred_probab,'probs',probs, 'y_score_all' ,y_score_all)\n",
    "            \n",
    "    y_pred = np.concatenate(y_pred_all)\n",
    "    y_true = np.concatenate(y_true_all)\n",
    "    y_score = np.concatenate(y_score_all)\n",
    "    \n",
    "    perf = metric(y_true, y_pred)\n",
    "    f1_score_eval = f1_score(y_true, y_pred, average='binary')\n",
    "    average_precision = average_precision_score(y_true, y_score)\n",
    "    \n",
    "    #print('VALID: No Mean',val_loss ,  np.mean(val_loss), perf, average_precision, f1_score_eval )  \n",
    "    # return loss, accuracy, AP, F1 score, delay\n",
    "    return np.mean(val_loss), perf, average_precision , f1_score_eval\n",
    "\n",
    "def _validate_1epoch(model, loader, criterion, device, metric):\n",
    "    # validation loop\n",
    "    model.eval()\n",
    "    \n",
    "    average_precision = 0\n",
    "    f1_score_eval = 0\n",
    "    false_positives = 0\n",
    "    detection_delay = 0\n",
    "    val_loss = np.zeros(len(loader))\n",
    "    y_pred_all, y_true_all, y_score_all = list(), list(), list()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx_batch, (batch_x, batch_y, batch_t) in enumerate(loader):\n",
    "            batch_x = batch_x.to(device=device, dtype=torch.float32)\n",
    "            batch_x = torch.squeeze(batch_x,1)\n",
    "            batch_y = batch_y.to(device=device, dtype=torch.int64)\n",
    "            batch_t = batch_t\n",
    "            output = model.forward(batch_x)\n",
    "            #output = torch.squeeze(output,1)\n",
    "            #print('predicted class', torch.argmax(output, axis=1).cpu().numpy())\n",
    "            #print('actual class', batch_y)\n",
    "            loss = criterion(output, batch_y)\n",
    "            val_loss[idx_batch] = loss.item()\n",
    "            \n",
    "            pred_probab = nn.Softmax(dim=1)(output)  # Move this TODO\n",
    "            probs = pred_probab[:, 1]\n",
    "            \n",
    "            y_pred_all.append(torch.argmax(output, axis=1).cpu().numpy())\n",
    "            y_true_all.append(batch_y.cpu().numpy())\n",
    "            y_score_all.append(probs.detach().cpu().numpy())\n",
    "            #print('Debug Average Precsion','pred_probab',pred_probab,'probs',probs, 'y_score_all' ,y_score_all)\n",
    "            \n",
    "            #''' # Create dataframe and plot true and predicted labels\n",
    "            num_bs =  len(batch_x)      #256\n",
    "            num_in = len(batch_x[0,0,0,:])   # 2560\n",
    "            #print('older t1', num_bs, num_in)\n",
    "            t1 = np.arange(0,num_in*num_bs,1)\n",
    "    \n",
    "            #create a dataframe to visualize randomized data,target and outputs\n",
    "            df_reconstruct_all = pd.DataFrame(columns=['Channel1', 'Channel2','Channel3','Channel4', 'True_Label', 'Predicted_Label'])\n",
    "            for i in range(0, len(batch_x)):\n",
    "                data_re = {'Channel1':  batch_x[i,0,0,:].cpu().numpy(),\n",
    "                    'Channel2': batch_x[i,0,1,:].cpu().numpy(),\n",
    "                    'Channel3': batch_x[i,0,2,:].cpu().numpy(),\n",
    "                    'Channel4': batch_x[i,0,3,:].cpu().numpy(),\n",
    "                    'True_Label' : np.full((num_in, ), batch_y[i].cpu().numpy()), # 2560 labels\n",
    "                    'Predicted_Label' : np.full((num_in, ),torch.argmax(output, axis=1)[i].cpu().numpy() ) ,\n",
    "                    'Time_Label': batch_t[i,:].cpu().numpy()\n",
    "                    }\n",
    "                df_reconstruct = pd.DataFrame(data_re)\n",
    "                df_reconstruct_all = df_reconstruct_all.append(df_reconstruct)\n",
    "    \n",
    "            #Calculate detection delay\n",
    "            dd, s_d = calculate_detection_delay_for_eval(df_reconstruct_all)\n",
    "            if s_d !=0:\n",
    "                detection_delay = dd\n",
    "    \n",
    "            #Plot all batches\n",
    "            ''' \n",
    "            figure, axs = plt.subplots(nrows=4, ncols=1,figsize=(10,5))\n",
    "            axs[0].plot( df_reconstruct_all.Time_Label, df_reconstruct_all.Channel1)\n",
    "            axs[0].set_title('Channel1')\n",
    "            \n",
    "            axs[1].plot( df_reconstruct_all.Time_Label, df_reconstruct_all.Channel2)\n",
    "            axs[1].set_title('Channel2')\n",
    "    \n",
    "            axs[2].plot( df_reconstruct_all.Time_Label, df_reconstruct_all.True_Label)\n",
    "            axs[2].set_title('True Label')\n",
    "    \n",
    "            axs[3].plot( df_reconstruct_all.Time_Label, df_reconstruct_all.Predicted_Label )\n",
    "            axs[3].set_title('Predicted Label')\n",
    "            plt.show()#'''\n",
    "            #print(df_reconstruct_all)  # Print the dataframe for each batch\n",
    "            \n",
    "    y_pred = np.concatenate(y_pred_all)\n",
    "    y_true = np.concatenate(y_true_all)\n",
    "    y_score = np.concatenate(y_score_all)\n",
    "    print('Check dimensions', y_pred.shape, y_true.shape, y_score.shape)\n",
    "    \n",
    "    test_bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    test_kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    print(f'Test balanced accuracy: {test_bal_acc:0.3f}')\n",
    "    print(f'Test Cohen\\'s kappa: {test_kappa:0.3f}')\n",
    "    \n",
    "    perf = metric(y_true, y_pred)\n",
    "    f1_score_eval = f1_score(y_true, y_pred, average='binary')\n",
    "    average_precision = average_precision_score(y_true, y_score)\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "    auc_precision_recall = auc(recall, precision)\n",
    "    \n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "    tn, false_positives, fn, tp = conf_mat.ravel()\n",
    "    print('conf_mat', conf_mat)\n",
    "    \n",
    "    plot_confusion_matrix(conf_mat)\n",
    "    \n",
    "    print(f'f1_score_eval: {f1_score_eval:0.3f} \\t  average_precision: {average_precision:0.3f} \\t'\n",
    "         f'auc_precision_recall: {auc_precision_recall:0.3f} \\t  detection_delay: {detection_delay:0.3f}  \\t'\n",
    "         f'false_positive: {false_positives:0.3f} \\t false_negativ: {fn:0.3f}' )\n",
    "    \n",
    "    #print('VALID:', np.mean(val_loss), perf, average_precision, f1_score_eval )  \n",
    "    \n",
    "    # return loss, accuracy, AP, F1 score,false_+s, delay\n",
    "    return np.mean(val_loss), perf, average_precision , f1_score_eval, false_positives, detection_delay, fn\n",
    "\n",
    "def train(model, loader_train, loader_test, optimizer, criterion, n_epochs, \n",
    "          patience, device, metric=None):\n",
    "    \"\"\"Training function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : instance of nn.Module\n",
    "        The model.\n",
    "    loader_train : instance of Sampler\n",
    "        The generator of EEG samples the model has to train on.\n",
    "        It contains n_train samples\n",
    "    loader_test : instance of Sampler\n",
    "        The generator of EEG samples the model has to validate on.\n",
    "        It contains n_val samples. The validation samples are used to\n",
    "        monitor the training process and to perform early stopping\n",
    "    optimizer : instance of optimizer\n",
    "        The optimizer to use for training.\n",
    "    n_epochs : int\n",
    "        The maximum of epochs to run.\n",
    "    patience : int\n",
    "        The patience parameter, i.e. how long to wait for the\n",
    "        validation error to go down.\n",
    "    metric : None | callable\n",
    "        Metric to use to evaluate performance on the training and\n",
    "        validation sets. Defaults to balanced accuracy.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    best_model : instance of nn.Module\n",
    "        The model that led to the best prediction on the validation\n",
    "        dataset.\n",
    "    history : list of dicts\n",
    "        Training history (loss, accuracy, etc.)\n",
    "    \"\"\"\n",
    "    #best_valid_loss = np.inf\n",
    "    #best_model = copy.deepcopy(model)\n",
    "    waiting = 0\n",
    "    history = list()\n",
    "    history_dict = {'epoch': [], 'train_loss': [], 'valid_loss': [],'train_perf':[],'valid_perf':[],'valid_ap':[],\n",
    "                    'valid_f1':[], 'false_positive':[],'detection_delay':[]}\n",
    "    \n",
    "    if metric is None:\n",
    "        metric = balanced_accuracy_score\n",
    "        \n",
    "    print('epoch \\t train_loss \\t valid_loss \\t train_perf \\t valid_perf \\t valid_ap \\t valid_f1')\n",
    "    print('-------------------------------------------------------------------')\n",
    "    print('Check number of epochs to run for LOOCV folds after selecting best hyperparam', n_epochs)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss, train_perf = _do_train(\n",
    "            model, loader_train, optimizer, criterion, device, metric=metric)\n",
    "        \n",
    "        valid_loss, valid_perf, valid_ap, valid_f1 = _validate(\n",
    "            model, loader_valid, criterion, device, metric=metric)\n",
    "        \n",
    "        history.append(\n",
    "            {'epoch': epoch, \n",
    "             'train_loss': train_loss, 'valid_loss': valid_loss,\n",
    "             'train_perf': train_perf, 'valid_perf': valid_perf, 'valid_ap': valid_ap,'valid_f1': valid_f1})\n",
    "        \n",
    "        print(f'{epoch} \\t {train_loss:0.4f} \\t {valid_loss:0.4f} '\n",
    "              f'\\t {train_perf:0.4f} \\t {valid_perf:0.4f}\\t {valid_ap:0.4f}\\t {valid_f1:0.4f}')\n",
    "                                                \n",
    "        history_dict['epoch'].append(epoch)\n",
    "        history_dict['train_loss'].append(train_loss)\n",
    "        history_dict['valid_loss'].append(valid_loss)\n",
    "        history_dict['train_perf'].append(train_perf)\n",
    "        history_dict['valid_perf'].append(valid_perf)\n",
    "        history_dict['valid_ap'].append(valid_ap)\n",
    "        history_dict['valid_f1'].append(valid_f1)\n",
    "        '''\n",
    "        # model saving\n",
    "        if valid_loss < best_valid_loss:\n",
    "            print(f'best val loss {best_valid_loss:.4f} -> {valid_loss:.4f}')\n",
    "            best_valid_loss = valid_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            waiting = 0\n",
    "        else:\n",
    "            waiting += 1\n",
    "\n",
    "        # model early stopping\n",
    "        if waiting >= patience:\n",
    "            print(f'Stop training at epoch {epoch}')\n",
    "            print(f'Best val loss : {best_valid_loss:.4f}')\n",
    "            break\n",
    "        '''\n",
    "       \n",
    "    valid_loss, valid_perf, valid_ap, valid_f1, fp, dd, fn = _validate_1epoch(\n",
    "            model, loader_valid, criterion, device, metric=metric)\n",
    "    \n",
    "    print('FINAL EVAL')\n",
    "    eval_metric = {\n",
    "        'valid_loss' : valid_loss, 'valid_perf' : valid_perf,'valid_ap' : valid_ap,\n",
    "        'valid_f1' : valid_f1, 'false_positive' : fp, 'detection_delay' : dd, 'false_negative' : fn\n",
    "    }\n",
    "    \n",
    "    return history_dict, eval_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making training test validation split: LOOCV\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "    \n",
    "def pick_recordings(dataset, subj_rec_num):\n",
    "    \"\"\"Pick recordings using subject and recording numbers.\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : ConcatDataset\n",
    "        The dataset to pick recordings from.        \n",
    "    subj_rec_num : list of tuples\n",
    "        List of pairs (subj_num, rec_num) to use in split.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pick_ds: The picked recordings.\n",
    "    remaining_ds : The remaining recordings | None\n",
    "    \"\"\"\n",
    "    pick_idx = list()\n",
    "    for subj_num, rec_num in subj_rec_num:\n",
    "        for i, ds in enumerate(dataset.datasets):\n",
    "            if (ds.subj_num == subj_num) and (ds.rec_num == rec_num):\n",
    "                pick_idx.append(i)\n",
    "                \n",
    "    remaining_idx = np.setdiff1d(range(len(dataset.datasets)), pick_idx)\n",
    "    print('pick index', pick_idx, 'remaining idx', remaining_idx)\n",
    "\n",
    "    pick_ds = ConcatDataset([dataset.datasets[i] for i in pick_idx])\n",
    "    \n",
    "    if len(remaining_idx) > 0:\n",
    "        remaining_ds = ConcatDataset([dataset.datasets[i] for i in remaining_idx])\n",
    "    else:\n",
    "        remaining_ds = None\n",
    "        \n",
    "    print('pick_ids',pick_ds,'remaining_ds' ,remaining_ds) #return test, train\n",
    "    return pick_ds, remaining_ds\n",
    "\n",
    "# We seed the random number generators to make our splits reproducible\n",
    "torch.manual_seed(87)\n",
    "np.random.seed(87)\n",
    "\n",
    "# Use 2 recordings for test set\n",
    "test_recs = [(subj_num, rec_num) for subj_num, rec_num in zip( [patient_id[p_id]] * 10, test_list)]\n",
    "print('test_recs', test_recs)\n",
    "test_ds, train_ds = pick_recordings(dataset, test_recs) # Returns  picks_ds, remaining_ds\n",
    "print(f'Test: {len(test_ds)}')\n",
    "print(f'train and valid: {len(train_ds)}') #Split into train and validation\n",
    "'''\n",
    "def train_test_split(dataset):\n",
    "    \"\"\"Split dataset into train and validation keeping n_groups out in validation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : ConcatDataset (The dataset to split.)\n",
    "    n_groups : int  The number of groups to leave out.\n",
    "    split_by : 'subj_num' | 'rec_num'\n",
    "        Property to use to split dataset.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    ConcatDataset\n",
    "        The training data.\n",
    "    ConcatDataset\n",
    "        The validation data.\n",
    "    \"\"\"\n",
    "    for fold, (train_idx,valid_idx) in enumerate(LeaveOneOut().split(np.arange(len(dataset.datasets)))):\n",
    "        \n",
    "    #splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "    #groups = [getattr(ds, split_by) for ds in dataset.datasets]\n",
    "    #print('groups', groups, 'n_groups', n_groups)\n",
    "    \n",
    "    #groups = [ds for ds in dataset.datasets]\n",
    "    #print('groups', groups)\n",
    "    #train_idx, valid_idx = next(LeaveOneOut().split(np.arange(len(dataset.datasets))))\n",
    "        print('Check values for train and test', train_idx, valid_idx)\n",
    "    \n",
    "    #train_idx, valid_idx = next(LeavePGroupsOut(n_groups).split(X=groups, groups=groups))\n",
    "    \n",
    "        train_ds = ConcatDataset([dataset.datasets[i] for i in train_idx])\n",
    "        valid_ds = ConcatDataset([dataset.datasets[i] for i in valid_idx])\n",
    "        print(f'Training: {len(train_ds)}')\n",
    "        print(f'Validation: {len(valid_ds)}')\n",
    "\n",
    "    return train_ds, valid_ds\n",
    "\n",
    "# Split remaining recordings into training and validation sets #kfold\n",
    "#n_subjects_valid = max(1, int(len(train_ds.datasets) * 0.2))\n",
    "#print('n_subjects_valid', n_subjects_valid,len(train_ds.datasets) )\n",
    "#train_ds, valid_ds = train_test_split(train_ds, n_subjects_valid, split_by='subj_num') \n",
    "\n",
    "#train_ds, valid_ds = train_test_split(train_ds)\n",
    "\n",
    "#print('Number of examples in each set:')\n",
    "#print(f'Training: {len(train_ds)}')\n",
    "#print(f'Validation: {len(valid_ds)}')\n",
    "#print(f'Test: {len(test_ds)}')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computing class weight\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "classes_mapping = {0: 'non-ictal', 1: 'ictal'}\n",
    "train_y = pd.Series([y for _, y,_ in train_ds]).map(classes_mapping)\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_y), y=train_y)\n",
    "print('Train ratio',class_weights)\n",
    "#Visualizing the class distribution, Very imbalanced right now #TODO\n",
    "#ax_ = train_y.value_counts().plot(kind='barh')\n",
    "#ax_.set_xlabel('Number of training examples');\n",
    "#ax_.set_ylabel('Sleep stage');\n",
    "\n",
    "test_y = pd.Series([y for _, y,_ in test_ds]).map(classes_mapping)\n",
    "class_weights_test = compute_class_weight('balanced', classes=np.unique(test_y), y=test_y)\n",
    "print('Test ratio',class_weights_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN AND MONITOR NETWORK\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create dataloaders\n",
    "train_batch_size = 240  # Important hyperparameter For Now : (720/4) # use 240\n",
    "valid_batch_size = 240  # Can be made as large as what fits in memory; won't impact performance\n",
    "num_workers = 0  # Number of processes to use for the data loading process; 0 is the main Python process\n",
    "\n",
    "loader_train = DataLoader(\n",
    "    train_ds, batch_size=train_batch_size, shuffle=True, num_workers=num_workers)\n",
    "loader_valid = DataLoader(\n",
    "    valid_ds, batch_size=valid_batch_size, shuffle=False, num_workers=num_workers)\n",
    "loader_test = DataLoader(\n",
    "    test_ds, batch_size=valid_batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss, BCELoss\n",
    "from torch.optim import Adam\n",
    "from torch.optim import SGD\n",
    "\n",
    "# For all fold results\n",
    "results = {}\n",
    "optimizer = Adam(model.parameters(), lr=2e-3, weight_decay=0.0001) # TODO : L2 regularizer\n",
    "#optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "print(torch.Tensor(class_weights))\n",
    "#criterion = CrossEntropyLoss(weight=torch.Tensor([24.4813,  1]).to(device)) \n",
    "criterion = CrossEntropyLoss(weight=torch.Tensor(class_weights).to(device)) #TODO- \n",
    "#criterion = CrossEntropyLoss()\n",
    "#criterion = BCELoss(weight=torch.Tensor(class_weights).to(device))\n",
    "#summary(model, ( 1, 2, 2560), 180)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue - Add HP, CV on HP, LOO on All, Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import joblib\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import csv\n",
    "\n",
    "def objective(trial, loader_train, loader_valid, class_weights):\n",
    "    \n",
    "    # Initialize the best_val_loss value\n",
    "    best_val_loss = float('Inf')\n",
    "    history_inner = list()\n",
    "    history_dict_inner = {'epoch': [], 'train_loss': [], 'valid_loss': [],'train_perf':[],'valid_perf':[],'valid_ap':[],'valid_f1':[]}\n",
    "   \n",
    "    joblib.dump(study, 'study.pkl')\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Do inner cross vaidation here\n",
    "    params = {\n",
    "        'n_filters_1': trial.suggest_int(\"n_filters_1\", 5, 20, step=5), #4\n",
    "        'n_filters_2': trial.suggest_int(\"n_filters_2\", 5, 15, step=5), #3\n",
    "        'n_filters_3': trial.suggest_int(\"n_filters_3\", 5, 10, step=5), #2\n",
    "        'n_filters_4': trial.suggest_int(\"n_filters_4\", 5, 10, step=5), #2\n",
    "        'n_output_fc1': trial.suggest_int(\"n_output_fc1\", 5, 9, step=2), #3 \n",
    "        'n_output_fc2': trial.suggest_int(\"n_output_fc2\", 4, 5, step=1), #2\n",
    "        #'dropout': trial.suggest_float(\"dropout\", 0.2, 0.6, step=0.2),\n",
    "        'num_epochs': trial.suggest_int(\"num_epochs\", 150, 200, step=50), #2\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 2e-3, 24e-3, log=True), #0.001-0.024\n",
    "        #'learning_rate': trial.suggest_loguniform('lgbc__learning_rate', 1e-5, 1e-0),\n",
    "        #'optimizer': trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "        'optimizer': trial.suggest_categorical(\"optimizer\", [\"Adam\"])\n",
    "              }\n",
    "    # momentum = trial.suggest_float(\"momentum\", 0.0, 1.0)\n",
    "    #learning_rate_init = trial.suggest_float(\"learning_rate_init\", 1e-5, 1e-3, log=True)\n",
    "    \n",
    "    model= IEEGSeizureDetection_2sec(trial,params,n_channels=n_channels, sfreq=sfreq, n_classes=2)\n",
    "    model= model.to(device)\n",
    "    model.apply(reset_weights)\n",
    "    #print('model',model)\n",
    "\n",
    "    criterion = CrossEntropyLoss(weight=torch.Tensor(class_weights).to(device))\n",
    "    #criterion = CrossEntropyLoss()\n",
    "    optimizer = getattr(optim, params['optimizer'])(model.parameters(), lr= params['learning_rate'])\n",
    "    \n",
    "    #print('epoch \\t train_loss \\t valid_loss \\t train_perf \\t valid_perf \\t valid_ap \\t valid_f1')\n",
    "    print('-------------------------------------------------------------------')\n",
    "    \n",
    "    start_trial = timer()\n",
    "    for epoch in range(1, params['num_epochs'] + 1):\n",
    "        \n",
    "        train_loss, train_perf = _do_train(\n",
    "            model, loader_train, optimizer, criterion, device, metric=balanced_accuracy_score)\n",
    "        \n",
    "        valid_loss, valid_perf, valid_ap, valid_f1  = _validate(\n",
    "            model, loader_valid, criterion, device, metric=balanced_accuracy_score)\n",
    "        \n",
    "        history_inner.append(\n",
    "            {'epoch': epoch, \n",
    "             'train_loss': train_loss, 'valid_loss': valid_loss,\n",
    "             'train_perf': train_perf, 'valid_perf': valid_perf, 'valid_ap': valid_ap,'valid_f1': valid_f1})\n",
    "        \n",
    "        #print(f'{epoch} \\t {train_loss:0.4f} \\t {valid_loss:0.4f} '\n",
    "        #      f'\\t {train_perf:0.4f} \\t {valid_perf:0.4f}\\t {valid_ap:0.4f}\\t {valid_f1:0.4f}')\n",
    "                                                \n",
    "        history_dict_inner['epoch'].append(epoch)\n",
    "        history_dict_inner['train_loss'].append(train_loss)\n",
    "        history_dict_inner['valid_loss'].append(valid_loss)\n",
    "        history_dict_inner['train_perf'].append(train_perf)\n",
    "        history_dict_inner['valid_perf'].append(valid_perf)\n",
    "        history_dict_inner['valid_ap'].append(valid_ap)\n",
    "        history_dict_inner['valid_f1'].append(valid_f1)\n",
    "    #return cross validation result\n",
    "    print('Time taken for 1 fold of HPO CV is', timer() - start_trial  )\n",
    "    \n",
    "    print('Return maximum f1 score ',max(history_dict_inner['valid_f1']) )\n",
    "    print('max valid_acc',max(history_dict_inner['valid_perf']) ,'max_AUPRC',max(history_dict_inner['valid_ap']) )\n",
    "    return max(history_dict_inner['valid_f1']) # return function you want to maximize\n",
    "\n",
    "#study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler())\n",
    "#if os.path.isfile('study.pkl'):\n",
    "#    study = joblib.load('study.pkl')\n",
    "#else:\n",
    "#    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler())\n",
    "    \n",
    "#study.optimize(objective(loader_train_inner, loader_valid_inner), n_trials=1)\n",
    "\n",
    "#study.trials_dataframe() - access results  or study.best_value\n",
    "#model.set_params(**study.best_params)  model.fit(X, y) #train with best params and run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss, BCELoss\n",
    "from torch.optim import Adam\n",
    "from torch.optim import SGD\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import LeaveOneOut, KFold\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#device = 'cuda'\n",
    "#sfreq = data_raws[0].info['sfreq']  # Sampling frequency\n",
    "sfreq = 256.0 \n",
    "#n_channels = data_raws[0].info['nchan']  # Number of channels\n",
    "n_channels = 4 \n",
    "#print(f'Using device \\'{device}\\'.')\n",
    "#print(model)\n",
    "best_valid_perf = 0.0\n",
    "fold_valid_perf = 0.0\n",
    "\n",
    "# Create dataloaders\n",
    "train_batch_size = 240  # Important hyperparameter For Now : (720/4) # use 240\n",
    "valid_batch_size = 240  # Can be made as large as what fits in memory; won't impact performance\n",
    "num_workers = 0  # Number of processes to use for the data loading process; 0 is the main Python process\n",
    "#n_epochs = 100\n",
    "patience = 5\n",
    "foldperf={}\n",
    "#F1_std_dev = 0\n",
    "\n",
    "# File to save final results\n",
    "out_file = 'LOOCV_results.csv'\n",
    "of_connection = open(out_file, 'w')\n",
    "writer = csv.writer(of_connection)\n",
    "# Write the headers to the file\n",
    "writer.writerow(['Best Hyperparameter', 'f1_score Mean', 'f1_score std_dev','Loss', 'Accuracy', 'Avg_Pre',\n",
    "                 'f1_score', 'false_positives', 'Detection Delay', 'False_negatives'])\n",
    "of_connection.close()\n",
    "\n",
    "# File to save intermediate results\n",
    "'''\n",
    "out_file_n = 'NestedCV_results.csv'\n",
    "of_connection_n = open(out_file_n, 'w')\n",
    "writer = csv.writer(of_connection_n)\n",
    "# Write the headers to the file\n",
    "writer.writerow(['Outer Loop', 'Inner Loop', 'Params_1', 'F1max_1', 'Params_2', 'F1max_2', 'Params_3', 'F1max_3', \n",
    "                 'Params_4', 'F1max_4', 'Params_5', 'F1max_5', 'Params_6', 'F1max_6',\n",
    "                 'Params_7', 'F1max_7', 'Params_8', 'F1max_8', 'Params_9', 'F1max_9',\n",
    "                 'Params_10', 'F1max_10', 'Params_11', 'F1max_11', 'Params_12', 'F1max_12',\n",
    "                 'Params_13', 'F1max_13', 'Params_14', 'F1max_14', 'Params_15', 'F1max_15',\n",
    "                 'Params_16', 'F1max_16', 'Params_17', 'F1max_17', 'Params_18', 'F1max_18',\n",
    "                 'Params_19', 'F1max_19', 'Params_20', 'F1max_20'])\n",
    "of_connection_n.close()'''\n",
    "\n",
    "#optimizer = Adam(model.parameters(), lr=2e-3, weight_decay=0) # TODO : add step decay, expo or time based decay\n",
    "#optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "#criterion = CrossEntropyLoss(weight=torch.Tensor([24.4813,  1]).to(device)) \n",
    "#criterion = CrossEntropyLoss(weight=torch.Tensor(class_weights).to(device)) #TODO- \n",
    "#criterion = CrossEntropyLoss()\n",
    "#criterion = nn.BCELoss()\n",
    "    \n",
    "#loader_test = DataLoader(\n",
    "#    test_ds, batch_size=valid_batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "#Ref : https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/\n",
    "       #https://arxiv.org/pdf/1809.09446.pdf\n",
    "\n",
    "for fold, (train_idx,valid_idx) in enumerate(LeaveOneOut().split(np.arange(len(dataset.datasets)))):\n",
    "    \n",
    "    print('LeaveOneOut Cross Validation {}'.format(fold + 1))\n",
    "    print('Check values for train and test', train_idx, valid_idx)\n",
    "    \n",
    "    train_ds = ConcatDataset([dataset.datasets[i] for i in train_idx])\n",
    "    valid_ds = ConcatDataset([dataset.datasets[i] for i in valid_idx])\n",
    "    print(f'Training: {len(train_ds)}  Validation: {len(valid_ds)}')\n",
    "    #print(f'Validation: {len(valid_ds)}')\n",
    "    \n",
    "    def objective_cv(trial):\n",
    "        \n",
    "        # Ref: https://stackoverflow.com/questions/63224426/how-can-i-cross-validate-by-pytorch-and-optuna\n",
    "        f1_scores_list = []\n",
    "        global F1_std_dev\n",
    "        # InnerFold CV\n",
    "        inner_cv = KFold(n_splits= int(len(train_ds.datasets)/2), shuffle=False)\n",
    "        print('Number of folds in inner cv',int(len(dataset.datasets)/2),',trial:',trial)\n",
    "        for (k_fold, (train_index_inner, test_index_inner)) in enumerate(inner_cv.split(range(len(train_ds.datasets)))):\n",
    "        \n",
    "            print('HPO with Cross Validation {}'.format(k_fold + 1))\n",
    "            print('Check values for inner train and test', train_index_inner, test_index_inner)\n",
    "    \n",
    "            train_ds_inner = ConcatDataset([train_ds.datasets[i] for i in train_index_inner])\n",
    "            valid_ds_inner = ConcatDataset([train_ds.datasets[i] for i in test_index_inner])\n",
    "            print(f'Training_inner_cv: {len(train_ds_inner)}  Validation_inner_cv: {len(valid_ds_inner)}')\n",
    "            \n",
    "            #Check weigh ratios for train and valid\n",
    "            classes_mapping = {0: 'non-ictal', 1: 'ictal'}\n",
    "            train_y_inner = pd.Series([y for _, y,_ in train_ds_inner]).map(classes_mapping)\n",
    "            valid_y_inner = pd.Series([y for _, y,_ in valid_ds_inner]).map(classes_mapping)\n",
    "            class_weights_inner = compute_class_weight('balanced', classes=np.unique(train_y_inner), y=train_y_inner) \n",
    "            class_weights_valid_inner = compute_class_weight('balanced', classes=np.unique(valid_y_inner), y=valid_y_inner)\n",
    "            print('Weights ratio: Train',class_weights_inner,'Test',class_weights_valid_inner)\n",
    "    \n",
    "            #Dataloaders for train and valid\n",
    "            loader_train_inner = DataLoader(train_ds_inner, batch_size=train_batch_size, shuffle=True, num_workers=num_workers)\n",
    "            loader_valid_inner = DataLoader(valid_ds_inner, batch_size=valid_batch_size, shuffle=False, num_workers=num_workers)\n",
    "        \n",
    "            f1_score_max = objective(trial, loader_train_inner, loader_valid_inner,class_weights_inner)\n",
    "            print('Maximum f1 score for innerfold ',k_fold,'is:', f1_score_max)\n",
    "            #TODO : take the mean of all cross validation scores\n",
    "            f1_scores_list.append(f1_score_max) #List length = number of folds\n",
    "            print('Length of f1_score_list', len(f1_scores_list), f1_scores_list)\n",
    "            \n",
    "        print('Mean f1_score for this trial is', np.mean(f1_scores_list),'with standard deviation',np.std(f1_scores_list))  \n",
    "        F1_std_dev =  np.std(f1_scores_list)\n",
    "        #return np.mean(scores)   \n",
    "        return np.mean(f1_scores_list)\n",
    "   \n",
    "    start = timer()\n",
    "    #Evaluate Hyperparameters and return best ones\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler())\n",
    "    study.optimize(objective_cv, n_trials=100)\n",
    "    print('Time taken for one study of optimization is',timer() - start)\n",
    "\n",
    "    pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "    complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "    \n",
    "    trial_df = study.trials_dataframe()\n",
    "    trial_df.to_csv('all_trials.csv', encoding='utf-8') # all trials, for all outer folds\n",
    "    \n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "    #print(\"  Dataframe from  all trials: \", trial_df)\n",
    "    trial = study.best_trial\n",
    "    print(\"Best trial:\",trial)\n",
    "    print(\"  Value: \", study.best_trial.value) # Returns the best f1_score (average across cross validations)\n",
    "    \n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "    \n",
    "    print('Using Best parameters from study of one LOOCV')\n",
    "    print('study.best_params',study.best_params)\n",
    "    #print('trial.params',trial.params)\n",
    "    #print('study.best_trial.params',study.best_trial.params)   \n",
    "    \n",
    "    #Check weigh ratios for train and valid\n",
    "    classes_mapping = {0: 'non-ictal', 1: 'ictal'}\n",
    "    train_y = pd.Series([y for _, y,_ in train_ds]).map(classes_mapping)\n",
    "    valid_y = pd.Series([y for _, y,_ in valid_ds]).map(classes_mapping)\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(train_y), y=train_y) \n",
    "    class_weights_valid = compute_class_weight('balanced', classes=np.unique(valid_y), y=valid_y)\n",
    "    print('Weights ratio: Train',class_weights,'Test',class_weights_valid)\n",
    "\n",
    "    #Dataloaders for train and valid\n",
    "    loader_train = DataLoader(train_ds, batch_size=train_batch_size, shuffle=True, num_workers=num_workers)\n",
    "    loader_valid = DataLoader(valid_ds, batch_size=valid_batch_size, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model= IEEGSeizureDetection_2sec(trial,study.best_params,n_channels=n_channels, sfreq=sfreq, n_classes=2)\n",
    "    model.to(device)\n",
    "    print('BEST MODEL SELECTED AFTER HYPERPARAMETER OPTIMIZATION IS', model)\n",
    "    #model = model.to(device)\n",
    "    model.apply(reset_weights)\n",
    "    #print('Values to select from best param',study.best_params['num_epochs'],study.best_params['learning_rate'],study.best_params['optimizer'])\n",
    "\n",
    "    criterion = CrossEntropyLoss(weight=torch.Tensor(class_weights).to(device))\n",
    "    #criterion = BCELoss(weight=torch.Tensor(class_weights).to(device), reduction='none')\n",
    "    optimizer = Adam(model.parameters(), lr=study.best_params['learning_rate'], weight_decay=0)\n",
    "    #optimizer = getattr(optim, param['optimizer'])(model.parameters(), lr= param['learning_rate'])\n",
    "    \n",
    "    #history = train(\n",
    "    #model, loader_train, loader_valid , optimizer, criterion, n_epochs, patience, \n",
    "    #device, metric=balanced_accuracy_score)\n",
    "    \n",
    "    history, eval_metric = train(\n",
    "    model, loader_train, loader_valid , optimizer, criterion, study.best_params['num_epochs'], patience, \n",
    "    device, metric=balanced_accuracy_score)\n",
    "    \n",
    "    # # Write to the final csv file \n",
    "    of_connection = open(out_file, 'a')\n",
    "    writer = csv.writer(of_connection)\n",
    "    writer.writerow([study.best_params, study.best_trial.value, F1_std_dev,eval_metric['valid_loss'],\n",
    "                     eval_metric['valid_perf'], eval_metric['valid_ap'], eval_metric['valid_f1'],\n",
    "                    eval_metric['false_positive'],eval_metric['detection_delay'] ,eval_metric['false_negative']])\n",
    "    of_connection.close()\n",
    "\n",
    "    foldperf['fold{}'.format(fold+1)] = history  \n",
    "    \n",
    "    fold_valid_perf = np.mean(foldperf['fold{}'.format(fold+1)]['valid_perf'][study.best_params['num_epochs']-10:study.best_params['num_epochs']-1])*100.0\n",
    "    if fold_valid_perf > best_valid_perf:\n",
    "        best_valid_perf = fold_valid_perf\n",
    "        print('best_valid_perf', best_valid_perf)\n",
    "        print('Training process has finished. Saving trained model for', fold)   \n",
    "        save_path = f'./saved_model/model-{patient_id[p_id]}-fold-{fold}.pth'\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    else:\n",
    "        print('Not the best_valid_perf',fold_valid_perf)\n",
    "    \n",
    "    print('Evaluation results at the end of test set of LOOCV',eval_metric,'for fold',fold)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sigmoid()\n",
    "loss = BCELoss()\n",
    "input = torch.randn(3)\n",
    "target = torch.empty(3).random_(2)\n",
    "print('input',np.shape(input),'Target',np.shape(target))\n",
    "output = loss(m(input), target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which returns dataframe of all evaluation for LOOCV\n",
    "#Evaluation metrics:  Validation Loss, Validation Accuracy, Average Precision, F1 score, False Positive, Detection delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_train_loss = 0.0\n",
    "avg_valid_loss = 0.0\n",
    "avg_train_perf = 0.0\n",
    "avg_valid_perf = 0.0\n",
    "avg_train_loss_list = []\n",
    "avg_valid_loss_list = []\n",
    "avg_train_perf_list = []\n",
    "avg_valid_perf_list = []\n",
    "#print(foldperf['fold1']['train_loss'][n_epochs-1])\n",
    "for fold in range(0,len(train_ds.datasets)+1):\n",
    "    #print(foldperf['fold{}'.format(fold+1)])\n",
    "    #print('fold{}'.format(fold+1))\n",
    "    avg_train_loss += np.mean(foldperf['fold{}'.format(fold+1)]['train_loss'][n_epochs-10:n_epochs-1])\n",
    "    avg_valid_loss += np.mean(foldperf['fold{}'.format(fold+1)]['valid_loss'][n_epochs-10:n_epochs-1])\n",
    "    avg_train_perf += np.mean(foldperf['fold{}'.format(fold+1)]['train_perf'][n_epochs-10:n_epochs-1])\n",
    "    avg_valid_perf += np.mean(foldperf['fold{}'.format(fold+1)]['valid_perf'][n_epochs-10:n_epochs-1])\n",
    "    avg_train_loss_list.append(np.mean(foldperf['fold{}'.format(fold+1)]['train_loss'][n_epochs-10:n_epochs-1]))\n",
    "    avg_valid_loss_list.append(np.mean(foldperf['fold{}'.format(fold+1)]['valid_loss'][n_epochs-10:n_epochs-1]))\n",
    "    avg_train_perf_list.append(np.mean(foldperf['fold{}'.format(fold+1)]['train_perf'][n_epochs-10:n_epochs-1]))\n",
    "    avg_valid_perf_list.append(np.mean(foldperf['fold{}'.format(fold+1)]['valid_perf'][n_epochs-10:n_epochs-1]))\n",
    "print(f'Average Train Loss: {avg_train_loss/len(train_ds.datasets)} ')\n",
    "print(f'Average Valid Loss: {avg_valid_loss/len(train_ds.datasets)} ')\n",
    "print(f'Average Train Perf: {avg_train_perf/len(train_ds.datasets)} ')\n",
    "print(f'Average Valid Perf: {avg_valid_perf/len(train_ds.datasets)} ')\n",
    "\n",
    "#Plot evluation metric for all folds\n",
    "cv_folds = list(range(len(train_ds.datasets)+1)) \n",
    "y = avg_valid_perf_list\n",
    "print('y-4',y)\n",
    "plt.xlabel(\"LOO folds\")\n",
    "plt.ylabel(\"Eval\")\n",
    "plt.title(\"LOO CV\")\n",
    "#for i in range(len(y)):\n",
    "#    plt.plot(cv_folds,[pt[i] for pt in y],label = 'Metric%s'%i)\n",
    "plt.plot(cv_folds,y,label = 'Training performance')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 150\n",
    "patience = 5\n",
    "\n",
    "history = train(\n",
    "    model, loader_train, loader_valid , optimizer, criterion, n_epochs, patience, \n",
    "    device, metric=balanced_accuracy_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model with best val and plot/use that\n",
    "# Visualizing the learning curves\n",
    "#print(foldperf['fold3'])\n",
    "history_df = pd.DataFrame(foldperf['fold5'])\n",
    "print(history_df)\n",
    "ax1 = history_df.plot(x='epoch', y=['train_loss', 'valid_loss'], marker='o')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax2 = history_df.plot(x='epoch', y=['train_perf', 'valid_perf'], marker='o')\n",
    "ax2.set_ylabel('balanced_accuracy_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history)\n",
    "ax1 = history_df.plot(x='epoch', y=['train_loss', 'valid_loss'], marker='o')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax2 = history_df.plot(x='epoch', y=['train_perf', 'valid_perf'], marker='o')\n",
    "ax2.set_ylabel('balanced_accuracy_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute test performance\n",
    "torch.set_printoptions(precision=2)\n",
    "#best_model.eval()\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "model.eval()\n",
    "\n",
    "y_pred_all, y_true_all = list(), list()\n",
    "y_score_all = list()\n",
    "for batch_x, batch_y,batch_t in loader_test:\n",
    "    batch_x = batch_x.to(device=device, dtype=torch.float32)\n",
    "    batch_y = batch_y.to(device=device, dtype=torch.int64)\n",
    "    batch_t = batch_t                                       #([240, 512])\n",
    "    \n",
    "    #print('Check X',batch_x, batch_x.shape )\n",
    "    #print('Check Y',batch_y, batch_y.shape )\n",
    "    #print('Check time delay',batch_t, batch_t.shape )\n",
    "    output = model.forward(batch_x)\n",
    "    pred_probab = nn.Softmax(dim=1)(output)  # Move this TODO\n",
    "    probs = pred_probab[:, 1]             #Predicting probability\n",
    "    torch.set_printoptions(precision=2)\n",
    "    #print('output of neural network',pred_probab, pred_probab.shape, 'probs',probs)\n",
    "    print('target class', batch_y.cpu().numpy())\n",
    "    print('predicted class', torch.argmax(output, axis=1).cpu().numpy())\n",
    "    \n",
    "    y_pred_all.append(torch.argmax(output, axis=1).cpu().numpy())\n",
    "    y_true_all.append(batch_y.cpu().numpy())\n",
    "    y_score_all.append(probs.detach().cpu().numpy())\n",
    "    \n",
    "    #''' # Create dataframe and plot true and predicted labels\n",
    "    num_bs =  len(batch_x)      #256\n",
    "    num_in = len(batch_x[0,0,0,:])   # 2560\n",
    "    print('older t1', num_bs, num_in)\n",
    "    t1 = np.arange(0,num_in*num_bs,1)\n",
    "    #t1 = batch_t\n",
    "    \n",
    "    #create a dataframe to visualize randomized data,target and outputs\n",
    "    df_reconstruct_all = pd.DataFrame(columns=['Channel1', 'Channel2','Channel3','Channel4', 'True_Label', 'Predicted_Label'])\n",
    "    for i in range(0, len(batch_x)):\n",
    "        data_re = {'Channel1':  batch_x[i,0,0,:].cpu().numpy(),\n",
    "            'Channel2': batch_x[i,0,1,:].cpu().numpy(),\n",
    "            'Channel3': batch_x[i,0,2,:].cpu().numpy(),\n",
    "            'Channel4': batch_x[i,0,3,:].cpu().numpy(),\n",
    "            'True_Label' : np.full((num_in, ), batch_y[i].cpu().numpy()), # 2560 labels\n",
    "            'Predicted_Label' : np.full((num_in, ),torch.argmax(output, axis=1)[i].cpu().numpy() ) ,\n",
    "            'Time_Label': batch_t[i,:].cpu().numpy()\n",
    "            }\n",
    "        df_reconstruct = pd.DataFrame(data_re)\n",
    "        df_reconstruct_all = df_reconstruct_all.append(df_reconstruct)\n",
    "    \n",
    "    #Calculate detection delay\n",
    "    calculate_detection_delay_for_eval(df_reconstruct_all)\n",
    "    \n",
    "    #Plot all batches\n",
    "    #''' \n",
    "    print('BATCH_NUM',i)\n",
    "    figure, axs = plt.subplots(nrows=4, ncols=1,figsize=(10,5))\n",
    "    axs[0].plot( df_reconstruct_all.Time_Label, df_reconstruct_all.Channel1)\n",
    "    axs[0].set_title('Channel1')\n",
    "    \n",
    "    axs[1].plot( df_reconstruct_all.Time_Label, df_reconstruct_all.Channel2)\n",
    "    axs[1].set_title('Channel2')\n",
    "    \n",
    "    axs[2].plot( df_reconstruct_all.Time_Label, df_reconstruct_all.True_Label)\n",
    "    axs[2].set_title('True Label')\n",
    "    \n",
    "    axs[3].plot( df_reconstruct_all.Time_Label, df_reconstruct_all.Predicted_Label )\n",
    "    axs[3].set_title('Predicted Label')\n",
    "    plt.show()\n",
    "    #axs[0].set_xlabel('distance (m)')\n",
    "    #axs[0].set_ylabel('Damped oscillation')'''\n",
    "    '''\n",
    "    plt.figure()\n",
    "    plt.subplot(411)\n",
    "    \n",
    "    plt.plot(t1, df_reconstruct_all.Channel1 )\n",
    "    \n",
    "    plt.subplot(412)\n",
    "    plt.plot(t1, df_reconstruct_all.Channel2)\n",
    "    \n",
    "    plt.subplot(413)\n",
    "    plt.plot(t1, df_reconstruct_all.True_Label )\n",
    "    \n",
    "    plt.subplot(414)\n",
    "    plt.plot(t1, df_reconstruct_all.Predicted_Label )\n",
    "    plt.show()\n",
    "    '''\n",
    "    #print(df_reconstruct_all)  # Preint the dataframe for each batch\n",
    "\n",
    "y_pred = np.concatenate(y_pred_all)\n",
    "y_true = np.concatenate(y_true_all)\n",
    "y_score = np.concatenate(y_score_all)\n",
    "\n",
    "test_bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "test_kappa = cohen_kappa_score(y_true, y_pred)\n",
    "print(f'Test balanced accuracy: {test_bal_acc:0.3f}')\n",
    "print(f'Test Cohen\\'s kappa: {test_kappa:0.3f}')\n",
    "print('y_score', y_score.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE THE RESULTS\n",
    "classes_mapping = {0: 'non-ictal', 1: 'ictal'}\n",
    "y_train = pd.Series([y for _, y,_ in train_ds]).map(classes_mapping)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(conf_mat, classes_mapping):\n",
    "    ticks = list(classes_mapping.keys())\n",
    "    tick_labels = classes_mapping.values()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    im = ax.imshow(conf_mat, cmap='Reds')\n",
    "    ax.set_yticks(ticks)\n",
    "    ax.set_yticklabels(tick_labels)\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(tick_labels)\n",
    "    ax.set_ylabel('True label')\n",
    "    ax.set_xlabel('Predicted label')\n",
    "    ax.set_title('Confusion matrix')\n",
    "\n",
    "    for i in range(len(ticks)):\n",
    "        for j in range(len(ticks)):\n",
    "            text = ax.text(\n",
    "                j, i, conf_mat[i, j], ha='center', va='center', color='k')\n",
    "\n",
    "    fig.colorbar(im, ax=ax, fraction=0.05, label='# examples')\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "conf_mat = confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = conf_mat.ravel()\n",
    "print('conf_mat', conf_mat)\n",
    "print('TN:',tn,'FP',fp, 'FN', fn, 'TP', tp)\n",
    "precision_cm = tp/(tp+fp)\n",
    "recall_cm = tp/(tp+fn)\n",
    "accuracy_cm = (tn +tp)/(tn+fp+fn+tp)\n",
    "F1_cm = 2*(precision_cm * recall_cm)/(precision_cm + recall_cm)\n",
    "\n",
    "tpr = tp/(tp+fn)\n",
    "fnr = fn/(tp+fn)\n",
    "tnr = tn/(tn+fp)\n",
    "fpr = fp/(tn+fp)\n",
    "\n",
    "print('Precision', precision_cm, '\\nRecall',recall_cm, '\\naccuracy',accuracy_cm, '\\nF1', F1_cm)\n",
    "print('TPR:',tpr,'TNR', tnr,'\\nFNR',fnr,  'FPR', fpr)\n",
    "plot_confusion_matrix(conf_mat, classes_mapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation Metrics - PR curve, AUC, F1, AP\n",
    "# Precision Recall\n",
    "from sklearn.metrics import plot_roc_curve, plot_precision_recall_curve, roc_curve\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, fbeta_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# calculate roc and roc-auc\n",
    "print('check size', y_true.shape, y_score.shape)\n",
    "fpr, tpr, thr = roc_curve(y_true, y_score)\n",
    "roc_auc = roc_auc_score(y_true, y_score)\n",
    "print('roc auc %.3f' % roc_auc)\n",
    "\n",
    "#Calculate F1, precision,recall\n",
    "F1_score = f1_score(y_true, y_pred, average='binary')\n",
    "F2_score = fbeta_score(y_true, y_pred, average='weighted', beta=2)\n",
    "Precision_score = precision_score(y_true, y_pred, average=\"micro\")\n",
    "Recall_score = recall_score(y_true, y_pred, average=\"micro\")\n",
    "test_kappa = cohen_kappa_score(y_true, y_pred)\n",
    "print('Test Cohen\\'s kappa: %.3f' % test_kappa)\n",
    "print('F1-Measure: %.3f' % F1_score)\n",
    "print('F2-Measure: %.3f' % F2_score)\n",
    "#rint('Precision-Measure: %.3f' % Precision_score)\n",
    "#rint('Recall-Measure: %.3f' % Recall_score)\n",
    "\n",
    "# plot roc curves\n",
    "#plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr, tpr, linestyle='--',color='green', label='ROC')\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive rate')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('ROC',dpi=300)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "#print('y_true', y_true, y_true.shape, 'y_score', y_score, y_score.shape)\n",
    "precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "average_precision = average_precision_score(y_true, y_score)\n",
    "auc_precision_recall = auc(recall, precision)\n",
    "\n",
    "print('Precision from PRcurve',np.mean(precision), 'Recall from PRcurve',np.mean(recall))\n",
    "print('Average Precision', average_precision)\n",
    "print('Areas under Precision Recall curve', auc_precision_recall)\n",
    "\n",
    "display = PrecisionRecallDisplay(recall=recall,precision=precision,average_precision=average_precision, estimator_name=None)\n",
    "display.plot()\n",
    "_ = display.ax_.set_title(\"Precision Recall Curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create ONNX model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_gpu",
   "language": "python",
   "name": "myenv_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
